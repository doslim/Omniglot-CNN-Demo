{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb1da40f-a5d2-4ae1-bac0-e25a28a52ba4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Load Data for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06d7d1ec-8d33-4a68-ac48-bb100a34cbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from codes.utils import *\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d120a09-415c-4d8d-903b-019064b6793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "num_classes = 50\n",
    "num_samples_train = 15\n",
    "num_samples_test = 5\n",
    "seed = 2022\n",
    "data_folder = './omniglot_resized'\n",
    "train_image, train_label, test_image, test_label = LoadData(num_classes, num_samples_train, \n",
    "                                                            num_samples_test, seed, data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99cd6253-62ec-4549-aeeb-929eb0baa399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['savefig.dpi'] = 300 \n",
    "plt.rcParams['figure.dpi'] = 300 \n",
    "plt.rcParams['figure.figsize'] = (12, 8) \n",
    "# plt.style.use(\"bmh\")\n",
    "# plt.rcParams['font.sans-serif']=['Heiti'] \n",
    "plt.rcParams['axes.unicode_minus']=False \n",
    "plt.style.use('seaborn-bright')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92d7567f-7e11-47fb-9fc0-bb5a4c32e84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3600x2400 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "index = [0,1,2,20,30,50, 70]\n",
    "for i in range(6):\n",
    "    plt.imshow(train_image[index[i]].reshape(28,28))\n",
    "    plt.axis('off')\n",
    "    plt.subplot(2,3,i+1)\n",
    "plt.imshow(train_image[index[i+1]].reshape(28,28))\n",
    "plt.axis('off')\n",
    "plt.subplot(2,3,i+1)\n",
    "plt.savefig('data_sample.pdf',  bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76dbc2f5-3d94-4a09-8a67-6b1c884d2dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, x, y=None):\n",
    "        self.x = torch.FloatTensor(x)\n",
    "        if y is not None:\n",
    "            self.y = torch.LongTensor(y)\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a0de0db-ff60-4cd9-9b42-e351070099c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(train_image, train_label, \n",
    "                                                  test_size=0.1, random_state=2022)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_data = ImgDataset(x_train.reshape(-1,1,28,28), y_train)\n",
    "val_data = ImgDataset(x_val.reshape(-1,1,28,28), y_val)\n",
    "test_data = ImgDataset(test_image.reshape(-1,1,28,28), test_label)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd4542f-e7c6-459e-9b2b-340d4351ef65",
   "metadata": {},
   "source": [
    "# 2. Define CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a524bcb2-340c-4833-b6a5-69e64c17faa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vanilla_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Vanilla_CNN, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 1, 1), # input channel, ouput channel, filter size, stride, padding\n",
    "            nn.BatchNorm2d(32), # num_features\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, 1, 1), \n",
    "            nn.BatchNorm2d(32), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0), # kernel size, stride, padding\n",
    "        \n",
    "            nn.Conv2d(32, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64*7*7, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512,50),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1) # resize, first axis is batch_num\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e978b2-a1e2-4490-a4ca-5822fb170306",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22f886e0-0ec6-40cc-a051-27856ae2b4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "751121a6-a33b-4993-9293-97be05e125e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameters: 1698194, trainable parameters: 1698194\n"
     ]
    }
   ],
   "source": [
    "vanilla_cnn = Vanilla_CNN()\n",
    "trainable = sum(p.numel() for p in vanilla_cnn.parameters() if p.requires_grad)\n",
    "total = sum(param.numel() for param in vanilla_cnn.parameters())\n",
    "model_para = 'total parameters: {}, trainable parameters: {}'.format(total, trainable)\n",
    "print(model_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c420a04-e4fb-417f-a236-b3c822ba1d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "Vanilla_CNN                              --\n",
       "├─Sequential: 1-1                        --\n",
       "│    └─Conv2d: 2-1                       320\n",
       "│    └─BatchNorm2d: 2-2                  64\n",
       "│    └─ReLU: 2-3                         --\n",
       "│    └─Conv2d: 2-4                       9,248\n",
       "│    └─BatchNorm2d: 2-5                  64\n",
       "│    └─ReLU: 2-6                         --\n",
       "│    └─MaxPool2d: 2-7                    --\n",
       "│    └─Conv2d: 2-8                       18,496\n",
       "│    └─BatchNorm2d: 2-9                  128\n",
       "│    └─ReLU: 2-10                        --\n",
       "│    └─Conv2d: 2-11                      36,928\n",
       "│    └─BatchNorm2d: 2-12                 128\n",
       "│    └─ReLU: 2-13                        --\n",
       "│    └─MaxPool2d: 2-14                   --\n",
       "├─Sequential: 1-2                        --\n",
       "│    └─Linear: 2-15                      1,606,144\n",
       "│    └─BatchNorm1d: 2-16                 1,024\n",
       "│    └─ReLU: 2-17                        --\n",
       "│    └─Dropout: 2-18                     --\n",
       "│    └─Linear: 2-19                      25,650\n",
       "│    └─Softmax: 2-20                     --\n",
       "=================================================================\n",
       "Total params: 1,698,194\n",
       "Trainable params: 1,698,194\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_info = summary(vanilla_cnn)\n",
    "model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8ae4506-e57f-4d97-a19b-438b32c9cc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(21)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0\n",
    "optimizer = Adam(vanilla_cnn.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5246eb22-171b-49ac-93df-cea1e882d2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training, total epochs: 200\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "vanilla_cnn.to(device)\n",
    "begin_message = \"Begin training, total epochs: {}\".format(epochs)\n",
    "print(begin_message)\n",
    "\n",
    "logs = [begin_message]\n",
    "best_loss = 10\n",
    "model_dir = './output'\n",
    "log_path = './logs/train_log.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e00f76b-f12c-41fc-abe1-5bbb164a1a9f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Epoch 1, Train ] | Loss:3.83376 Time:0.712292\n",
      "[ Epoch 1, Val ] | Loss:3.91301 Time:0.006568\n",
      "save model with val loss 3.913\n",
      "[ Epoch 2, Train ] | Loss:3.64924 Time:0.056341\n",
      "[ Epoch 2, Val ] | Loss:3.92017 Time:0.004383\n",
      "[ Epoch 3, Train ] | Loss:3.51865 Time:0.052882\n",
      "[ Epoch 3, Val ] | Loss:3.91079 Time:0.002692\n",
      "save model with val loss 3.911\n",
      "[ Epoch 4, Train ] | Loss:3.41448 Time:0.039349\n",
      "[ Epoch 4, Val ] | Loss:3.88792 Time:0.002282\n",
      "save model with val loss 3.888\n",
      "[ Epoch 5, Train ] | Loss:3.31957 Time:0.038312\n",
      "[ Epoch 5, Val ] | Loss:3.83429 Time:0.002154\n",
      "save model with val loss 3.834\n",
      "[ Epoch 6, Train ] | Loss:3.22989 Time:0.038049\n",
      "[ Epoch 6, Val ] | Loss:3.72864 Time:0.002214\n",
      "save model with val loss 3.729\n",
      "[ Epoch 7, Train ] | Loss:3.16539 Time:0.037850\n",
      "[ Epoch 7, Val ] | Loss:3.58087 Time:0.002132\n",
      "save model with val loss 3.581\n",
      "[ Epoch 8, Train ] | Loss:3.14388 Time:0.043945\n",
      "[ Epoch 8, Val ] | Loss:3.49788 Time:0.002670\n",
      "save model with val loss 3.498\n",
      "[ Epoch 9, Train ] | Loss:3.10767 Time:0.037049\n",
      "[ Epoch 9, Val ] | Loss:3.42985 Time:0.002017\n",
      "save model with val loss 3.430\n",
      "[ Epoch 10, Train ] | Loss:3.06551 Time:0.037098\n",
      "[ Epoch 10, Val ] | Loss:3.38567 Time:0.002075\n",
      "save model with val loss 3.386\n",
      "[ Epoch 11, Train ] | Loss:3.01496 Time:0.037004\n",
      "[ Epoch 11, Val ] | Loss:3.25269 Time:0.002149\n",
      "save model with val loss 3.253\n",
      "[ Epoch 12, Train ] | Loss:2.99160 Time:0.037210\n",
      "[ Epoch 12, Val ] | Loss:3.23940 Time:0.002184\n",
      "save model with val loss 3.239\n",
      "[ Epoch 13, Train ] | Loss:2.97622 Time:0.049311\n",
      "[ Epoch 13, Val ] | Loss:3.21388 Time:0.002292\n",
      "save model with val loss 3.214\n",
      "[ Epoch 14, Train ] | Loss:2.96860 Time:0.036732\n",
      "[ Epoch 14, Val ] | Loss:3.19333 Time:0.001974\n",
      "save model with val loss 3.193\n",
      "[ Epoch 15, Train ] | Loss:2.95963 Time:0.036733\n",
      "[ Epoch 15, Val ] | Loss:3.15224 Time:0.001967\n",
      "save model with val loss 3.152\n",
      "[ Epoch 16, Train ] | Loss:2.95212 Time:0.037654\n",
      "[ Epoch 16, Val ] | Loss:3.21118 Time:0.002199\n",
      "[ Epoch 17, Train ] | Loss:2.95175 Time:0.036561\n",
      "[ Epoch 17, Val ] | Loss:3.18388 Time:0.001953\n",
      "[ Epoch 18, Train ] | Loss:2.94934 Time:0.036673\n",
      "[ Epoch 18, Val ] | Loss:3.15595 Time:0.002095\n",
      "[ Epoch 19, Train ] | Loss:2.94877 Time:0.038209\n",
      "[ Epoch 19, Val ] | Loss:3.15128 Time:0.002025\n",
      "save model with val loss 3.151\n",
      "[ Epoch 20, Train ] | Loss:2.94818 Time:0.037110\n",
      "[ Epoch 20, Val ] | Loss:3.15693 Time:0.002070\n",
      "[ Epoch 21, Train ] | Loss:2.94778 Time:0.037068\n",
      "[ Epoch 21, Val ] | Loss:3.14900 Time:0.002070\n",
      "save model with val loss 3.149\n",
      "[ Epoch 22, Train ] | Loss:2.94752 Time:0.037149\n",
      "[ Epoch 22, Val ] | Loss:3.14318 Time:0.002076\n",
      "save model with val loss 3.143\n",
      "[ Epoch 23, Train ] | Loss:2.94728 Time:0.036891\n",
      "[ Epoch 23, Val ] | Loss:3.14374 Time:0.002118\n",
      "[ Epoch 24, Train ] | Loss:2.94694 Time:0.036451\n",
      "[ Epoch 24, Val ] | Loss:3.13905 Time:0.002120\n",
      "save model with val loss 3.139\n",
      "[ Epoch 25, Train ] | Loss:2.94687 Time:0.037264\n",
      "[ Epoch 25, Val ] | Loss:3.13474 Time:0.002189\n",
      "save model with val loss 3.135\n",
      "[ Epoch 26, Train ] | Loss:2.94685 Time:0.040664\n",
      "[ Epoch 26, Val ] | Loss:3.13301 Time:0.002260\n",
      "save model with val loss 3.133\n",
      "[ Epoch 27, Train ] | Loss:2.94672 Time:0.036669\n",
      "[ Epoch 27, Val ] | Loss:3.13083 Time:0.001977\n",
      "save model with val loss 3.131\n",
      "[ Epoch 28, Train ] | Loss:2.94698 Time:0.037060\n",
      "[ Epoch 28, Val ] | Loss:3.13302 Time:0.002028\n",
      "[ Epoch 29, Train ] | Loss:2.94669 Time:0.036510\n",
      "[ Epoch 29, Val ] | Loss:3.12937 Time:0.002061\n",
      "save model with val loss 3.129\n",
      "[ Epoch 30, Train ] | Loss:2.94669 Time:0.036561\n",
      "[ Epoch 30, Val ] | Loss:3.12545 Time:0.002093\n",
      "save model with val loss 3.125\n",
      "[ Epoch 31, Train ] | Loss:2.94657 Time:0.037030\n",
      "[ Epoch 31, Val ] | Loss:3.12351 Time:0.002085\n",
      "save model with val loss 3.124\n",
      "[ Epoch 32, Train ] | Loss:2.94646 Time:0.037179\n",
      "[ Epoch 32, Val ] | Loss:3.12723 Time:0.001921\n",
      "[ Epoch 33, Train ] | Loss:2.94648 Time:0.038562\n",
      "[ Epoch 33, Val ] | Loss:3.12801 Time:0.002178\n",
      "[ Epoch 34, Train ] | Loss:2.94643 Time:0.036886\n",
      "[ Epoch 34, Val ] | Loss:3.12865 Time:0.001992\n",
      "[ Epoch 35, Train ] | Loss:2.94639 Time:0.037738\n",
      "[ Epoch 35, Val ] | Loss:3.12864 Time:0.002227\n",
      "[ Epoch 36, Train ] | Loss:2.94636 Time:0.037423\n",
      "[ Epoch 36, Val ] | Loss:3.12786 Time:0.002224\n",
      "[ Epoch 37, Train ] | Loss:2.94638 Time:0.037297\n",
      "[ Epoch 37, Val ] | Loss:3.12767 Time:0.002323\n",
      "[ Epoch 38, Train ] | Loss:2.94636 Time:0.036961\n",
      "[ Epoch 38, Val ] | Loss:3.12593 Time:0.002242\n",
      "[ Epoch 39, Train ] | Loss:2.94633 Time:0.036899\n",
      "[ Epoch 39, Val ] | Loss:3.12572 Time:0.002199\n",
      "[ Epoch 40, Train ] | Loss:2.94630 Time:0.036543\n",
      "[ Epoch 40, Val ] | Loss:3.12427 Time:0.002094\n",
      "[ Epoch 41, Train ] | Loss:2.94631 Time:0.036166\n",
      "[ Epoch 41, Val ] | Loss:3.12447 Time:0.002081\n",
      "[ Epoch 42, Train ] | Loss:2.94632 Time:0.037047\n",
      "[ Epoch 42, Val ] | Loss:3.12569 Time:0.002421\n",
      "[ Epoch 43, Train ] | Loss:2.94632 Time:0.036828\n",
      "[ Epoch 43, Val ] | Loss:3.12719 Time:0.002110\n",
      "[ Epoch 44, Train ] | Loss:2.94625 Time:0.036516\n",
      "[ Epoch 44, Val ] | Loss:3.12782 Time:0.002003\n",
      "[ Epoch 45, Train ] | Loss:2.94627 Time:0.037636\n",
      "[ Epoch 45, Val ] | Loss:3.12697 Time:0.002163\n",
      "[ Epoch 46, Train ] | Loss:2.94622 Time:0.036788\n",
      "[ Epoch 46, Val ] | Loss:3.12559 Time:0.002040\n",
      "[ Epoch 47, Train ] | Loss:2.94622 Time:0.036852\n",
      "[ Epoch 47, Val ] | Loss:3.12359 Time:0.002043\n",
      "[ Epoch 48, Train ] | Loss:2.94616 Time:0.037046\n",
      "[ Epoch 48, Val ] | Loss:3.12285 Time:0.002170\n",
      "save model with val loss 3.123\n",
      "[ Epoch 49, Train ] | Loss:2.94620 Time:0.037099\n",
      "[ Epoch 49, Val ] | Loss:3.12444 Time:0.001938\n",
      "[ Epoch 50, Train ] | Loss:2.94622 Time:0.037487\n",
      "[ Epoch 50, Val ] | Loss:3.12350 Time:0.002153\n",
      "[ Epoch 51, Train ] | Loss:2.94616 Time:0.036567\n",
      "[ Epoch 51, Val ] | Loss:3.12311 Time:0.002017\n",
      "[ Epoch 52, Train ] | Loss:2.94614 Time:0.037005\n",
      "[ Epoch 52, Val ] | Loss:3.12311 Time:0.002076\n",
      "[ Epoch 53, Train ] | Loss:2.94619 Time:0.037112\n",
      "[ Epoch 53, Val ] | Loss:3.12351 Time:0.002078\n",
      "[ Epoch 54, Train ] | Loss:2.94612 Time:0.036584\n",
      "[ Epoch 54, Val ] | Loss:3.12225 Time:0.002054\n",
      "save model with val loss 3.122\n",
      "[ Epoch 55, Train ] | Loss:2.94613 Time:0.037232\n",
      "[ Epoch 55, Val ] | Loss:3.12225 Time:0.002105\n",
      "save model with val loss 3.122\n",
      "[ Epoch 56, Train ] | Loss:2.94612 Time:0.046641\n",
      "[ Epoch 56, Val ] | Loss:3.12101 Time:0.002203\n",
      "save model with val loss 3.121\n",
      "[ Epoch 57, Train ] | Loss:2.94614 Time:0.037512\n",
      "[ Epoch 57, Val ] | Loss:3.12192 Time:0.002163\n",
      "[ Epoch 58, Train ] | Loss:2.94609 Time:0.036559\n",
      "[ Epoch 58, Val ] | Loss:3.12161 Time:0.002000\n",
      "[ Epoch 59, Train ] | Loss:2.94612 Time:0.036956\n",
      "[ Epoch 59, Val ] | Loss:3.12312 Time:0.002021\n",
      "[ Epoch 60, Train ] | Loss:2.94607 Time:0.036668\n",
      "[ Epoch 60, Val ] | Loss:3.12422 Time:0.001942\n",
      "[ Epoch 61, Train ] | Loss:2.94612 Time:0.036575\n",
      "[ Epoch 61, Val ] | Loss:3.12429 Time:0.001934\n",
      "[ Epoch 62, Train ] | Loss:2.94609 Time:0.036856\n",
      "[ Epoch 62, Val ] | Loss:3.12293 Time:0.002328\n",
      "[ Epoch 63, Train ] | Loss:2.94616 Time:0.036755\n",
      "[ Epoch 63, Val ] | Loss:3.12148 Time:0.002013\n",
      "[ Epoch 64, Train ] | Loss:2.94609 Time:0.036882\n",
      "[ Epoch 64, Val ] | Loss:3.11914 Time:0.002105\n",
      "save model with val loss 3.119\n",
      "[ Epoch 65, Train ] | Loss:2.94605 Time:0.037590\n",
      "[ Epoch 65, Val ] | Loss:3.11846 Time:0.002050\n",
      "save model with val loss 3.118\n",
      "[ Epoch 66, Train ] | Loss:2.94610 Time:0.037341\n",
      "[ Epoch 66, Val ] | Loss:3.11837 Time:0.002260\n",
      "save model with val loss 3.118\n",
      "[ Epoch 67, Train ] | Loss:2.94605 Time:0.037281\n",
      "[ Epoch 67, Val ] | Loss:3.11828 Time:0.002217\n",
      "save model with val loss 3.118\n",
      "[ Epoch 68, Train ] | Loss:2.94604 Time:0.037344\n",
      "[ Epoch 68, Val ] | Loss:3.11798 Time:0.002155\n",
      "save model with val loss 3.118\n",
      "[ Epoch 69, Train ] | Loss:2.94603 Time:0.037269\n",
      "[ Epoch 69, Val ] | Loss:3.11826 Time:0.002286\n",
      "[ Epoch 70, Train ] | Loss:2.94607 Time:0.037052\n",
      "[ Epoch 70, Val ] | Loss:3.11962 Time:0.002146\n",
      "[ Epoch 71, Train ] | Loss:2.94603 Time:0.036873\n",
      "[ Epoch 71, Val ] | Loss:3.11780 Time:0.001878\n",
      "save model with val loss 3.118\n",
      "[ Epoch 72, Train ] | Loss:2.94601 Time:0.037947\n",
      "[ Epoch 72, Val ] | Loss:3.11890 Time:0.002166\n",
      "[ Epoch 73, Train ] | Loss:2.94605 Time:0.038018\n",
      "[ Epoch 73, Val ] | Loss:3.11972 Time:0.002040\n",
      "[ Epoch 74, Train ] | Loss:2.94601 Time:0.036542\n",
      "[ Epoch 74, Val ] | Loss:3.11880 Time:0.002006\n",
      "[ Epoch 75, Train ] | Loss:2.94600 Time:0.040510\n",
      "[ Epoch 75, Val ] | Loss:3.12120 Time:0.002776\n",
      "[ Epoch 76, Train ] | Loss:2.94601 Time:0.038121\n",
      "[ Epoch 76, Val ] | Loss:3.12146 Time:0.001947\n",
      "[ Epoch 77, Train ] | Loss:2.94601 Time:0.037797\n",
      "[ Epoch 77, Val ] | Loss:3.12123 Time:0.001969\n",
      "[ Epoch 78, Train ] | Loss:2.94597 Time:0.036633\n",
      "[ Epoch 78, Val ] | Loss:3.12178 Time:0.001919\n",
      "[ Epoch 79, Train ] | Loss:2.94599 Time:0.036907\n",
      "[ Epoch 79, Val ] | Loss:3.12083 Time:0.001978\n",
      "[ Epoch 80, Train ] | Loss:2.94600 Time:0.036809\n",
      "[ Epoch 80, Val ] | Loss:3.12064 Time:0.002014\n",
      "[ Epoch 81, Train ] | Loss:2.94599 Time:0.038051\n",
      "[ Epoch 81, Val ] | Loss:3.11951 Time:0.002024\n",
      "[ Epoch 82, Train ] | Loss:2.94598 Time:0.036109\n",
      "[ Epoch 82, Val ] | Loss:3.11940 Time:0.001896\n",
      "[ Epoch 83, Train ] | Loss:2.94599 Time:0.040271\n",
      "[ Epoch 83, Val ] | Loss:3.12003 Time:0.001965\n",
      "[ Epoch 84, Train ] | Loss:2.94598 Time:0.040450\n",
      "[ Epoch 84, Val ] | Loss:3.11909 Time:0.002137\n",
      "[ Epoch 85, Train ] | Loss:2.94597 Time:0.036619\n",
      "[ Epoch 85, Val ] | Loss:3.12106 Time:0.002158\n",
      "[ Epoch 86, Train ] | Loss:2.94598 Time:0.036026\n",
      "[ Epoch 86, Val ] | Loss:3.11974 Time:0.001979\n",
      "[ Epoch 87, Train ] | Loss:2.94597 Time:0.036084\n",
      "[ Epoch 87, Val ] | Loss:3.11875 Time:0.001938\n",
      "[ Epoch 88, Train ] | Loss:2.94597 Time:0.037152\n",
      "[ Epoch 88, Val ] | Loss:3.11944 Time:0.002107\n",
      "[ Epoch 89, Train ] | Loss:2.94597 Time:0.036595\n",
      "[ Epoch 89, Val ] | Loss:3.11904 Time:0.004982\n",
      "[ Epoch 90, Train ] | Loss:2.94596 Time:0.036673\n",
      "[ Epoch 90, Val ] | Loss:3.12042 Time:0.002098\n",
      "[ Epoch 91, Train ] | Loss:2.94597 Time:0.036626\n",
      "[ Epoch 91, Val ] | Loss:3.12149 Time:0.001966\n",
      "[ Epoch 92, Train ] | Loss:2.94595 Time:0.036904\n",
      "[ Epoch 92, Val ] | Loss:3.12065 Time:0.002059\n",
      "[ Epoch 93, Train ] | Loss:2.94596 Time:0.036533\n",
      "[ Epoch 93, Val ] | Loss:3.11770 Time:0.002028\n",
      "save model with val loss 3.118\n",
      "[ Epoch 94, Train ] | Loss:2.94596 Time:0.036986\n",
      "[ Epoch 94, Val ] | Loss:3.11830 Time:0.001973\n",
      "[ Epoch 95, Train ] | Loss:2.94597 Time:0.039506\n",
      "[ Epoch 95, Val ] | Loss:3.11805 Time:0.002019\n",
      "[ Epoch 96, Train ] | Loss:2.94595 Time:0.036036\n",
      "[ Epoch 96, Val ] | Loss:3.11667 Time:0.002030\n",
      "save model with val loss 3.117\n",
      "[ Epoch 97, Train ] | Loss:2.94596 Time:0.036857\n",
      "[ Epoch 97, Val ] | Loss:3.11653 Time:0.002128\n",
      "save model with val loss 3.117\n",
      "[ Epoch 98, Train ] | Loss:2.94595 Time:0.036618\n",
      "[ Epoch 98, Val ] | Loss:3.11841 Time:0.002171\n",
      "[ Epoch 99, Train ] | Loss:2.94594 Time:0.042269\n",
      "[ Epoch 99, Val ] | Loss:3.11567 Time:0.002443\n",
      "save model with val loss 3.116\n",
      "[ Epoch 100, Train ] | Loss:2.94594 Time:0.037228\n",
      "[ Epoch 100, Val ] | Loss:3.11780 Time:0.001887\n",
      "[ Epoch 101, Train ] | Loss:2.94593 Time:0.036968\n",
      "[ Epoch 101, Val ] | Loss:3.11711 Time:0.001950\n",
      "[ Epoch 102, Train ] | Loss:2.94594 Time:0.037302\n",
      "[ Epoch 102, Val ] | Loss:3.11814 Time:0.002021\n",
      "[ Epoch 103, Train ] | Loss:2.94594 Time:0.036284\n",
      "[ Epoch 103, Val ] | Loss:3.11881 Time:0.001879\n",
      "[ Epoch 104, Train ] | Loss:2.94594 Time:0.036823\n",
      "[ Epoch 104, Val ] | Loss:3.11804 Time:0.001952\n",
      "[ Epoch 105, Train ] | Loss:2.94594 Time:0.037373\n",
      "[ Epoch 105, Val ] | Loss:3.11783 Time:0.002018\n",
      "[ Epoch 106, Train ] | Loss:2.94593 Time:0.039467\n",
      "[ Epoch 106, Val ] | Loss:3.11687 Time:0.001958\n",
      "[ Epoch 107, Train ] | Loss:2.94592 Time:0.037257\n",
      "[ Epoch 107, Val ] | Loss:3.11635 Time:0.002008\n",
      "[ Epoch 108, Train ] | Loss:2.94593 Time:0.036694\n",
      "[ Epoch 108, Val ] | Loss:3.11602 Time:0.002007\n",
      "[ Epoch 109, Train ] | Loss:2.94593 Time:0.036493\n",
      "[ Epoch 109, Val ] | Loss:3.11493 Time:0.002000\n",
      "save model with val loss 3.115\n",
      "[ Epoch 110, Train ] | Loss:2.94592 Time:0.037859\n",
      "[ Epoch 110, Val ] | Loss:3.11636 Time:0.002138\n",
      "[ Epoch 111, Train ] | Loss:2.94592 Time:0.039140\n",
      "[ Epoch 111, Val ] | Loss:3.11715 Time:0.002214\n",
      "[ Epoch 112, Train ] | Loss:2.94592 Time:0.037063\n",
      "[ Epoch 112, Val ] | Loss:3.11813 Time:0.002070\n",
      "[ Epoch 113, Train ] | Loss:2.94593 Time:0.036675\n",
      "[ Epoch 113, Val ] | Loss:3.11573 Time:0.001968\n",
      "[ Epoch 114, Train ] | Loss:2.94592 Time:0.036616\n",
      "[ Epoch 114, Val ] | Loss:3.11607 Time:0.001969\n",
      "[ Epoch 115, Train ] | Loss:2.94592 Time:0.037550\n",
      "[ Epoch 115, Val ] | Loss:3.11629 Time:0.002069\n",
      "[ Epoch 116, Train ] | Loss:2.94592 Time:0.036755\n",
      "[ Epoch 116, Val ] | Loss:3.11629 Time:0.002056\n",
      "[ Epoch 117, Train ] | Loss:2.94591 Time:0.036271\n",
      "[ Epoch 117, Val ] | Loss:3.11685 Time:0.001908\n",
      "[ Epoch 118, Train ] | Loss:2.94591 Time:0.036843\n",
      "[ Epoch 118, Val ] | Loss:3.11679 Time:0.001986\n",
      "[ Epoch 119, Train ] | Loss:2.94592 Time:0.036545\n",
      "[ Epoch 119, Val ] | Loss:3.11823 Time:0.002048\n",
      "[ Epoch 120, Train ] | Loss:2.94590 Time:0.036539\n",
      "[ Epoch 120, Val ] | Loss:3.11684 Time:0.002139\n",
      "[ Epoch 121, Train ] | Loss:2.94591 Time:0.036148\n",
      "[ Epoch 121, Val ] | Loss:3.11602 Time:0.001982\n",
      "[ Epoch 122, Train ] | Loss:2.94590 Time:0.037450\n",
      "[ Epoch 122, Val ] | Loss:3.11768 Time:0.002427\n",
      "[ Epoch 123, Train ] | Loss:2.94591 Time:0.038023\n",
      "[ Epoch 123, Val ] | Loss:3.11701 Time:0.001949\n",
      "[ Epoch 124, Train ] | Loss:2.94591 Time:0.038338\n",
      "[ Epoch 124, Val ] | Loss:3.11660 Time:0.002076\n",
      "[ Epoch 125, Train ] | Loss:2.94591 Time:0.037747\n",
      "[ Epoch 125, Val ] | Loss:3.11584 Time:0.002269\n",
      "[ Epoch 126, Train ] | Loss:2.94590 Time:0.037780\n",
      "[ Epoch 126, Val ] | Loss:3.11534 Time:0.002085\n",
      "[ Epoch 127, Train ] | Loss:2.94591 Time:0.036299\n",
      "[ Epoch 127, Val ] | Loss:3.11401 Time:0.001866\n",
      "save model with val loss 3.114\n",
      "[ Epoch 128, Train ] | Loss:2.94589 Time:0.036939\n",
      "[ Epoch 128, Val ] | Loss:3.11498 Time:0.002131\n",
      "[ Epoch 129, Train ] | Loss:2.94592 Time:0.037833\n",
      "[ Epoch 129, Val ] | Loss:3.11605 Time:0.002213\n",
      "[ Epoch 130, Train ] | Loss:2.94589 Time:0.037427\n",
      "[ Epoch 130, Val ] | Loss:3.11678 Time:0.002284\n",
      "[ Epoch 131, Train ] | Loss:2.94590 Time:0.036468\n",
      "[ Epoch 131, Val ] | Loss:3.11678 Time:0.002005\n",
      "[ Epoch 132, Train ] | Loss:2.94591 Time:0.035982\n",
      "[ Epoch 132, Val ] | Loss:3.11651 Time:0.001970\n",
      "[ Epoch 133, Train ] | Loss:2.94590 Time:0.036354\n",
      "[ Epoch 133, Val ] | Loss:3.11744 Time:0.002128\n",
      "[ Epoch 134, Train ] | Loss:2.94589 Time:0.036341\n",
      "[ Epoch 134, Val ] | Loss:3.11613 Time:0.002007\n",
      "[ Epoch 135, Train ] | Loss:2.94589 Time:0.036376\n",
      "[ Epoch 135, Val ] | Loss:3.11582 Time:0.002138\n",
      "[ Epoch 136, Train ] | Loss:2.94590 Time:0.036408\n",
      "[ Epoch 136, Val ] | Loss:3.11597 Time:0.002069\n",
      "[ Epoch 137, Train ] | Loss:2.94589 Time:0.036193\n",
      "[ Epoch 137, Val ] | Loss:3.11649 Time:0.002046\n",
      "[ Epoch 138, Train ] | Loss:2.94588 Time:0.036254\n",
      "[ Epoch 138, Val ] | Loss:3.11553 Time:0.002040\n",
      "[ Epoch 139, Train ] | Loss:2.94590 Time:0.036041\n",
      "[ Epoch 139, Val ] | Loss:3.11665 Time:0.002013\n",
      "[ Epoch 140, Train ] | Loss:2.94589 Time:0.036427\n",
      "[ Epoch 140, Val ] | Loss:3.11693 Time:0.002088\n",
      "[ Epoch 141, Train ] | Loss:2.94588 Time:0.036179\n",
      "[ Epoch 141, Val ] | Loss:3.11528 Time:0.002072\n",
      "[ Epoch 142, Train ] | Loss:2.94588 Time:0.036933\n",
      "[ Epoch 142, Val ] | Loss:3.11469 Time:0.002109\n",
      "[ Epoch 143, Train ] | Loss:2.94588 Time:0.036470\n",
      "[ Epoch 143, Val ] | Loss:3.11646 Time:0.001859\n",
      "[ Epoch 144, Train ] | Loss:2.94589 Time:0.036158\n",
      "[ Epoch 144, Val ] | Loss:3.11829 Time:0.001995\n",
      "[ Epoch 145, Train ] | Loss:2.94588 Time:0.036764\n",
      "[ Epoch 145, Val ] | Loss:3.11780 Time:0.002042\n",
      "[ Epoch 146, Train ] | Loss:2.94590 Time:0.036568\n",
      "[ Epoch 146, Val ] | Loss:3.11619 Time:0.001916\n",
      "[ Epoch 147, Train ] | Loss:2.94588 Time:0.036763\n",
      "[ Epoch 147, Val ] | Loss:3.11654 Time:0.001932\n",
      "[ Epoch 148, Train ] | Loss:2.94590 Time:0.036532\n",
      "[ Epoch 148, Val ] | Loss:3.11681 Time:0.001860\n",
      "[ Epoch 149, Train ] | Loss:2.94588 Time:0.036606\n",
      "[ Epoch 149, Val ] | Loss:3.11622 Time:0.001898\n",
      "[ Epoch 150, Train ] | Loss:2.94588 Time:0.036971\n",
      "[ Epoch 150, Val ] | Loss:3.11585 Time:0.001988\n",
      "[ Epoch 151, Train ] | Loss:2.94589 Time:0.037591\n",
      "[ Epoch 151, Val ] | Loss:3.11571 Time:0.002209\n",
      "[ Epoch 152, Train ] | Loss:2.94588 Time:0.038246\n",
      "[ Epoch 152, Val ] | Loss:3.11408 Time:0.002047\n",
      "[ Epoch 153, Train ] | Loss:2.94588 Time:0.036620\n",
      "[ Epoch 153, Val ] | Loss:3.11286 Time:0.002020\n",
      "save model with val loss 3.113\n",
      "[ Epoch 154, Train ] | Loss:2.94588 Time:0.036885\n",
      "[ Epoch 154, Val ] | Loss:3.11397 Time:0.002131\n",
      "[ Epoch 155, Train ] | Loss:2.94588 Time:0.036705\n",
      "[ Epoch 155, Val ] | Loss:3.11403 Time:0.002168\n",
      "[ Epoch 156, Train ] | Loss:2.94587 Time:0.036579\n",
      "[ Epoch 156, Val ] | Loss:3.11295 Time:0.002046\n",
      "[ Epoch 157, Train ] | Loss:2.94587 Time:0.036471\n",
      "[ Epoch 157, Val ] | Loss:3.11459 Time:0.001950\n",
      "[ Epoch 158, Train ] | Loss:2.94587 Time:0.036856\n",
      "[ Epoch 158, Val ] | Loss:3.11507 Time:0.002102\n",
      "[ Epoch 159, Train ] | Loss:2.94587 Time:0.036367\n",
      "[ Epoch 159, Val ] | Loss:3.11546 Time:0.001978\n",
      "[ Epoch 160, Train ] | Loss:2.94589 Time:0.036065\n",
      "[ Epoch 160, Val ] | Loss:3.11622 Time:0.002085\n",
      "[ Epoch 161, Train ] | Loss:2.94587 Time:0.035856\n",
      "[ Epoch 161, Val ] | Loss:3.11741 Time:0.001988\n",
      "[ Epoch 162, Train ] | Loss:2.94587 Time:0.035779\n",
      "[ Epoch 162, Val ] | Loss:3.11815 Time:0.001936\n",
      "[ Epoch 163, Train ] | Loss:2.94588 Time:0.035923\n",
      "[ Epoch 163, Val ] | Loss:3.11699 Time:0.001940\n",
      "[ Epoch 164, Train ] | Loss:2.94587 Time:0.035853\n",
      "[ Epoch 164, Val ] | Loss:3.11642 Time:0.001925\n",
      "[ Epoch 165, Train ] | Loss:2.94587 Time:0.036727\n",
      "[ Epoch 165, Val ] | Loss:3.11367 Time:0.002283\n",
      "[ Epoch 166, Train ] | Loss:2.94587 Time:0.037116\n",
      "[ Epoch 166, Val ] | Loss:3.11394 Time:0.002039\n",
      "[ Epoch 167, Train ] | Loss:2.94587 Time:0.036346\n",
      "[ Epoch 167, Val ] | Loss:3.11494 Time:0.001945\n",
      "[ Epoch 168, Train ] | Loss:2.94587 Time:0.036226\n",
      "[ Epoch 168, Val ] | Loss:3.11383 Time:0.001894\n",
      "[ Epoch 169, Train ] | Loss:2.94587 Time:0.035956\n",
      "[ Epoch 169, Val ] | Loss:3.11564 Time:0.001930\n",
      "[ Epoch 170, Train ] | Loss:2.94588 Time:0.036555\n",
      "[ Epoch 170, Val ] | Loss:3.11511 Time:0.001832\n",
      "[ Epoch 171, Train ] | Loss:2.94587 Time:0.036493\n",
      "[ Epoch 171, Val ] | Loss:3.11445 Time:0.001956\n",
      "[ Epoch 172, Train ] | Loss:2.94588 Time:0.036801\n",
      "[ Epoch 172, Val ] | Loss:3.11323 Time:0.001914\n",
      "[ Epoch 173, Train ] | Loss:2.94586 Time:0.035835\n",
      "[ Epoch 173, Val ] | Loss:3.11479 Time:0.001956\n",
      "[ Epoch 174, Train ] | Loss:2.94586 Time:0.036170\n",
      "[ Epoch 174, Val ] | Loss:3.11419 Time:0.001876\n",
      "[ Epoch 175, Train ] | Loss:2.94587 Time:0.036538\n",
      "[ Epoch 175, Val ] | Loss:3.11378 Time:0.001949\n",
      "[ Epoch 176, Train ] | Loss:2.94586 Time:0.037390\n",
      "[ Epoch 176, Val ] | Loss:3.11464 Time:0.002121\n",
      "[ Epoch 177, Train ] | Loss:2.94587 Time:0.035719\n",
      "[ Epoch 177, Val ] | Loss:3.11599 Time:0.001952\n",
      "[ Epoch 178, Train ] | Loss:2.94586 Time:0.035762\n",
      "[ Epoch 178, Val ] | Loss:3.11560 Time:0.001994\n",
      "[ Epoch 179, Train ] | Loss:2.94586 Time:0.035752\n",
      "[ Epoch 179, Val ] | Loss:3.11604 Time:0.001994\n",
      "[ Epoch 180, Train ] | Loss:2.94587 Time:0.036109\n",
      "[ Epoch 180, Val ] | Loss:3.11525 Time:0.002080\n",
      "[ Epoch 181, Train ] | Loss:2.94586 Time:0.035800\n",
      "[ Epoch 181, Val ] | Loss:3.11481 Time:0.002002\n",
      "[ Epoch 182, Train ] | Loss:2.94586 Time:0.035867\n",
      "[ Epoch 182, Val ] | Loss:3.11500 Time:0.001993\n",
      "[ Epoch 183, Train ] | Loss:2.94587 Time:0.035654\n",
      "[ Epoch 183, Val ] | Loss:3.11464 Time:0.002000\n",
      "[ Epoch 184, Train ] | Loss:2.94586 Time:0.035770\n",
      "[ Epoch 184, Val ] | Loss:3.11712 Time:0.002014\n",
      "[ Epoch 185, Train ] | Loss:2.94587 Time:0.036159\n",
      "[ Epoch 185, Val ] | Loss:3.11609 Time:0.002091\n",
      "[ Epoch 186, Train ] | Loss:2.94587 Time:0.037840\n",
      "[ Epoch 186, Val ] | Loss:3.11555 Time:0.002045\n",
      "[ Epoch 187, Train ] | Loss:2.94586 Time:0.036373\n",
      "[ Epoch 187, Val ] | Loss:3.11674 Time:0.002009\n",
      "[ Epoch 188, Train ] | Loss:2.94587 Time:0.036188\n",
      "[ Epoch 188, Val ] | Loss:3.11638 Time:0.001945\n",
      "[ Epoch 189, Train ] | Loss:2.94586 Time:0.035658\n",
      "[ Epoch 189, Val ] | Loss:3.11772 Time:0.001989\n",
      "[ Epoch 190, Train ] | Loss:2.94586 Time:0.036044\n",
      "[ Epoch 190, Val ] | Loss:3.11813 Time:0.002061\n",
      "[ Epoch 191, Train ] | Loss:2.94587 Time:0.035620\n",
      "[ Epoch 191, Val ] | Loss:3.11901 Time:0.001940\n",
      "[ Epoch 192, Train ] | Loss:2.94586 Time:0.035639\n",
      "[ Epoch 192, Val ] | Loss:3.11692 Time:0.002002\n",
      "[ Epoch 193, Train ] | Loss:2.94586 Time:0.035686\n",
      "[ Epoch 193, Val ] | Loss:3.11815 Time:0.001973\n",
      "[ Epoch 194, Train ] | Loss:2.94586 Time:0.035519\n",
      "[ Epoch 194, Val ] | Loss:3.11651 Time:0.001999\n",
      "[ Epoch 195, Train ] | Loss:2.94586 Time:0.035714\n",
      "[ Epoch 195, Val ] | Loss:3.11656 Time:0.002090\n",
      "[ Epoch 196, Train ] | Loss:2.94586 Time:0.035549\n",
      "[ Epoch 196, Val ] | Loss:3.11569 Time:0.001998\n",
      "[ Epoch 197, Train ] | Loss:2.94586 Time:0.035801\n",
      "[ Epoch 197, Val ] | Loss:3.11722 Time:0.002035\n",
      "[ Epoch 198, Train ] | Loss:2.94585 Time:0.035895\n",
      "[ Epoch 198, Val ] | Loss:3.11619 Time:0.002040\n",
      "[ Epoch 199, Train ] | Loss:2.94585 Time:0.036404\n",
      "[ Epoch 199, Val ] | Loss:3.11521 Time:0.002078\n",
      "[ Epoch 200, Train ] | Loss:2.94585 Time:0.036699\n",
      "[ Epoch 200, Val ] | Loss:3.11376 Time:0.001816\n",
      "Finish training!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    vanilla_cnn.train()\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    start_time = time.time()\n",
    "    train_log = {}  # save training logs\n",
    "\n",
    "    lr_message = 'learning rate of epoch {}: {}'.format(epoch + 1, learning_rate)\n",
    "    train_log['lr_message'] = lr_message\n",
    "\n",
    "    # training\n",
    "    for i, batch_data in enumerate(train_loader, 1):\n",
    "        inputs = batch_data[0].to(device)\n",
    "        labels = batch_data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = vanilla_cnn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "    epoch_train_loss = np.mean(train_loss)\n",
    "    train_message = '[ Epoch {}, Train ] | Loss:{:.5f} Time:{:.6f}'.format(epoch + 1,\n",
    "                                                                        epoch_train_loss,\n",
    "                                                                        time.time() - start_time)\n",
    "    print(train_message)\n",
    "\n",
    "    # validation\n",
    "    vanilla_cnn.eval()\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, batch_data in enumerate(val_loader, 1):\n",
    "            inputs = batch_data[0].to(device)\n",
    "            labels = batch_data[1].to(device)\n",
    "\n",
    "            outputs = vanilla_cnn(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "    epoch_val_loss = np.mean(val_loss)\n",
    "    val_message = '[ Epoch {}, Val ] | Loss:{:.5f} Time:{:.6f}'.format(epoch + 1, epoch_val_loss,\n",
    "                                                                         time.time() - start_time)\n",
    "    print(val_message)\n",
    "    flag = False\n",
    "    if epoch_val_loss < best_loss:\n",
    "        best_loss = epoch_val_loss\n",
    "        save_message = 'save model with val loss {:.3f}'.format(best_loss)\n",
    "        print(save_message)\n",
    "        flag = True\n",
    "        torch.save(vanilla_cnn, \"{}/{}.pt\".format(model_dir, 'vanilla_cnn'))\n",
    "\n",
    "        # if self.lr_decay:\n",
    "        #     self.lr_scheduler.step()\n",
    "\n",
    "    train_log[\"epoch\"] = epoch + 1\n",
    "    train_log[\"train_message\"] = train_message\n",
    "    train_log[\"val_message\"] = val_message\n",
    "    train_log[\"epoch_train_loss\"] = epoch_train_loss\n",
    "    train_log[\"epoch_val_loss\"] = epoch_val_loss\n",
    "    if flag:\n",
    "        train_log[\"save_message\"] = save_message\n",
    "    logs.append(train_log)\n",
    "    with open(log_path, \"w\") as fp:\n",
    "        json.dump(logs, fp)\n",
    "print(\"Finish training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a10e6ae-f5a6-4f34-9936-04a65fd31870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin testing\n",
      "Test loss: 3.1013695001602173\n",
      "——————Test——————\n",
      "           class precision  recall f1-score support\n",
      "0              0    1.0000  0.8000   0.8889       5\n",
      "1              1    0.8000  0.8000   0.8000       5\n",
      "2              2    1.0000  1.0000   1.0000       5\n",
      "3              3    1.0000  1.0000   1.0000       5\n",
      "4              4    1.0000  0.6000   0.7500       5\n",
      "5              5    0.8000  0.8000   0.8000       5\n",
      "6              6    0.6000  0.6000   0.6000       5\n",
      "7              7    0.8000  0.8000   0.8000       5\n",
      "8              8    1.0000  1.0000   1.0000       5\n",
      "9              9    1.0000  1.0000   1.0000       5\n",
      "10            10    1.0000  1.0000   1.0000       5\n",
      "11            11    1.0000  1.0000   1.0000       5\n",
      "12            12    0.7143  1.0000   0.8333       5\n",
      "13            13    1.0000  1.0000   1.0000       5\n",
      "14            14    0.7500  0.6000   0.6667       5\n",
      "15            15    1.0000  0.8000   0.8889       5\n",
      "16            16    1.0000  1.0000   1.0000       5\n",
      "17            17    0.8333  1.0000   0.9091       5\n",
      "18            18    1.0000  0.8000   0.8889       5\n",
      "19            19    0.7143  1.0000   0.8333       5\n",
      "20            20    1.0000  0.8000   0.8889       5\n",
      "21            21    1.0000  0.8000   0.8889       5\n",
      "22            22    0.6000  0.6000   0.6000       5\n",
      "23            23    1.0000  0.8000   0.8889       5\n",
      "24            24    1.0000  1.0000   1.0000       5\n",
      "25            25    1.0000  1.0000   1.0000       5\n",
      "26            26    1.0000  1.0000   1.0000       5\n",
      "27            27    1.0000  1.0000   1.0000       5\n",
      "28            28    0.8333  1.0000   0.9091       5\n",
      "29            29    0.8000  0.8000   0.8000       5\n",
      "30            30    1.0000  1.0000   1.0000       5\n",
      "31            31    1.0000  1.0000   1.0000       5\n",
      "32            32    1.0000  0.8000   0.8889       5\n",
      "33            33    1.0000  1.0000   1.0000       5\n",
      "34            34    1.0000  1.0000   1.0000       5\n",
      "35            35    1.0000  1.0000   1.0000       5\n",
      "36            36    0.8000  0.8000   0.8000       5\n",
      "37            37    0.8333  1.0000   0.9091       5\n",
      "38            38    1.0000  1.0000   1.0000       5\n",
      "39            39    0.8333  1.0000   0.9091       5\n",
      "40            40    1.0000  1.0000   1.0000       5\n",
      "41            41    0.8000  0.8000   0.8000       5\n",
      "42            42    0.8333  1.0000   0.9091       5\n",
      "43            43    0.8333  1.0000   0.9091       5\n",
      "44            44    0.7500  0.6000   0.6667       5\n",
      "45            45    0.7143  1.0000   0.8333       5\n",
      "46            46    0.6000  0.6000   0.6000       5\n",
      "47            47    1.0000  1.0000   1.0000       5\n",
      "48            48    1.0000  1.0000   1.0000       5\n",
      "49            49    1.0000  0.8000   0.8889       5\n",
      "50      accuracy                     0.8960     250\n",
      "51     macro avg    0.9049  0.8960   0.8950     250\n",
      "52  weighted avg    0.9049  0.8960   0.8950     250\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"{}/{}.pt\".format(model_dir, 'vanilla_cnn'))\n",
    "predictions = []\n",
    "y_labels = []\n",
    "test_loss = []\n",
    "\n",
    "print(\"Begin testing\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, labels) in enumerate(test_loader, 1):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        test_loss.append(loss.item())\n",
    "        pred = outputs.argmax(dim=1)\n",
    "        predictions.extend(pred.cpu().numpy().tolist())\n",
    "        y_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    epoch_test_loss = np.mean(test_loss)\n",
    "    test_message = \"Test loss: {}\".format(epoch_test_loss)\n",
    "    report = classification_report(y_labels, predictions, digits=4, zero_division=1)\n",
    "    print(test_message)\n",
    "    \n",
    "# make the classification report more readable\n",
    "report = report.splitlines()\n",
    "columns = ['class'] + report[0].split()\n",
    "col_1, col_2, col_3, col_4, col_5 = [], [], [], [], []\n",
    "for row in report[1:]:\n",
    "    if len(row.split()) != 0:\n",
    "        row = row.split()\n",
    "        if len(row) < 5:\n",
    "            col_1.append(row[0])\n",
    "            col_2.append('')\n",
    "            col_3.append('')\n",
    "            col_4.append(row[1])\n",
    "            col_5.append(row[2])\n",
    "        elif len(row) > 5:\n",
    "            col_1.append(row[0] + ' ' + row[1])\n",
    "            col_2.append(row[2])\n",
    "            col_3.append(row[3])\n",
    "            col_4.append(row[4])\n",
    "            col_5.append(row[5])\n",
    "        else:\n",
    "            col_1.append(row[0])\n",
    "            col_2.append(row[1])\n",
    "            col_3.append(row[2])\n",
    "            col_4.append(row[3])\n",
    "            col_5.append(row[4])\n",
    "result = pd.DataFrame()\n",
    "result[columns[0]] = col_1\n",
    "result[columns[1]] = col_2\n",
    "result[columns[2]] = col_3\n",
    "result[columns[3]] = col_4\n",
    "result[columns[4]] = col_5\n",
    "print(\"——————Test——————\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c07969-140a-48ec-b3ca-591e2f5c197a",
   "metadata": {},
   "source": [
    "# 4. Load data for fully-connected network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d5ac8b1a-bf82-460c-b236-be7c53671e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "num_classes = 50\n",
    "num_samples_train = 15\n",
    "num_samples_test = 5\n",
    "seed = 2022\n",
    "data_folder = './omniglot_resized'\n",
    "train_image, train_label, test_image, test_label = LoadData(num_classes, num_samples_train, \n",
    "                                                            num_samples_test, seed, data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b62b36ec-94e3-4636-b2cf-a4641ecf7a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(train_image, train_label, \n",
    "                                                  test_size=0.1, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "25209145-3d40-4d9d-9160-35c9cb9d10f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_data = ImgDataset(x_train, y_train)\n",
    "val_data = ImgDataset(x_val, y_val)\n",
    "test_data = ImgDataset(test_image, test_label)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12933499-1d2e-40ae-b721-12ce7f6e5b2d",
   "metadata": {},
   "source": [
    "# 5. Define the fully-connected network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "0874999d-93b9-4456-bfa9-5ff7c72fbf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC_Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FC_Network, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(28*28, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512,50),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f587624-dcc8-49e6-b885-05b100f67513",
   "metadata": {},
   "source": [
    "# 6. Train the FC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "92cbe065-12fe-45c8-b193-e78d246709a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameters: 1621042, trainable parameters: 1621042\n"
     ]
    }
   ],
   "source": [
    "fc_network = FC_Network()\n",
    "trainable = sum(p.numel() for p in fc_network.parameters() if p.requires_grad)\n",
    "total = sum(param.numel() for param in fc_network.parameters())\n",
    "model_para = 'total parameters: {}, trainable parameters: {}'.format(total, trainable)\n",
    "print(model_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "02c3f50f-afc4-40cc-ba80-2dadbb98e492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "FC_Network                               --\n",
       "├─Sequential: 1-1                        --\n",
       "│    └─Linear: 2-1                       803,840\n",
       "│    └─BatchNorm1d: 2-2                  2,048\n",
       "│    └─ReLU: 2-3                         --\n",
       "│    └─Linear: 2-4                       524,800\n",
       "│    └─BatchNorm1d: 2-5                  1,024\n",
       "│    └─ReLU: 2-6                         --\n",
       "│    └─Linear: 2-7                       262,656\n",
       "│    └─BatchNorm1d: 2-8                  1,024\n",
       "│    └─ReLU: 2-9                         --\n",
       "│    └─Dropout: 2-10                     --\n",
       "│    └─Linear: 2-11                      25,650\n",
       "│    └─Softmax: 2-12                     --\n",
       "=================================================================\n",
       "Total params: 1,621,042\n",
       "Trainable params: 1,621,042\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_info = summary(fc_network)\n",
    "model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "6b169138-83eb-4f4c-a1ce-388283a18b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(21)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4\n",
    "optimizer = Adam(fc_network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c037e707-2f9b-4ca1-aac8-987936b72e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training, total epochs: 200\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "fc_network.to(device)\n",
    "begin_message = \"Begin training, total epochs: {}\".format(epochs)\n",
    "print(begin_message)\n",
    "\n",
    "logs = [begin_message]\n",
    "best_loss = 10\n",
    "model_dir = './output'\n",
    "log_path = './logs/train_log_fc.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "bb10142e-260f-44bb-929a-0a70905c9edc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Epoch 1, Train ] | Loss:3.87375 Time:0.061895\n",
      "[ Epoch 1, Val ] | Loss:3.91092 Time:0.005299\n",
      "save model with val loss 3.911\n",
      "[ Epoch 2, Train ] | Loss:3.73374 Time:0.061889\n",
      "[ Epoch 2, Val ] | Loss:3.90741 Time:0.004077\n",
      "save model with val loss 3.907\n",
      "[ Epoch 3, Train ] | Loss:3.58633 Time:0.050169\n",
      "[ Epoch 3, Val ] | Loss:3.89404 Time:0.004492\n",
      "save model with val loss 3.894\n",
      "[ Epoch 4, Train ] | Loss:3.48793 Time:0.062068\n",
      "[ Epoch 4, Val ] | Loss:3.85705 Time:0.005520\n",
      "save model with val loss 3.857\n",
      "[ Epoch 5, Train ] | Loss:3.37542 Time:0.049878\n",
      "[ Epoch 5, Val ] | Loss:3.79549 Time:0.004775\n",
      "save model with val loss 3.795\n",
      "[ Epoch 6, Train ] | Loss:3.28036 Time:0.054908\n",
      "[ Epoch 6, Val ] | Loss:3.71984 Time:0.005187\n",
      "save model with val loss 3.720\n",
      "[ Epoch 7, Train ] | Loss:3.22381 Time:0.052263\n",
      "[ Epoch 7, Val ] | Loss:3.65939 Time:0.005742\n",
      "save model with val loss 3.659\n",
      "[ Epoch 8, Train ] | Loss:3.17086 Time:0.050433\n",
      "[ Epoch 8, Val ] | Loss:3.63822 Time:0.004627\n",
      "save model with val loss 3.638\n",
      "[ Epoch 9, Train ] | Loss:3.13349 Time:0.049037\n",
      "[ Epoch 9, Val ] | Loss:3.60177 Time:0.004638\n",
      "save model with val loss 3.602\n",
      "[ Epoch 10, Train ] | Loss:3.09482 Time:0.051625\n",
      "[ Epoch 10, Val ] | Loss:3.60428 Time:0.005645\n",
      "[ Epoch 11, Train ] | Loss:3.06378 Time:0.055585\n",
      "[ Epoch 11, Val ] | Loss:3.57203 Time:0.003673\n",
      "save model with val loss 3.572\n",
      "[ Epoch 12, Train ] | Loss:3.03547 Time:0.056118\n",
      "[ Epoch 12, Val ] | Loss:3.55129 Time:0.005275\n",
      "save model with val loss 3.551\n",
      "[ Epoch 13, Train ] | Loss:3.01190 Time:0.061792\n",
      "[ Epoch 13, Val ] | Loss:3.53491 Time:0.003780\n",
      "save model with val loss 3.535\n",
      "[ Epoch 14, Train ] | Loss:2.99317 Time:0.048023\n",
      "[ Epoch 14, Val ] | Loss:3.51577 Time:0.004636\n",
      "save model with val loss 3.516\n",
      "[ Epoch 15, Train ] | Loss:2.98536 Time:0.053526\n",
      "[ Epoch 15, Val ] | Loss:3.50506 Time:0.004631\n",
      "save model with val loss 3.505\n",
      "[ Epoch 16, Train ] | Loss:2.97752 Time:0.051073\n",
      "[ Epoch 16, Val ] | Loss:3.50914 Time:0.004584\n",
      "[ Epoch 17, Train ] | Loss:2.97622 Time:0.054254\n",
      "[ Epoch 17, Val ] | Loss:3.52660 Time:0.005248\n",
      "[ Epoch 18, Train ] | Loss:2.96147 Time:0.053842\n",
      "[ Epoch 18, Val ] | Loss:3.53440 Time:0.004207\n",
      "[ Epoch 19, Train ] | Loss:2.95587 Time:0.058194\n",
      "[ Epoch 19, Val ] | Loss:3.51502 Time:0.005542\n",
      "[ Epoch 20, Train ] | Loss:2.95197 Time:0.056117\n",
      "[ Epoch 20, Val ] | Loss:3.50362 Time:0.004340\n",
      "save model with val loss 3.504\n",
      "[ Epoch 21, Train ] | Loss:2.95129 Time:0.055038\n",
      "[ Epoch 21, Val ] | Loss:3.50315 Time:0.004266\n",
      "save model with val loss 3.503\n",
      "[ Epoch 22, Train ] | Loss:2.95388 Time:0.052120\n",
      "[ Epoch 22, Val ] | Loss:3.49671 Time:0.004072\n",
      "save model with val loss 3.497\n",
      "[ Epoch 23, Train ] | Loss:2.95034 Time:0.049506\n",
      "[ Epoch 23, Val ] | Loss:3.49142 Time:0.005307\n",
      "save model with val loss 3.491\n",
      "[ Epoch 24, Train ] | Loss:2.94986 Time:0.048340\n",
      "[ Epoch 24, Val ] | Loss:3.48568 Time:0.004073\n",
      "save model with val loss 3.486\n",
      "[ Epoch 25, Train ] | Loss:2.94902 Time:0.057611\n",
      "[ Epoch 25, Val ] | Loss:3.48496 Time:0.003887\n",
      "save model with val loss 3.485\n",
      "[ Epoch 26, Train ] | Loss:2.94837 Time:0.058436\n",
      "[ Epoch 26, Val ] | Loss:3.48552 Time:0.004427\n",
      "[ Epoch 27, Train ] | Loss:2.94846 Time:0.049387\n",
      "[ Epoch 27, Val ] | Loss:3.48673 Time:0.003734\n",
      "[ Epoch 28, Train ] | Loss:2.94769 Time:0.050841\n",
      "[ Epoch 28, Val ] | Loss:3.49304 Time:0.005110\n",
      "[ Epoch 29, Train ] | Loss:2.94731 Time:0.047972\n",
      "[ Epoch 29, Val ] | Loss:3.49310 Time:0.004349\n",
      "[ Epoch 30, Train ] | Loss:2.94726 Time:0.049914\n",
      "[ Epoch 30, Val ] | Loss:3.48994 Time:0.004711\n",
      "[ Epoch 31, Train ] | Loss:2.94709 Time:0.054847\n",
      "[ Epoch 31, Val ] | Loss:3.49113 Time:0.004763\n",
      "[ Epoch 32, Train ] | Loss:2.94692 Time:0.053173\n",
      "[ Epoch 32, Val ] | Loss:3.49367 Time:0.004851\n",
      "[ Epoch 33, Train ] | Loss:2.94692 Time:0.047934\n",
      "[ Epoch 33, Val ] | Loss:3.49366 Time:0.004184\n",
      "[ Epoch 34, Train ] | Loss:2.94686 Time:0.052934\n",
      "[ Epoch 34, Val ] | Loss:3.49548 Time:0.003767\n",
      "[ Epoch 35, Train ] | Loss:2.94675 Time:0.048892\n",
      "[ Epoch 35, Val ] | Loss:3.49606 Time:0.004277\n",
      "[ Epoch 36, Train ] | Loss:2.94679 Time:0.055530\n",
      "[ Epoch 36, Val ] | Loss:3.49554 Time:0.004392\n",
      "[ Epoch 37, Train ] | Loss:2.94666 Time:0.049903\n",
      "[ Epoch 37, Val ] | Loss:3.49279 Time:0.004703\n",
      "[ Epoch 38, Train ] | Loss:2.94669 Time:0.049444\n",
      "[ Epoch 38, Val ] | Loss:3.48939 Time:0.004629\n",
      "[ Epoch 39, Train ] | Loss:2.94676 Time:0.047039\n",
      "[ Epoch 39, Val ] | Loss:3.48932 Time:0.004689\n",
      "[ Epoch 40, Train ] | Loss:2.94662 Time:0.047038\n",
      "[ Epoch 40, Val ] | Loss:3.48963 Time:0.004687\n",
      "[ Epoch 41, Train ] | Loss:2.94661 Time:0.047059\n",
      "[ Epoch 41, Val ] | Loss:3.49047 Time:0.004694\n",
      "[ Epoch 42, Train ] | Loss:2.94672 Time:0.051518\n",
      "[ Epoch 42, Val ] | Loss:3.49354 Time:0.004635\n",
      "[ Epoch 43, Train ] | Loss:2.94665 Time:0.049111\n",
      "[ Epoch 43, Val ] | Loss:3.49088 Time:0.004702\n",
      "[ Epoch 44, Train ] | Loss:2.94662 Time:0.049120\n",
      "[ Epoch 44, Val ] | Loss:3.49022 Time:0.004695\n",
      "[ Epoch 45, Train ] | Loss:2.94664 Time:0.049065\n",
      "[ Epoch 45, Val ] | Loss:3.48957 Time:0.004695\n",
      "[ Epoch 46, Train ] | Loss:2.94658 Time:0.051490\n",
      "[ Epoch 46, Val ] | Loss:3.49189 Time:0.004626\n",
      "[ Epoch 47, Train ] | Loss:2.94660 Time:0.049040\n",
      "[ Epoch 47, Val ] | Loss:3.49261 Time:0.004687\n",
      "[ Epoch 48, Train ] | Loss:2.94656 Time:0.049048\n",
      "[ Epoch 48, Val ] | Loss:3.49276 Time:0.004691\n",
      "[ Epoch 49, Train ] | Loss:2.94656 Time:0.049033\n",
      "[ Epoch 49, Val ] | Loss:3.49229 Time:0.004698\n",
      "[ Epoch 50, Train ] | Loss:2.94686 Time:0.051383\n",
      "[ Epoch 50, Val ] | Loss:3.49376 Time:0.004634\n",
      "[ Epoch 51, Train ] | Loss:2.94665 Time:0.049011\n",
      "[ Epoch 51, Val ] | Loss:3.49725 Time:0.004681\n",
      "[ Epoch 52, Train ] | Loss:2.94674 Time:0.048997\n",
      "[ Epoch 52, Val ] | Loss:3.50001 Time:0.004706\n",
      "[ Epoch 53, Train ] | Loss:2.94670 Time:0.048979\n",
      "[ Epoch 53, Val ] | Loss:3.50257 Time:0.004697\n",
      "[ Epoch 54, Train ] | Loss:2.94669 Time:0.051302\n",
      "[ Epoch 54, Val ] | Loss:3.50113 Time:0.004629\n",
      "[ Epoch 55, Train ] | Loss:2.94668 Time:0.048925\n",
      "[ Epoch 55, Val ] | Loss:3.50723 Time:0.004695\n",
      "[ Epoch 56, Train ] | Loss:2.94682 Time:0.048931\n",
      "[ Epoch 56, Val ] | Loss:3.50733 Time:0.004699\n",
      "[ Epoch 57, Train ] | Loss:2.94679 Time:0.048912\n",
      "[ Epoch 57, Val ] | Loss:3.50799 Time:0.004700\n",
      "[ Epoch 58, Train ] | Loss:2.94665 Time:0.051290\n",
      "[ Epoch 58, Val ] | Loss:3.50671 Time:0.004624\n",
      "[ Epoch 59, Train ] | Loss:2.94657 Time:0.048884\n",
      "[ Epoch 59, Val ] | Loss:3.50536 Time:0.004691\n",
      "[ Epoch 60, Train ] | Loss:2.94655 Time:0.048861\n",
      "[ Epoch 60, Val ] | Loss:3.50726 Time:0.004660\n",
      "[ Epoch 61, Train ] | Loss:2.94660 Time:0.048858\n",
      "[ Epoch 61, Val ] | Loss:3.50689 Time:0.004691\n",
      "[ Epoch 62, Train ] | Loss:2.94688 Time:0.051213\n",
      "[ Epoch 62, Val ] | Loss:3.50492 Time:0.004643\n",
      "[ Epoch 63, Train ] | Loss:2.94680 Time:0.048795\n",
      "[ Epoch 63, Val ] | Loss:3.50689 Time:0.004698\n",
      "[ Epoch 64, Train ] | Loss:2.94671 Time:0.048801\n",
      "[ Epoch 64, Val ] | Loss:3.51039 Time:0.004708\n",
      "[ Epoch 65, Train ] | Loss:2.94670 Time:0.048753\n",
      "[ Epoch 65, Val ] | Loss:3.51383 Time:0.004693\n",
      "[ Epoch 66, Train ] | Loss:2.94675 Time:0.051183\n",
      "[ Epoch 66, Val ] | Loss:3.51349 Time:0.004642\n",
      "[ Epoch 67, Train ] | Loss:2.94672 Time:0.048771\n",
      "[ Epoch 67, Val ] | Loss:3.51371 Time:0.004692\n",
      "[ Epoch 68, Train ] | Loss:2.94650 Time:0.056292\n",
      "[ Epoch 68, Val ] | Loss:3.51424 Time:0.007770\n",
      "[ Epoch 69, Train ] | Loss:2.94675 Time:0.053413\n",
      "[ Epoch 69, Val ] | Loss:3.51361 Time:0.004688\n",
      "[ Epoch 70, Train ] | Loss:2.94670 Time:0.051118\n",
      "[ Epoch 70, Val ] | Loss:3.50886 Time:0.004626\n",
      "[ Epoch 71, Train ] | Loss:2.94659 Time:0.048702\n",
      "[ Epoch 71, Val ] | Loss:3.50294 Time:0.004692\n",
      "[ Epoch 72, Train ] | Loss:2.94665 Time:0.048672\n",
      "[ Epoch 72, Val ] | Loss:3.50417 Time:0.004705\n",
      "[ Epoch 73, Train ] | Loss:2.94678 Time:0.048655\n",
      "[ Epoch 73, Val ] | Loss:3.50821 Time:0.004682\n",
      "[ Epoch 74, Train ] | Loss:2.94666 Time:0.051044\n",
      "[ Epoch 74, Val ] | Loss:3.51398 Time:0.004640\n",
      "[ Epoch 75, Train ] | Loss:2.94677 Time:0.048616\n",
      "[ Epoch 75, Val ] | Loss:3.51606 Time:0.004703\n",
      "[ Epoch 76, Train ] | Loss:2.94682 Time:0.048651\n",
      "[ Epoch 76, Val ] | Loss:3.51871 Time:0.004699\n",
      "[ Epoch 77, Train ] | Loss:2.94664 Time:0.048661\n",
      "[ Epoch 77, Val ] | Loss:3.52179 Time:0.004707\n",
      "[ Epoch 78, Train ] | Loss:2.94666 Time:0.051023\n",
      "[ Epoch 78, Val ] | Loss:3.52201 Time:0.004639\n",
      "[ Epoch 79, Train ] | Loss:2.94671 Time:0.048635\n",
      "[ Epoch 79, Val ] | Loss:3.51696 Time:0.004673\n",
      "[ Epoch 80, Train ] | Loss:2.94670 Time:0.048562\n",
      "[ Epoch 80, Val ] | Loss:3.52033 Time:0.004704\n",
      "[ Epoch 81, Train ] | Loss:2.94664 Time:0.048597\n",
      "[ Epoch 81, Val ] | Loss:3.51979 Time:0.004702\n",
      "[ Epoch 82, Train ] | Loss:2.94669 Time:0.051602\n",
      "[ Epoch 82, Val ] | Loss:3.52140 Time:0.003916\n",
      "[ Epoch 83, Train ] | Loss:2.94673 Time:0.051317\n",
      "[ Epoch 83, Val ] | Loss:3.52358 Time:0.004504\n",
      "[ Epoch 84, Train ] | Loss:2.94664 Time:0.058551\n",
      "[ Epoch 84, Val ] | Loss:3.52611 Time:0.005355\n",
      "[ Epoch 85, Train ] | Loss:2.94672 Time:0.049774\n",
      "[ Epoch 85, Val ] | Loss:3.53028 Time:0.005491\n",
      "[ Epoch 86, Train ] | Loss:2.94682 Time:0.052071\n",
      "[ Epoch 86, Val ] | Loss:3.53151 Time:0.003937\n",
      "[ Epoch 87, Train ] | Loss:2.94663 Time:0.055396\n",
      "[ Epoch 87, Val ] | Loss:3.52961 Time:0.004819\n",
      "[ Epoch 88, Train ] | Loss:2.94671 Time:0.053281\n",
      "[ Epoch 88, Val ] | Loss:3.52993 Time:0.004110\n",
      "[ Epoch 89, Train ] | Loss:2.94691 Time:0.050002\n",
      "[ Epoch 89, Val ] | Loss:3.52711 Time:0.005022\n",
      "[ Epoch 90, Train ] | Loss:2.94677 Time:0.054751\n",
      "[ Epoch 90, Val ] | Loss:3.52948 Time:0.004079\n",
      "[ Epoch 91, Train ] | Loss:2.94680 Time:0.047847\n",
      "[ Epoch 91, Val ] | Loss:3.53304 Time:0.004689\n",
      "[ Epoch 92, Train ] | Loss:2.94679 Time:0.048374\n",
      "[ Epoch 92, Val ] | Loss:3.53547 Time:0.004692\n",
      "[ Epoch 93, Train ] | Loss:2.94679 Time:0.048381\n",
      "[ Epoch 93, Val ] | Loss:3.53341 Time:0.004697\n",
      "[ Epoch 94, Train ] | Loss:2.94676 Time:0.050789\n",
      "[ Epoch 94, Val ] | Loss:3.53219 Time:0.004626\n",
      "[ Epoch 95, Train ] | Loss:2.94683 Time:0.048414\n",
      "[ Epoch 95, Val ] | Loss:3.53474 Time:0.004695\n",
      "[ Epoch 96, Train ] | Loss:2.94677 Time:0.048404\n",
      "[ Epoch 96, Val ] | Loss:3.53677 Time:0.004683\n",
      "[ Epoch 97, Train ] | Loss:2.94671 Time:0.048338\n",
      "[ Epoch 97, Val ] | Loss:3.53706 Time:0.004698\n",
      "[ Epoch 98, Train ] | Loss:2.94679 Time:0.050723\n",
      "[ Epoch 98, Val ] | Loss:3.53452 Time:0.004631\n",
      "[ Epoch 99, Train ] | Loss:2.94670 Time:0.048304\n",
      "[ Epoch 99, Val ] | Loss:3.53015 Time:0.004696\n",
      "[ Epoch 100, Train ] | Loss:2.94682 Time:0.048322\n",
      "[ Epoch 100, Val ] | Loss:3.53608 Time:0.004696\n",
      "[ Epoch 101, Train ] | Loss:2.94682 Time:0.052845\n",
      "[ Epoch 101, Val ] | Loss:3.54069 Time:0.004564\n",
      "[ Epoch 102, Train ] | Loss:2.94678 Time:0.048264\n",
      "[ Epoch 102, Val ] | Loss:3.53570 Time:0.004629\n",
      "[ Epoch 103, Train ] | Loss:2.94684 Time:0.048252\n",
      "[ Epoch 103, Val ] | Loss:3.53430 Time:0.004692\n",
      "[ Epoch 104, Train ] | Loss:2.94703 Time:0.048279\n",
      "[ Epoch 104, Val ] | Loss:3.54741 Time:0.004684\n",
      "[ Epoch 105, Train ] | Loss:2.94708 Time:0.050345\n",
      "[ Epoch 105, Val ] | Loss:3.54093 Time:0.004696\n",
      "[ Epoch 106, Train ] | Loss:2.94730 Time:0.055108\n",
      "[ Epoch 106, Val ] | Loss:3.54904 Time:0.004619\n",
      "[ Epoch 107, Train ] | Loss:2.94747 Time:0.048257\n",
      "[ Epoch 107, Val ] | Loss:3.55856 Time:0.004690\n",
      "[ Epoch 108, Train ] | Loss:2.94874 Time:0.048202\n",
      "[ Epoch 108, Val ] | Loss:3.55842 Time:0.004694\n",
      "[ Epoch 109, Train ] | Loss:2.94911 Time:0.048186\n",
      "[ Epoch 109, Val ] | Loss:3.56311 Time:0.004689\n",
      "[ Epoch 110, Train ] | Loss:2.95445 Time:0.054480\n",
      "[ Epoch 110, Val ] | Loss:3.57638 Time:0.004631\n",
      "[ Epoch 111, Train ] | Loss:2.96442 Time:0.048167\n",
      "[ Epoch 111, Val ] | Loss:3.59508 Time:0.004699\n",
      "[ Epoch 112, Train ] | Loss:3.01090 Time:0.048163\n",
      "[ Epoch 112, Val ] | Loss:3.59321 Time:0.004674\n",
      "[ Epoch 113, Train ] | Loss:3.08664 Time:0.048168\n",
      "[ Epoch 113, Val ] | Loss:3.57149 Time:0.004689\n",
      "[ Epoch 114, Train ] | Loss:3.07835 Time:0.052308\n",
      "[ Epoch 114, Val ] | Loss:3.47600 Time:0.004627\n",
      "save model with val loss 3.476\n",
      "[ Epoch 115, Train ] | Loss:3.04312 Time:0.048177\n",
      "[ Epoch 115, Val ] | Loss:3.47209 Time:0.004586\n",
      "save model with val loss 3.472\n",
      "[ Epoch 116, Train ] | Loss:3.00202 Time:0.047235\n",
      "[ Epoch 116, Val ] | Loss:3.48182 Time:0.004616\n",
      "[ Epoch 117, Train ] | Loss:2.98314 Time:0.047995\n",
      "[ Epoch 117, Val ] | Loss:3.48278 Time:0.006524\n",
      "[ Epoch 118, Train ] | Loss:2.96964 Time:0.052428\n",
      "[ Epoch 118, Val ] | Loss:3.47085 Time:0.004674\n",
      "save model with val loss 3.471\n",
      "[ Epoch 119, Train ] | Loss:2.96410 Time:0.049313\n",
      "[ Epoch 119, Val ] | Loss:3.48228 Time:0.004621\n",
      "[ Epoch 120, Train ] | Loss:2.95628 Time:0.047929\n",
      "[ Epoch 120, Val ] | Loss:3.48277 Time:0.004675\n",
      "[ Epoch 121, Train ] | Loss:2.95733 Time:0.050376\n",
      "[ Epoch 121, Val ] | Loss:3.46986 Time:0.004613\n",
      "save model with val loss 3.470\n",
      "[ Epoch 122, Train ] | Loss:2.95802 Time:0.047411\n",
      "[ Epoch 122, Val ] | Loss:3.44494 Time:0.004617\n",
      "save model with val loss 3.445\n",
      "[ Epoch 123, Train ] | Loss:2.95554 Time:0.047350\n",
      "[ Epoch 123, Val ] | Loss:3.44243 Time:0.004631\n",
      "save model with val loss 3.442\n",
      "[ Epoch 124, Train ] | Loss:2.95647 Time:0.049746\n",
      "[ Epoch 124, Val ] | Loss:3.47371 Time:0.004567\n",
      "[ Epoch 125, Train ] | Loss:2.95368 Time:0.049984\n",
      "[ Epoch 125, Val ] | Loss:3.48430 Time:0.004698\n",
      "[ Epoch 126, Train ] | Loss:2.95262 Time:0.047930\n",
      "[ Epoch 126, Val ] | Loss:3.47483 Time:0.004688\n",
      "[ Epoch 127, Train ] | Loss:2.94686 Time:0.047926\n",
      "[ Epoch 127, Val ] | Loss:3.48218 Time:0.004685\n",
      "[ Epoch 128, Train ] | Loss:2.94735 Time:0.059875\n",
      "[ Epoch 128, Val ] | Loss:3.49746 Time:0.005654\n",
      "[ Epoch 129, Train ] | Loss:2.94736 Time:0.056744\n",
      "[ Epoch 129, Val ] | Loss:3.49594 Time:0.005088\n",
      "[ Epoch 130, Train ] | Loss:2.94673 Time:0.052565\n",
      "[ Epoch 130, Val ] | Loss:3.48236 Time:0.004206\n",
      "[ Epoch 131, Train ] | Loss:2.94676 Time:0.051615\n",
      "[ Epoch 131, Val ] | Loss:3.47225 Time:0.003884\n",
      "[ Epoch 132, Train ] | Loss:2.94687 Time:0.050745\n",
      "[ Epoch 132, Val ] | Loss:3.46300 Time:0.004244\n",
      "[ Epoch 133, Train ] | Loss:2.94645 Time:0.052344\n",
      "[ Epoch 133, Val ] | Loss:3.46576 Time:0.003664\n",
      "[ Epoch 134, Train ] | Loss:2.94656 Time:0.062919\n",
      "[ Epoch 134, Val ] | Loss:3.46457 Time:0.005492\n",
      "[ Epoch 135, Train ] | Loss:2.94626 Time:0.054724\n",
      "[ Epoch 135, Val ] | Loss:3.45977 Time:0.004522\n",
      "[ Epoch 136, Train ] | Loss:2.94640 Time:0.055767\n",
      "[ Epoch 136, Val ] | Loss:3.45656 Time:0.004316\n",
      "[ Epoch 137, Train ] | Loss:2.94626 Time:0.056816\n",
      "[ Epoch 137, Val ] | Loss:3.45850 Time:0.003772\n",
      "[ Epoch 138, Train ] | Loss:2.94636 Time:0.051446\n",
      "[ Epoch 138, Val ] | Loss:3.45673 Time:0.011161\n",
      "[ Epoch 139, Train ] | Loss:2.94631 Time:0.052358\n",
      "[ Epoch 139, Val ] | Loss:3.46174 Time:0.004669\n",
      "[ Epoch 140, Train ] | Loss:2.94626 Time:0.050325\n",
      "[ Epoch 140, Val ] | Loss:3.46403 Time:0.005558\n",
      "[ Epoch 141, Train ] | Loss:2.94624 Time:0.048435\n",
      "[ Epoch 141, Val ] | Loss:3.46217 Time:0.003695\n",
      "[ Epoch 142, Train ] | Loss:2.94620 Time:0.050135\n",
      "[ Epoch 142, Val ] | Loss:3.46138 Time:0.004622\n",
      "[ Epoch 143, Train ] | Loss:2.94641 Time:0.049836\n",
      "[ Epoch 143, Val ] | Loss:3.46212 Time:0.004417\n",
      "[ Epoch 144, Train ] | Loss:2.94622 Time:0.047620\n",
      "[ Epoch 144, Val ] | Loss:3.46581 Time:0.004685\n",
      "[ Epoch 145, Train ] | Loss:2.94621 Time:0.047660\n",
      "[ Epoch 145, Val ] | Loss:3.46950 Time:0.004707\n",
      "[ Epoch 146, Train ] | Loss:2.94624 Time:0.050067\n",
      "[ Epoch 146, Val ] | Loss:3.46784 Time:0.004600\n",
      "[ Epoch 147, Train ] | Loss:2.94624 Time:0.047642\n",
      "[ Epoch 147, Val ] | Loss:3.46947 Time:0.004689\n",
      "[ Epoch 148, Train ] | Loss:2.94630 Time:0.047626\n",
      "[ Epoch 148, Val ] | Loss:3.46892 Time:0.004697\n",
      "[ Epoch 149, Train ] | Loss:2.94627 Time:0.050017\n",
      "[ Epoch 149, Val ] | Loss:3.46766 Time:0.004694\n",
      "[ Epoch 150, Train ] | Loss:2.94620 Time:0.047625\n",
      "[ Epoch 150, Val ] | Loss:3.46850 Time:0.004633\n",
      "[ Epoch 151, Train ] | Loss:2.94618 Time:0.047610\n",
      "[ Epoch 151, Val ] | Loss:3.46810 Time:0.004690\n",
      "[ Epoch 152, Train ] | Loss:2.94642 Time:0.047560\n",
      "[ Epoch 152, Val ] | Loss:3.46340 Time:0.004694\n",
      "[ Epoch 153, Train ] | Loss:2.94617 Time:0.047543\n",
      "[ Epoch 153, Val ] | Loss:3.46465 Time:0.004696\n",
      "[ Epoch 154, Train ] | Loss:2.94621 Time:0.049943\n",
      "[ Epoch 154, Val ] | Loss:3.46741 Time:0.004636\n",
      "[ Epoch 155, Train ] | Loss:2.94622 Time:0.047542\n",
      "[ Epoch 155, Val ] | Loss:3.46716 Time:0.004686\n",
      "[ Epoch 156, Train ] | Loss:2.94630 Time:0.047505\n",
      "[ Epoch 156, Val ] | Loss:3.46532 Time:0.004679\n",
      "[ Epoch 157, Train ] | Loss:2.94617 Time:0.047545\n",
      "[ Epoch 157, Val ] | Loss:3.46891 Time:0.004698\n",
      "[ Epoch 158, Train ] | Loss:2.94625 Time:0.049860\n",
      "[ Epoch 158, Val ] | Loss:3.46913 Time:0.004636\n",
      "[ Epoch 159, Train ] | Loss:2.94626 Time:0.047459\n",
      "[ Epoch 159, Val ] | Loss:3.47026 Time:0.004692\n",
      "[ Epoch 160, Train ] | Loss:2.94635 Time:0.047479\n",
      "[ Epoch 160, Val ] | Loss:3.46988 Time:0.004700\n",
      "[ Epoch 161, Train ] | Loss:2.94624 Time:0.047452\n",
      "[ Epoch 161, Val ] | Loss:3.47290 Time:0.004694\n",
      "[ Epoch 162, Train ] | Loss:2.94636 Time:0.049827\n",
      "[ Epoch 162, Val ] | Loss:3.47292 Time:0.004636\n",
      "[ Epoch 163, Train ] | Loss:2.94624 Time:0.047423\n",
      "[ Epoch 163, Val ] | Loss:3.47261 Time:0.004697\n",
      "[ Epoch 164, Train ] | Loss:2.94628 Time:0.047389\n",
      "[ Epoch 164, Val ] | Loss:3.47359 Time:0.004278\n",
      "[ Epoch 165, Train ] | Loss:2.94630 Time:0.049852\n",
      "[ Epoch 165, Val ] | Loss:3.47076 Time:0.005437\n",
      "[ Epoch 166, Train ] | Loss:2.94642 Time:0.051672\n",
      "[ Epoch 166, Val ] | Loss:3.47427 Time:0.003888\n",
      "[ Epoch 167, Train ] | Loss:2.94643 Time:0.049919\n",
      "[ Epoch 167, Val ] | Loss:3.47532 Time:0.004183\n",
      "[ Epoch 168, Train ] | Loss:2.94631 Time:0.050958\n",
      "[ Epoch 168, Val ] | Loss:3.47085 Time:0.004926\n",
      "[ Epoch 169, Train ] | Loss:2.94662 Time:0.053224\n",
      "[ Epoch 169, Val ] | Loss:3.47130 Time:0.004174\n",
      "[ Epoch 170, Train ] | Loss:2.94637 Time:0.054007\n",
      "[ Epoch 170, Val ] | Loss:3.47195 Time:0.003628\n",
      "[ Epoch 171, Train ] | Loss:2.94628 Time:0.049506\n",
      "[ Epoch 171, Val ] | Loss:3.47090 Time:0.004689\n",
      "[ Epoch 172, Train ] | Loss:2.94642 Time:0.047298\n",
      "[ Epoch 172, Val ] | Loss:3.47191 Time:0.004691\n",
      "[ Epoch 173, Train ] | Loss:2.94634 Time:0.047304\n",
      "[ Epoch 173, Val ] | Loss:3.47665 Time:0.004698\n",
      "[ Epoch 174, Train ] | Loss:2.94642 Time:0.047306\n",
      "[ Epoch 174, Val ] | Loss:3.47472 Time:0.004637\n",
      "[ Epoch 175, Train ] | Loss:2.94630 Time:0.047287\n",
      "[ Epoch 175, Val ] | Loss:3.47614 Time:0.004681\n",
      "[ Epoch 176, Train ] | Loss:2.94625 Time:0.047258\n",
      "[ Epoch 176, Val ] | Loss:3.47743 Time:0.004686\n",
      "[ Epoch 177, Train ] | Loss:2.94638 Time:0.051789\n",
      "[ Epoch 177, Val ] | Loss:3.47945 Time:0.004610\n",
      "[ Epoch 178, Train ] | Loss:2.94639 Time:0.047230\n",
      "[ Epoch 178, Val ] | Loss:3.47763 Time:0.004613\n",
      "[ Epoch 179, Train ] | Loss:2.94635 Time:0.047265\n",
      "[ Epoch 179, Val ] | Loss:3.47982 Time:0.004685\n",
      "[ Epoch 180, Train ] | Loss:2.94636 Time:0.047216\n",
      "[ Epoch 180, Val ] | Loss:3.47956 Time:0.004693\n",
      "[ Epoch 181, Train ] | Loss:2.94632 Time:0.047210\n",
      "[ Epoch 181, Val ] | Loss:3.47924 Time:0.004688\n",
      "[ Epoch 182, Train ] | Loss:2.94643 Time:0.051724\n",
      "[ Epoch 182, Val ] | Loss:3.48099 Time:0.004609\n",
      "[ Epoch 183, Train ] | Loss:2.94656 Time:0.047189\n",
      "[ Epoch 183, Val ] | Loss:3.48021 Time:0.004689\n",
      "[ Epoch 184, Train ] | Loss:2.94639 Time:0.047154\n",
      "[ Epoch 184, Val ] | Loss:3.48427 Time:0.004683\n",
      "[ Epoch 185, Train ] | Loss:2.94649 Time:0.051332\n",
      "[ Epoch 185, Val ] | Loss:3.48808 Time:0.004676\n",
      "[ Epoch 186, Train ] | Loss:2.94652 Time:0.047139\n",
      "[ Epoch 186, Val ] | Loss:3.48842 Time:0.004627\n",
      "[ Epoch 187, Train ] | Loss:2.94642 Time:0.047129\n",
      "[ Epoch 187, Val ] | Loss:3.49025 Time:0.004679\n",
      "[ Epoch 188, Train ] | Loss:2.94635 Time:0.047098\n",
      "[ Epoch 188, Val ] | Loss:3.48865 Time:0.004697\n",
      "[ Epoch 189, Train ] | Loss:2.94652 Time:0.049159\n",
      "[ Epoch 189, Val ] | Loss:3.49017 Time:0.004694\n",
      "[ Epoch 190, Train ] | Loss:2.94640 Time:0.051275\n",
      "[ Epoch 190, Val ] | Loss:3.49169 Time:0.004643\n",
      "[ Epoch 191, Train ] | Loss:2.94674 Time:0.047104\n",
      "[ Epoch 191, Val ] | Loss:3.49428 Time:0.004690\n",
      "[ Epoch 192, Train ] | Loss:2.94650 Time:0.049122\n",
      "[ Epoch 192, Val ] | Loss:3.48845 Time:0.004680\n",
      "[ Epoch 193, Train ] | Loss:2.94644 Time:0.051521\n",
      "[ Epoch 193, Val ] | Loss:3.49110 Time:0.004688\n",
      "[ Epoch 194, Train ] | Loss:2.94646 Time:0.051511\n",
      "[ Epoch 194, Val ] | Loss:3.48854 Time:0.004641\n",
      "[ Epoch 195, Train ] | Loss:2.94640 Time:0.049069\n",
      "[ Epoch 195, Val ] | Loss:3.48928 Time:0.004692\n",
      "[ Epoch 196, Train ] | Loss:2.94646 Time:0.049050\n",
      "[ Epoch 196, Val ] | Loss:3.48609 Time:0.004689\n",
      "[ Epoch 197, Train ] | Loss:2.94659 Time:0.049078\n",
      "[ Epoch 197, Val ] | Loss:3.48856 Time:0.004679\n",
      "[ Epoch 198, Train ] | Loss:2.94648 Time:0.051451\n",
      "[ Epoch 198, Val ] | Loss:3.49029 Time:0.004601\n",
      "[ Epoch 199, Train ] | Loss:2.94633 Time:0.053844\n",
      "[ Epoch 199, Val ] | Loss:3.49282 Time:0.004196\n",
      "[ Epoch 200, Train ] | Loss:2.94653 Time:0.055640\n",
      "[ Epoch 200, Val ] | Loss:3.49059 Time:0.004196\n",
      "Finish training!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    fc_network.train()\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    start_time = time.time()\n",
    "    train_log = {}  # save training logs\n",
    "\n",
    "    lr_message = 'learning rate of epoch {}: {}'.format(epoch + 1, learning_rate)\n",
    "    train_log['lr_message'] = lr_message\n",
    "\n",
    "    # training\n",
    "    for i, batch_data in enumerate(train_loader, 1):\n",
    "        inputs = batch_data[0].to(device)\n",
    "        labels = batch_data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = fc_network(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "    epoch_train_loss = np.mean(train_loss)\n",
    "    train_message = '[ Epoch {}, Train ] | Loss:{:.5f} Time:{:.6f}'.format(epoch + 1,\n",
    "                                                                        epoch_train_loss,\n",
    "                                                                        time.time() - start_time)\n",
    "    print(train_message)\n",
    "\n",
    "    # validation\n",
    "    fc_network.eval()\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, batch_data in enumerate(val_loader, 1):\n",
    "            inputs = batch_data[0].to(device)\n",
    "            labels = batch_data[1].to(device)\n",
    "\n",
    "            outputs = fc_network(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "    epoch_val_loss = np.mean(val_loss)\n",
    "    val_message = '[ Epoch {}, Val ] | Loss:{:.5f} Time:{:.6f}'.format(epoch + 1, epoch_val_loss,\n",
    "                                                                         time.time() - start_time)\n",
    "    print(val_message)\n",
    "    flag = False\n",
    "    if epoch_val_loss < best_loss:\n",
    "        best_loss = epoch_val_loss\n",
    "        save_message = 'save model with val loss {:.3f}'.format(best_loss)\n",
    "        print(save_message)\n",
    "        flag = True\n",
    "        torch.save(fc_network, \"{}/{}.pt\".format(model_dir, 'fc_network'))\n",
    "\n",
    "        # if self.lr_decay:\n",
    "        #     self.lr_scheduler.step()\n",
    "\n",
    "    train_log[\"epoch\"] = epoch + 1\n",
    "    train_log[\"train_message\"] = train_message\n",
    "    train_log[\"val_message\"] = val_message\n",
    "    train_log[\"epoch_train_loss\"] = epoch_train_loss\n",
    "    train_log[\"epoch_val_loss\"] = epoch_val_loss\n",
    "    if flag:\n",
    "        train_log[\"save_message\"] = save_message\n",
    "    logs.append(train_log)\n",
    "    with open(log_path, \"w\") as fp:\n",
    "        json.dump(logs, fp)\n",
    "print(\"Finish training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "344bdea3-8c5d-45fd-bd16-216e29a403d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin testing\n",
      "Test loss: 3.479562282562256\n",
      "——————Test——————\n",
      "           class precision  recall f1-score support\n",
      "0              0    0.7500  0.6000   0.6667       5\n",
      "1              1    0.6667  0.4000   0.5000       5\n",
      "2              2    0.2222  0.4000   0.2857       5\n",
      "3              3    0.6667  0.4000   0.5000       5\n",
      "4              4    0.5000  0.2000   0.2857       5\n",
      "5              5    0.7500  0.6000   0.6667       5\n",
      "6              6    0.5000  0.4000   0.4444       5\n",
      "7              7    0.8000  0.8000   0.8000       5\n",
      "8              8    0.0000  0.0000   0.0000       5\n",
      "9              9    1.0000  0.6000   0.7500       5\n",
      "10            10    1.0000  0.8000   0.8889       5\n",
      "11            11    0.6667  0.4000   0.5000       5\n",
      "12            12    0.7500  0.6000   0.6667       5\n",
      "13            13    1.0000  0.6000   0.7500       5\n",
      "14            14    0.0000  0.0000   0.0000       5\n",
      "15            15    0.4000  0.8000   0.5333       5\n",
      "16            16    0.5000  0.8000   0.6154       5\n",
      "17            17    0.0000  0.0000   0.0000       5\n",
      "18            18    0.3333  0.4000   0.3636       5\n",
      "19            19    0.8000  0.8000   0.8000       5\n",
      "20            20    0.6667  0.8000   0.7273       5\n",
      "21            21    0.5000  0.6000   0.5455       5\n",
      "22            22    1.0000  0.2000   0.3333       5\n",
      "23            23    0.3333  0.2000   0.2500       5\n",
      "24            24    0.5000  0.2000   0.2857       5\n",
      "25            25    0.5556  1.0000   0.7143       5\n",
      "26            26    0.5000  0.8000   0.6154       5\n",
      "27            27    0.6250  1.0000   0.7692       5\n",
      "28            28    0.5000  0.4000   0.4444       5\n",
      "29            29    0.2000  0.2000   0.2000       5\n",
      "30            30    0.5000  0.6000   0.5455       5\n",
      "31            31    1.0000  0.2000   0.3333       5\n",
      "32            32    0.5000  0.4000   0.4444       5\n",
      "33            33    0.6000  0.6000   0.6000       5\n",
      "34            34    1.0000  0.4000   0.5714       5\n",
      "35            35    1.0000  0.8000   0.8889       5\n",
      "36            36    1.0000  0.4000   0.5714       5\n",
      "37            37    1.0000  0.4000   0.5714       5\n",
      "38            38    0.5000  0.8000   0.6154       5\n",
      "39            39    0.6250  1.0000   0.7692       5\n",
      "40            40    0.1667  0.2000   0.1818       5\n",
      "41            41    0.8000  0.8000   0.8000       5\n",
      "42            42    0.6000  0.6000   0.6000       5\n",
      "43            43    0.6250  1.0000   0.7692       5\n",
      "44            44    0.6667  0.4000   0.5000       5\n",
      "45            45    0.3000  0.6000   0.4000       5\n",
      "46            46    0.1429  0.4000   0.2105       5\n",
      "47            47    0.4000  0.8000   0.5333       5\n",
      "48            48    1.0000  0.8000   0.8889       5\n",
      "49            49    1.0000  0.2000   0.3333       5\n",
      "50      accuracy                     0.5280     250\n",
      "51     macro avg    0.6022  0.5280   0.5206     250\n",
      "52  weighted avg    0.6022  0.5280   0.5206     250\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"{}/{}.pt\".format(model_dir, 'fc_network'))\n",
    "predictions = []\n",
    "y_labels = []\n",
    "test_loss = []\n",
    "\n",
    "print(\"Begin testing\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, labels) in enumerate(test_loader, 1):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        test_loss.append(loss.item())\n",
    "        pred = outputs.argmax(dim=1)\n",
    "        predictions.extend(pred.cpu().numpy().tolist())\n",
    "        y_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    epoch_test_loss = np.mean(test_loss)\n",
    "    test_message = \"Test loss: {}\".format(epoch_test_loss)\n",
    "    report = classification_report(y_labels, predictions, digits=4, zero_division=1)\n",
    "    print(test_message)\n",
    "    \n",
    "# make the classification report more readable\n",
    "report = report.splitlines()\n",
    "columns = ['class'] + report[0].split()\n",
    "col_1, col_2, col_3, col_4, col_5 = [], [], [], [], []\n",
    "for row in report[1:]:\n",
    "    if len(row.split()) != 0:\n",
    "        row = row.split()\n",
    "        if len(row) < 5:\n",
    "            col_1.append(row[0])\n",
    "            col_2.append('')\n",
    "            col_3.append('')\n",
    "            col_4.append(row[1])\n",
    "            col_5.append(row[2])\n",
    "        elif len(row) > 5:\n",
    "            col_1.append(row[0] + ' ' + row[1])\n",
    "            col_2.append(row[2])\n",
    "            col_3.append(row[3])\n",
    "            col_4.append(row[4])\n",
    "            col_5.append(row[5])\n",
    "        else:\n",
    "            col_1.append(row[0])\n",
    "            col_2.append(row[1])\n",
    "            col_3.append(row[2])\n",
    "            col_4.append(row[3])\n",
    "            col_5.append(row[4])\n",
    "result = pd.DataFrame()\n",
    "result[columns[0]] = col_1\n",
    "result[columns[1]] = col_2\n",
    "result[columns[2]] = col_3\n",
    "result[columns[3]] = col_4\n",
    "result[columns[4]] = col_5\n",
    "print(\"——————Test——————\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f52964-e714-4486-b80c-3394413d51a6",
   "metadata": {},
   "source": [
    "# 7. Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f3e7a6c-bddc-419e-8437-c7c9ce899ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    '''\n",
    "    Trainer class\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, model, epochs, train_dataloader, val_dataloader, test_dataloader,\n",
    "                 criterion, optimizer, lr, device, model_dir, model_name, random_seed):\n",
    "        '''\n",
    "        parameters:\n",
    "        - model: torch.nn.module, model to be trained\n",
    "        - epochs: number of epochs\n",
    "        - train_dataloader: DataLoader that contains training data\n",
    "        - val_dataloader:  DataLoader that contains validation data\n",
    "        - test_dataloader:  DataLoader that contains test data\n",
    "        - criterion: loss function, expected to be torch.nn.CrossEntropyLoss()\n",
    "        - optimizer: optimizer used in training, expected to be classes in torch.optim\n",
    "        - device: the training device, expected to be torch.device\n",
    "        - model_dir: path to save the model\n",
    "        - model_name: model name, str\n",
    "        - random_seed: random seed, int\n",
    "        '''\n",
    "        \n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.model_dir = model_dir\n",
    "        self.model_name = model_name\n",
    "        self.model.to(self.device)\n",
    "        self.log_path = os.path.join(self.model_dir, \"train_log_{}.json\".format(self.model_name))\n",
    "        self.random_seed = random_seed\n",
    "        \n",
    "    def train(self):\n",
    "        '''\n",
    "        Train the model.\n",
    "        During each epoch:\n",
    "        - save the training, validation message (including the loss, the learning rate and time) into a json file.\n",
    "        - save the model if the validation loss is the best.\n",
    "        '''\n",
    "        torch.random.manual_seed(self.random_seed)\n",
    "        torch.cuda.random.manual_seed(self.random_seed)\n",
    "        \n",
    "        trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        total = sum(param.numel() for param in self.model.parameters())\n",
    "        model_para = 'total parameters: {}, trainable parameters: {}'.format(total, trainable)\n",
    "        print(model_para)\n",
    "        \n",
    "        optimizer_param = self.optimizer.state_dict()['param_groups'][0]\n",
    "        train_setting = '''\n",
    "        initial learning rate: {}, weight_decay: {},\n",
    "        total epochs: {}\n",
    "        '''.format(optimizer_param['lr'], optimizer_param['weight_decay'],\n",
    "                   self.epochs)\n",
    "        print(train_setting)\n",
    "        \n",
    "        begin_message = \"Begin training, total epochs: {}\".format(self.epochs)\n",
    "        print(begin_message)\n",
    "        \n",
    "        logs = [model_para, train_setting, begin_message]  # to save all information\n",
    "        best_loss = 10\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            train_loss = []\n",
    "            val_loss = []\n",
    "            start_time = time.time()\n",
    "            train_log = {}  # save training logs\n",
    "\n",
    "            lr_message = 'learning rate of epoch {}: {}'.format(epoch + 1, optimizer_param['lr'])\n",
    "            train_log['lr_message'] = lr_message\n",
    "\n",
    "            # training\n",
    "            for i, batch_data in enumerate(self.train_dataloader, 1):\n",
    "                inputs = batch_data[0].to(self.device)\n",
    "                labels = batch_data[1].to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "            epoch_train_loss = np.mean(train_loss)\n",
    "            train_message = '[ Epoch {}, Train ] | Loss:{:.5f} Time:{:.6f}'.format(epoch + 1,\n",
    "                                                                                   epoch_train_loss,\n",
    "                                                                                   time.time() - start_time)\n",
    "            print(train_message)\n",
    "\n",
    "            # validation\n",
    "            self.model.eval()\n",
    "            start_time = time.time()\n",
    "            with torch.no_grad():\n",
    "                for i, batch_data in enumerate(self.val_dataloader, 1):\n",
    "                    inputs = batch_data[0].to(self.device)\n",
    "                    labels = batch_data[1].to(self.device)\n",
    "\n",
    "                    outputs = self.model(inputs)\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "\n",
    "                    val_loss.append(loss.item())\n",
    "\n",
    "            epoch_val_loss = np.mean(val_loss)\n",
    "            val_message = '[ Epoch {}, Val ] | Loss:{:.5f} Time:{:.6f}'.format(epoch + 1, epoch_val_loss,\n",
    "                                                                               time.time() - start_time)\n",
    "            print(val_message)\n",
    "            flag = False\n",
    "            if epoch_val_loss < best_loss:\n",
    "                best_loss = epoch_val_loss\n",
    "                save_message = 'save model with val loss {:.3f}'.format(best_loss)\n",
    "                print(save_message)\n",
    "                flag = True\n",
    "                torch.save(self.model, \"{}/{}.pt\".format(self.model_dir, self.model_name))\n",
    "\n",
    "\n",
    "            train_log[\"epoch\"] = epoch + 1\n",
    "            train_log[\"train_message\"] = train_message\n",
    "            train_log[\"val_message\"] = val_message\n",
    "            train_log[\"epoch_train_loss\"] = epoch_train_loss\n",
    "            train_log[\"epoch_val_loss\"] = epoch_val_loss\n",
    "            if flag:\n",
    "                train_log[\"save_message\"] = save_message\n",
    "            logs.append(train_log)\n",
    "            with open(self.log_path, \"w\") as fp:\n",
    "                json.dump(logs, fp)\n",
    "        print(\"Finish training!\")\n",
    "        \n",
    "        \n",
    "    def test(self):\n",
    "        torch.random.manual_seed(self.random_seed)\n",
    "        torch.cuda.random.manual_seed(self.random_seed)\n",
    "        model = torch.load(\"{}/{}.pt\".format(self.model_dir, self.model_name))\n",
    "        predictions = []\n",
    "        y_labels = []\n",
    "        test_loss = []\n",
    "\n",
    "        print(\"Begin testing\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, labels) in enumerate(self.test_dataloader, 1):\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "\n",
    "                test_loss.append(loss.item())\n",
    "                pred = outputs.argmax(dim=1)\n",
    "                predictions.extend(pred.cpu().numpy().tolist())\n",
    "                y_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "        epoch_test_loss = np.mean(test_loss)\n",
    "        test_message = \"Test loss: {}\".format(epoch_test_loss)\n",
    "        report = classification_report(y_labels, predictions, digits=4, zero_division=1)\n",
    "        print(test_message)\n",
    "\n",
    "        # make the classification report more readable\n",
    "        report = report.splitlines()\n",
    "        columns = ['class'] + report[0].split()\n",
    "        col_1, col_2, col_3, col_4, col_5 = [], [], [], [], []\n",
    "        for row in report[1:]:\n",
    "            if len(row.split()) != 0:\n",
    "                row = row.split()\n",
    "                if len(row) < 5:\n",
    "                    col_1.append(row[0])\n",
    "                    col_2.append('')\n",
    "                    col_3.append('')\n",
    "                    col_4.append(row[1])\n",
    "                    col_5.append(row[2])\n",
    "                elif len(row) > 5:\n",
    "                    col_1.append(row[0] + ' ' + row[1])\n",
    "                    col_2.append(row[2])\n",
    "                    col_3.append(row[3])\n",
    "                    col_4.append(row[4])\n",
    "                    col_5.append(row[5])\n",
    "                else:\n",
    "                    col_1.append(row[0])\n",
    "                    col_2.append(row[1])\n",
    "                    col_3.append(row[2])\n",
    "                    col_4.append(row[3])\n",
    "                    col_5.append(row[4])\n",
    "        result = pd.DataFrame()\n",
    "        result[columns[0]] = col_1\n",
    "        result[columns[1]] = col_2\n",
    "        result[columns[2]] = col_3\n",
    "        result[columns[3]] = col_4\n",
    "        result[columns[4]] = col_5\n",
    "        print(\"——————Test——————\")\n",
    "        print(result)\n",
    "\n",
    "        # save results\n",
    "        result.to_csv(\"{}/{}_{}.csv\".format(self.model_dir, 'result', self.model_name), index=False)\n",
    "\n",
    "        with open(self.log_path, \"r\") as fp:\n",
    "            logs = json.load(fp)\n",
    "        logs.append(test_message)\n",
    "        with open(self.log_path, \"w\") as fp:\n",
    "            json.dump(logs, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "a815503b-d145-4050-ac4f-a9bf54b0a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_data = ImgDataset(x_train.reshape(-1,1,28,28), y_train)\n",
    "val_data = ImgDataset(x_val.reshape(-1,1,28,28), y_val)\n",
    "test_data = ImgDataset(test_image.reshape(-1,1,28,28), test_label)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da79d388-8260-4541-bc29-824646d48610",
   "metadata": {},
   "source": [
    "## 7.1 New CNN Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "27acb1a9-5600-49fa-82d9-50d02d5dcf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_classifier, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, 1, 1), # input channel, ouput channel, filter size, stride, padding\n",
    "            nn.BatchNorm2d(64), # num_features\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0), # kernel size, stride, padding\n",
    "        \n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "            \n",
    "            nn.Conv2d(256, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "        \n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 50)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1) # resize, first axis is batch_num\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8885e35d-8778-40ea-9c11-0377e6c8980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cnn = CNN_classifier()\n",
    "epochs = 300\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-3\n",
    "optimizer = Adam(new_cnn.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_dir = './output'\n",
    "model_name = 'new_cnn'\n",
    "random_seed = 2022\n",
    "trainer = Trainer(model=new_cnn, \n",
    "                  epochs=epochs, \n",
    "                  train_dataloader=train_loader, \n",
    "                  val_dataloader=val_loader, \n",
    "                  test_dataloader=test_loader,\n",
    "                  criterion=criterion, \n",
    "                  optimizer=optimizer, \n",
    "                  lr=learning_rate, \n",
    "                  device=device, \n",
    "                  model_dir=model_dir, \n",
    "                  model_name=model_name, \n",
    "                  random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8d2ba228-0b03-4108-871a-4ef5efa66cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "CNN_classifier                           --\n",
       "├─Sequential: 1-1                        --\n",
       "│    └─Conv2d: 2-1                       640\n",
       "│    └─BatchNorm2d: 2-2                  128\n",
       "│    └─ReLU: 2-3                         --\n",
       "│    └─MaxPool2d: 2-4                    --\n",
       "│    └─Conv2d: 2-5                       73,856\n",
       "│    └─BatchNorm2d: 2-6                  256\n",
       "│    └─ReLU: 2-7                         --\n",
       "│    └─MaxPool2d: 2-8                    --\n",
       "│    └─Conv2d: 2-9                       295,168\n",
       "│    └─BatchNorm2d: 2-10                 512\n",
       "│    └─ReLU: 2-11                        --\n",
       "│    └─MaxPool2d: 2-12                   --\n",
       "│    └─Conv2d: 2-13                      1,180,160\n",
       "│    └─BatchNorm2d: 2-14                 1,024\n",
       "│    └─ReLU: 2-15                        --\n",
       "│    └─MaxPool2d: 2-16                   --\n",
       "├─Sequential: 1-2                        --\n",
       "│    └─Linear: 2-17                      131,328\n",
       "│    └─ReLU: 2-18                        --\n",
       "│    └─Linear: 2-19                      65,792\n",
       "│    └─ReLU: 2-20                        --\n",
       "│    └─Dropout: 2-21                     --\n",
       "│    └─Linear: 2-22                      12,850\n",
       "=================================================================\n",
       "Total params: 1,761,714\n",
       "Trainable params: 1,761,714\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(new_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "d9e14131-2e91-415d-8bf2-f115faa6f11a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameters: 1761714, trainable parameters: 1761714\n",
      "\n",
      "        initial learning rate: 0.001, weight_decay: 0.001,\n",
      "        total epochs: 300\n",
      "        \n",
      "Begin training, total epochs: 300\n",
      "[ Epoch 1, Train ] | Loss:3.87088 Time:0.085282\n",
      "[ Epoch 1, Val ] | Loss:3.91502 Time:0.005479\n",
      "save model with val loss 3.915\n",
      "[ Epoch 2, Train ] | Loss:3.52894 Time:0.082133\n",
      "[ Epoch 2, Val ] | Loss:4.12573 Time:0.005498\n",
      "[ Epoch 3, Train ] | Loss:2.92031 Time:0.083113\n",
      "[ Epoch 3, Val ] | Loss:5.09796 Time:0.005361\n",
      "[ Epoch 4, Train ] | Loss:2.23953 Time:0.083149\n",
      "[ Epoch 4, Val ] | Loss:5.95583 Time:0.005453\n",
      "[ Epoch 5, Train ] | Loss:1.65485 Time:0.083251\n",
      "[ Epoch 5, Val ] | Loss:6.58466 Time:0.005432\n",
      "[ Epoch 6, Train ] | Loss:1.13240 Time:0.083021\n",
      "[ Epoch 6, Val ] | Loss:4.95955 Time:0.005553\n",
      "[ Epoch 7, Train ] | Loss:0.82182 Time:0.083182\n",
      "[ Epoch 7, Val ] | Loss:1.64748 Time:0.005562\n",
      "save model with val loss 1.647\n",
      "[ Epoch 8, Train ] | Loss:0.55160 Time:0.084462\n",
      "[ Epoch 8, Val ] | Loss:2.84550 Time:0.005342\n",
      "[ Epoch 9, Train ] | Loss:0.37973 Time:0.082812\n",
      "[ Epoch 9, Val ] | Loss:1.10522 Time:0.005569\n",
      "save model with val loss 1.105\n",
      "[ Epoch 10, Train ] | Loss:0.25427 Time:0.084228\n",
      "[ Epoch 10, Val ] | Loss:1.04185 Time:0.005351\n",
      "save model with val loss 1.042\n",
      "[ Epoch 11, Train ] | Loss:0.18293 Time:0.084215\n",
      "[ Epoch 11, Val ] | Loss:1.34936 Time:0.005525\n",
      "[ Epoch 12, Train ] | Loss:0.12397 Time:0.082956\n",
      "[ Epoch 12, Val ] | Loss:0.91744 Time:0.005560\n",
      "save model with val loss 0.917\n",
      "[ Epoch 13, Train ] | Loss:0.11276 Time:0.084519\n",
      "[ Epoch 13, Val ] | Loss:0.44080 Time:0.005390\n",
      "save model with val loss 0.441\n",
      "[ Epoch 14, Train ] | Loss:0.07920 Time:0.082903\n",
      "[ Epoch 14, Val ] | Loss:0.44062 Time:0.005383\n",
      "save model with val loss 0.441\n",
      "[ Epoch 15, Train ] | Loss:0.07503 Time:0.084279\n",
      "[ Epoch 15, Val ] | Loss:0.46638 Time:0.005280\n",
      "[ Epoch 16, Train ] | Loss:0.06078 Time:0.082218\n",
      "[ Epoch 16, Val ] | Loss:0.71492 Time:0.005427\n",
      "[ Epoch 17, Train ] | Loss:0.05718 Time:0.082798\n",
      "[ Epoch 17, Val ] | Loss:0.63304 Time:0.005428\n",
      "[ Epoch 18, Train ] | Loss:0.04401 Time:0.082206\n",
      "[ Epoch 18, Val ] | Loss:0.38896 Time:0.005379\n",
      "save model with val loss 0.389\n",
      "[ Epoch 19, Train ] | Loss:0.02926 Time:0.084427\n",
      "[ Epoch 19, Val ] | Loss:0.64555 Time:0.005360\n",
      "[ Epoch 20, Train ] | Loss:0.03312 Time:0.082523\n",
      "[ Epoch 20, Val ] | Loss:0.39198 Time:0.005431\n",
      "[ Epoch 21, Train ] | Loss:0.02832 Time:0.084754\n",
      "[ Epoch 21, Val ] | Loss:0.50556 Time:0.005538\n",
      "[ Epoch 22, Train ] | Loss:0.03174 Time:0.082865\n",
      "[ Epoch 22, Val ] | Loss:0.39695 Time:0.005437\n",
      "[ Epoch 23, Train ] | Loss:0.03006 Time:0.082697\n",
      "[ Epoch 23, Val ] | Loss:0.44468 Time:0.005557\n",
      "[ Epoch 24, Train ] | Loss:0.02514 Time:0.082837\n",
      "[ Epoch 24, Val ] | Loss:0.48193 Time:0.005439\n",
      "[ Epoch 25, Train ] | Loss:0.02379 Time:0.082894\n",
      "[ Epoch 25, Val ] | Loss:0.47360 Time:0.005424\n",
      "[ Epoch 26, Train ] | Loss:0.02314 Time:0.082529\n",
      "[ Epoch 26, Val ] | Loss:0.46724 Time:0.005372\n",
      "[ Epoch 27, Train ] | Loss:0.02110 Time:0.082753\n",
      "[ Epoch 27, Val ] | Loss:0.41863 Time:0.005444\n",
      "[ Epoch 28, Train ] | Loss:0.01520 Time:0.082833\n",
      "[ Epoch 28, Val ] | Loss:0.55556 Time:0.005433\n",
      "[ Epoch 29, Train ] | Loss:0.01180 Time:0.082639\n",
      "[ Epoch 29, Val ] | Loss:0.47142 Time:0.005375\n",
      "[ Epoch 30, Train ] | Loss:0.01410 Time:0.082602\n",
      "[ Epoch 30, Val ] | Loss:0.35042 Time:0.005544\n",
      "save model with val loss 0.350\n",
      "[ Epoch 31, Train ] | Loss:0.01200 Time:0.083676\n",
      "[ Epoch 31, Val ] | Loss:0.50103 Time:0.005360\n",
      "[ Epoch 32, Train ] | Loss:0.00878 Time:0.084595\n",
      "[ Epoch 32, Val ] | Loss:0.45314 Time:0.005553\n",
      "[ Epoch 33, Train ] | Loss:0.00645 Time:0.082737\n",
      "[ Epoch 33, Val ] | Loss:0.41816 Time:0.005449\n",
      "[ Epoch 34, Train ] | Loss:0.00631 Time:0.082723\n",
      "[ Epoch 34, Val ] | Loss:0.38731 Time:0.005567\n",
      "[ Epoch 35, Train ] | Loss:0.00721 Time:0.082686\n",
      "[ Epoch 35, Val ] | Loss:0.32285 Time:0.005554\n",
      "save model with val loss 0.323\n",
      "[ Epoch 36, Train ] | Loss:0.00409 Time:0.083341\n",
      "[ Epoch 36, Val ] | Loss:0.38270 Time:0.005375\n",
      "[ Epoch 37, Train ] | Loss:0.00566 Time:0.084600\n",
      "[ Epoch 37, Val ] | Loss:0.38513 Time:0.005422\n",
      "[ Epoch 38, Train ] | Loss:0.00285 Time:0.084612\n",
      "[ Epoch 38, Val ] | Loss:0.38671 Time:0.005553\n",
      "[ Epoch 39, Train ] | Loss:0.00375 Time:0.082700\n",
      "[ Epoch 39, Val ] | Loss:0.30247 Time:0.005393\n",
      "save model with val loss 0.302\n",
      "[ Epoch 40, Train ] | Loss:0.00285 Time:0.084008\n",
      "[ Epoch 40, Val ] | Loss:0.32575 Time:0.005513\n",
      "[ Epoch 41, Train ] | Loss:0.00349 Time:0.086694\n",
      "[ Epoch 41, Val ] | Loss:0.40581 Time:0.005197\n",
      "[ Epoch 42, Train ] | Loss:0.00288 Time:0.084752\n",
      "[ Epoch 42, Val ] | Loss:0.33806 Time:0.005425\n",
      "[ Epoch 43, Train ] | Loss:0.00355 Time:0.084500\n",
      "[ Epoch 43, Val ] | Loss:0.26026 Time:0.005589\n",
      "save model with val loss 0.260\n",
      "[ Epoch 44, Train ] | Loss:0.00308 Time:0.083339\n",
      "[ Epoch 44, Val ] | Loss:0.25457 Time:0.005336\n",
      "save model with val loss 0.255\n",
      "[ Epoch 45, Train ] | Loss:0.00631 Time:0.083717\n",
      "[ Epoch 45, Val ] | Loss:0.34789 Time:0.005373\n",
      "[ Epoch 46, Train ] | Loss:0.00937 Time:0.086785\n",
      "[ Epoch 46, Val ] | Loss:0.49689 Time:0.005302\n",
      "[ Epoch 47, Train ] | Loss:0.01587 Time:0.084383\n",
      "[ Epoch 47, Val ] | Loss:0.83934 Time:0.005445\n",
      "[ Epoch 48, Train ] | Loss:0.03754 Time:0.084509\n",
      "[ Epoch 48, Val ] | Loss:0.95840 Time:0.005439\n",
      "[ Epoch 49, Train ] | Loss:0.08076 Time:0.084621\n",
      "[ Epoch 49, Val ] | Loss:1.03502 Time:0.005373\n",
      "[ Epoch 50, Train ] | Loss:0.07657 Time:0.084431\n",
      "[ Epoch 50, Val ] | Loss:0.62109 Time:0.005446\n",
      "[ Epoch 51, Train ] | Loss:0.08296 Time:0.084420\n",
      "[ Epoch 51, Val ] | Loss:1.80880 Time:0.005448\n",
      "[ Epoch 52, Train ] | Loss:0.11112 Time:0.084608\n",
      "[ Epoch 52, Val ] | Loss:1.52728 Time:0.005374\n",
      "[ Epoch 53, Train ] | Loss:0.08596 Time:0.084598\n",
      "[ Epoch 53, Val ] | Loss:1.38719 Time:0.005443\n",
      "[ Epoch 54, Train ] | Loss:0.04137 Time:0.084636\n",
      "[ Epoch 54, Val ] | Loss:1.29359 Time:0.005436\n",
      "[ Epoch 55, Train ] | Loss:0.03733 Time:0.083358\n",
      "[ Epoch 55, Val ] | Loss:0.60389 Time:0.005381\n",
      "[ Epoch 56, Train ] | Loss:0.02400 Time:0.084555\n",
      "[ Epoch 56, Val ] | Loss:0.49294 Time:0.005438\n",
      "[ Epoch 57, Train ] | Loss:0.02104 Time:0.084563\n",
      "[ Epoch 57, Val ] | Loss:0.70628 Time:0.005568\n",
      "[ Epoch 58, Train ] | Loss:0.01838 Time:0.082726\n",
      "[ Epoch 58, Val ] | Loss:0.83209 Time:0.005556\n",
      "[ Epoch 59, Train ] | Loss:0.01508 Time:0.084150\n",
      "[ Epoch 59, Val ] | Loss:0.40729 Time:0.005446\n",
      "[ Epoch 60, Train ] | Loss:0.01213 Time:0.084497\n",
      "[ Epoch 60, Val ] | Loss:0.78812 Time:0.005431\n",
      "[ Epoch 61, Train ] | Loss:0.00710 Time:0.084383\n",
      "[ Epoch 61, Val ] | Loss:0.41375 Time:0.005360\n",
      "[ Epoch 62, Train ] | Loss:0.00559 Time:0.084444\n",
      "[ Epoch 62, Val ] | Loss:0.45158 Time:0.005439\n",
      "[ Epoch 63, Train ] | Loss:0.00437 Time:0.084302\n",
      "[ Epoch 63, Val ] | Loss:0.28854 Time:0.005442\n",
      "[ Epoch 64, Train ] | Loss:0.00405 Time:0.084314\n",
      "[ Epoch 64, Val ] | Loss:0.30030 Time:0.005548\n",
      "[ Epoch 65, Train ] | Loss:0.00400 Time:0.084372\n",
      "[ Epoch 65, Val ] | Loss:0.33040 Time:0.005552\n",
      "[ Epoch 66, Train ] | Loss:0.00270 Time:0.084401\n",
      "[ Epoch 66, Val ] | Loss:0.37685 Time:0.005443\n",
      "[ Epoch 67, Train ] | Loss:0.00319 Time:0.084491\n",
      "[ Epoch 67, Val ] | Loss:0.35566 Time:0.005389\n",
      "[ Epoch 68, Train ] | Loss:0.00291 Time:0.084366\n",
      "[ Epoch 68, Val ] | Loss:0.29575 Time:0.005437\n",
      "[ Epoch 69, Train ] | Loss:0.00156 Time:0.084412\n",
      "[ Epoch 69, Val ] | Loss:0.31234 Time:0.005432\n",
      "[ Epoch 70, Train ] | Loss:0.00229 Time:0.084376\n",
      "[ Epoch 70, Val ] | Loss:0.37001 Time:0.005549\n",
      "[ Epoch 71, Train ] | Loss:0.00185 Time:0.084009\n",
      "[ Epoch 71, Val ] | Loss:0.37369 Time:0.005433\n",
      "[ Epoch 72, Train ] | Loss:0.00228 Time:0.084255\n",
      "[ Epoch 72, Val ] | Loss:0.33872 Time:0.005437\n",
      "[ Epoch 73, Train ] | Loss:0.00182 Time:0.084243\n",
      "[ Epoch 73, Val ] | Loss:0.30598 Time:0.005370\n",
      "[ Epoch 74, Train ] | Loss:0.00186 Time:0.084280\n",
      "[ Epoch 74, Val ] | Loss:0.27508 Time:0.005442\n",
      "[ Epoch 75, Train ] | Loss:0.00187 Time:0.082613\n",
      "[ Epoch 75, Val ] | Loss:0.28473 Time:0.005551\n",
      "[ Epoch 76, Train ] | Loss:0.00145 Time:0.084249\n",
      "[ Epoch 76, Val ] | Loss:0.31628 Time:0.005540\n",
      "[ Epoch 77, Train ] | Loss:0.00194 Time:0.084059\n",
      "[ Epoch 77, Val ] | Loss:0.34623 Time:0.005551\n",
      "[ Epoch 78, Train ] | Loss:0.00113 Time:0.084147\n",
      "[ Epoch 78, Val ] | Loss:0.33952 Time:0.005435\n",
      "[ Epoch 79, Train ] | Loss:0.00161 Time:0.084169\n",
      "[ Epoch 79, Val ] | Loss:0.31833 Time:0.005558\n",
      "[ Epoch 80, Train ] | Loss:0.00178 Time:0.084131\n",
      "[ Epoch 80, Val ] | Loss:0.29616 Time:0.005445\n",
      "[ Epoch 81, Train ] | Loss:0.00225 Time:0.084155\n",
      "[ Epoch 81, Val ] | Loss:0.28337 Time:0.005442\n",
      "[ Epoch 82, Train ] | Loss:0.00147 Time:0.084192\n",
      "[ Epoch 82, Val ] | Loss:0.26411 Time:0.005384\n",
      "[ Epoch 83, Train ] | Loss:0.00138 Time:0.084250\n",
      "[ Epoch 83, Val ] | Loss:0.28846 Time:0.005567\n",
      "[ Epoch 84, Train ] | Loss:0.00158 Time:0.082468\n",
      "[ Epoch 84, Val ] | Loss:0.31293 Time:0.005549\n",
      "[ Epoch 85, Train ] | Loss:0.00165 Time:0.084176\n",
      "[ Epoch 85, Val ] | Loss:0.32028 Time:0.005377\n",
      "[ Epoch 86, Train ] | Loss:0.00240 Time:0.084103\n",
      "[ Epoch 86, Val ] | Loss:0.33376 Time:0.005453\n",
      "[ Epoch 87, Train ] | Loss:0.00161 Time:0.084083\n",
      "[ Epoch 87, Val ] | Loss:0.35672 Time:0.005444\n",
      "[ Epoch 88, Train ] | Loss:0.00228 Time:0.083705\n",
      "[ Epoch 88, Val ] | Loss:0.34379 Time:0.005548\n",
      "[ Epoch 89, Train ] | Loss:0.00190 Time:0.084055\n",
      "[ Epoch 89, Val ] | Loss:0.32070 Time:0.005440\n",
      "[ Epoch 90, Train ] | Loss:0.00234 Time:0.084065\n",
      "[ Epoch 90, Val ] | Loss:0.30140 Time:0.005429\n",
      "[ Epoch 91, Train ] | Loss:0.00217 Time:0.084332\n",
      "[ Epoch 91, Val ] | Loss:0.25356 Time:0.005537\n",
      "save model with val loss 0.254\n",
      "[ Epoch 92, Train ] | Loss:0.00291 Time:0.082737\n",
      "[ Epoch 92, Val ] | Loss:0.25554 Time:0.005342\n",
      "[ Epoch 93, Train ] | Loss:0.00630 Time:0.083951\n",
      "[ Epoch 93, Val ] | Loss:0.60855 Time:0.005216\n",
      "[ Epoch 94, Train ] | Loss:0.00980 Time:0.083869\n",
      "[ Epoch 94, Val ] | Loss:0.49651 Time:0.005435\n",
      "[ Epoch 95, Train ] | Loss:0.01270 Time:0.085790\n",
      "[ Epoch 95, Val ] | Loss:0.46730 Time:0.005571\n",
      "[ Epoch 96, Train ] | Loss:0.01818 Time:0.083757\n",
      "[ Epoch 96, Val ] | Loss:0.54206 Time:0.005383\n",
      "[ Epoch 97, Train ] | Loss:0.01954 Time:0.083687\n",
      "[ Epoch 97, Val ] | Loss:0.39475 Time:0.005440\n",
      "[ Epoch 98, Train ] | Loss:0.02474 Time:0.083990\n",
      "[ Epoch 98, Val ] | Loss:0.68262 Time:0.005556\n",
      "[ Epoch 99, Train ] | Loss:0.02225 Time:0.083846\n",
      "[ Epoch 99, Val ] | Loss:0.75569 Time:0.005369\n",
      "[ Epoch 100, Train ] | Loss:0.01731 Time:0.083903\n",
      "[ Epoch 100, Val ] | Loss:0.36688 Time:0.005443\n",
      "[ Epoch 101, Train ] | Loss:0.01303 Time:0.083868\n",
      "[ Epoch 101, Val ] | Loss:0.53139 Time:0.005442\n",
      "[ Epoch 102, Train ] | Loss:0.01279 Time:0.083839\n",
      "[ Epoch 102, Val ] | Loss:0.49246 Time:0.005373\n",
      "[ Epoch 103, Train ] | Loss:0.00902 Time:0.083513\n",
      "[ Epoch 103, Val ] | Loss:0.53821 Time:0.005558\n",
      "[ Epoch 104, Train ] | Loss:0.00781 Time:0.083794\n",
      "[ Epoch 104, Val ] | Loss:0.28966 Time:0.005453\n",
      "[ Epoch 105, Train ] | Loss:0.00647 Time:0.083855\n",
      "[ Epoch 105, Val ] | Loss:0.28961 Time:0.005384\n",
      "[ Epoch 106, Train ] | Loss:0.00459 Time:0.083443\n",
      "[ Epoch 106, Val ] | Loss:0.33426 Time:0.005452\n",
      "[ Epoch 107, Train ] | Loss:0.00450 Time:0.083755\n",
      "[ Epoch 107, Val ] | Loss:0.30621 Time:0.005573\n",
      "[ Epoch 108, Train ] | Loss:0.00363 Time:0.084134\n",
      "[ Epoch 108, Val ] | Loss:0.29148 Time:0.005371\n",
      "[ Epoch 109, Train ] | Loss:0.00278 Time:0.083781\n",
      "[ Epoch 109, Val ] | Loss:0.29708 Time:0.005444\n",
      "[ Epoch 110, Train ] | Loss:0.00218 Time:0.082731\n",
      "[ Epoch 110, Val ] | Loss:0.28767 Time:0.005434\n",
      "[ Epoch 111, Train ] | Loss:0.00196 Time:0.083707\n",
      "[ Epoch 111, Val ] | Loss:0.31337 Time:0.005376\n",
      "[ Epoch 112, Train ] | Loss:0.00208 Time:0.083799\n",
      "[ Epoch 112, Val ] | Loss:0.29671 Time:0.005445\n",
      "[ Epoch 113, Train ] | Loss:0.00197 Time:0.083759\n",
      "[ Epoch 113, Val ] | Loss:0.24886 Time:0.005439\n",
      "save model with val loss 0.249\n",
      "[ Epoch 114, Train ] | Loss:0.00332 Time:0.084545\n",
      "[ Epoch 114, Val ] | Loss:0.37444 Time:0.005303\n",
      "[ Epoch 115, Train ] | Loss:0.00350 Time:0.083640\n",
      "[ Epoch 115, Val ] | Loss:0.35050 Time:0.005444\n",
      "[ Epoch 116, Train ] | Loss:0.00436 Time:0.083528\n",
      "[ Epoch 116, Val ] | Loss:0.48513 Time:0.005426\n",
      "[ Epoch 117, Train ] | Loss:0.00814 Time:0.083653\n",
      "[ Epoch 117, Val ] | Loss:0.39698 Time:0.005549\n",
      "[ Epoch 118, Train ] | Loss:0.00906 Time:0.083634\n",
      "[ Epoch 118, Val ] | Loss:0.48676 Time:0.005432\n",
      "[ Epoch 119, Train ] | Loss:0.01177 Time:0.083634\n",
      "[ Epoch 119, Val ] | Loss:0.32481 Time:0.005447\n",
      "[ Epoch 120, Train ] | Loss:0.01354 Time:0.083553\n",
      "[ Epoch 120, Val ] | Loss:0.29170 Time:0.005380\n",
      "[ Epoch 121, Train ] | Loss:0.01031 Time:0.083572\n",
      "[ Epoch 121, Val ] | Loss:0.32017 Time:0.005551\n",
      "[ Epoch 122, Train ] | Loss:0.00915 Time:0.083629\n",
      "[ Epoch 122, Val ] | Loss:0.32497 Time:0.005543\n",
      "[ Epoch 123, Train ] | Loss:0.00902 Time:0.083268\n",
      "[ Epoch 123, Val ] | Loss:0.44556 Time:0.005591\n",
      "[ Epoch 124, Train ] | Loss:0.01427 Time:0.083596\n",
      "[ Epoch 124, Val ] | Loss:0.85891 Time:0.005442\n",
      "[ Epoch 125, Train ] | Loss:0.01760 Time:0.083502\n",
      "[ Epoch 125, Val ] | Loss:0.42680 Time:0.005437\n",
      "[ Epoch 126, Train ] | Loss:0.01607 Time:0.083472\n",
      "[ Epoch 126, Val ] | Loss:0.69631 Time:0.005373\n",
      "[ Epoch 127, Train ] | Loss:0.01264 Time:0.083511\n",
      "[ Epoch 127, Val ] | Loss:0.57523 Time:0.005441\n",
      "[ Epoch 128, Train ] | Loss:0.01719 Time:0.083544\n",
      "[ Epoch 128, Val ] | Loss:0.44433 Time:0.005439\n",
      "[ Epoch 129, Train ] | Loss:0.01856 Time:0.083607\n",
      "[ Epoch 129, Val ] | Loss:1.18223 Time:0.005562\n",
      "[ Epoch 130, Train ] | Loss:0.02553 Time:0.083454\n",
      "[ Epoch 130, Val ] | Loss:0.77520 Time:0.005440\n",
      "[ Epoch 131, Train ] | Loss:0.02360 Time:0.083199\n",
      "[ Epoch 131, Val ] | Loss:0.63018 Time:0.005563\n",
      "[ Epoch 132, Train ] | Loss:0.02695 Time:0.083415\n",
      "[ Epoch 132, Val ] | Loss:0.72257 Time:0.005359\n",
      "[ Epoch 133, Train ] | Loss:0.03007 Time:0.083445\n",
      "[ Epoch 133, Val ] | Loss:1.50870 Time:0.005440\n",
      "[ Epoch 134, Train ] | Loss:0.02660 Time:0.083398\n",
      "[ Epoch 134, Val ] | Loss:0.99373 Time:0.005437\n",
      "[ Epoch 135, Train ] | Loss:0.03642 Time:0.083169\n",
      "[ Epoch 135, Val ] | Loss:0.76637 Time:0.005535\n",
      "[ Epoch 136, Train ] | Loss:0.03876 Time:0.083213\n",
      "[ Epoch 136, Val ] | Loss:0.85464 Time:0.005549\n",
      "[ Epoch 137, Train ] | Loss:0.05638 Time:0.083561\n",
      "[ Epoch 137, Val ] | Loss:3.32112 Time:0.005436\n",
      "[ Epoch 138, Train ] | Loss:0.16394 Time:0.083159\n",
      "[ Epoch 138, Val ] | Loss:2.52334 Time:0.005390\n",
      "[ Epoch 139, Train ] | Loss:0.12773 Time:0.083220\n",
      "[ Epoch 139, Val ] | Loss:1.69961 Time:0.005549\n",
      "[ Epoch 140, Train ] | Loss:0.05350 Time:0.083381\n",
      "[ Epoch 140, Val ] | Loss:1.46645 Time:0.005430\n",
      "[ Epoch 141, Train ] | Loss:0.04498 Time:0.083367\n",
      "[ Epoch 141, Val ] | Loss:0.60819 Time:0.005381\n",
      "[ Epoch 142, Train ] | Loss:0.07040 Time:0.083108\n",
      "[ Epoch 142, Val ] | Loss:0.54320 Time:0.005580\n",
      "[ Epoch 143, Train ] | Loss:0.03974 Time:0.083469\n",
      "[ Epoch 143, Val ] | Loss:0.62589 Time:0.005434\n",
      "[ Epoch 144, Train ] | Loss:0.02914 Time:0.083297\n",
      "[ Epoch 144, Val ] | Loss:0.51514 Time:0.005385\n",
      "[ Epoch 145, Train ] | Loss:0.01984 Time:0.083340\n",
      "[ Epoch 145, Val ] | Loss:0.61042 Time:0.005455\n",
      "[ Epoch 146, Train ] | Loss:0.01234 Time:0.082864\n",
      "[ Epoch 146, Val ] | Loss:0.26533 Time:0.005427\n",
      "[ Epoch 147, Train ] | Loss:0.00734 Time:0.083278\n",
      "[ Epoch 147, Val ] | Loss:0.31563 Time:0.005387\n",
      "[ Epoch 148, Train ] | Loss:0.00527 Time:0.083345\n",
      "[ Epoch 148, Val ] | Loss:0.35544 Time:0.005570\n",
      "[ Epoch 149, Train ] | Loss:0.00532 Time:0.083840\n",
      "[ Epoch 149, Val ] | Loss:0.35682 Time:0.005459\n",
      "[ Epoch 150, Train ] | Loss:0.00299 Time:0.083302\n",
      "[ Epoch 150, Val ] | Loss:0.27244 Time:0.005389\n",
      "[ Epoch 151, Train ] | Loss:0.00448 Time:0.083216\n",
      "[ Epoch 151, Val ] | Loss:0.22399 Time:0.005601\n",
      "save model with val loss 0.224\n",
      "[ Epoch 152, Train ] | Loss:0.00282 Time:0.083707\n",
      "[ Epoch 152, Val ] | Loss:0.36301 Time:0.005520\n",
      "[ Epoch 153, Train ] | Loss:0.00569 Time:0.083098\n",
      "[ Epoch 153, Val ] | Loss:0.41599 Time:0.005423\n",
      "[ Epoch 154, Train ] | Loss:0.00258 Time:0.083147\n",
      "[ Epoch 154, Val ] | Loss:0.35659 Time:0.005440\n",
      "[ Epoch 155, Train ] | Loss:0.00260 Time:0.082685\n",
      "[ Epoch 155, Val ] | Loss:0.27197 Time:0.005538\n",
      "[ Epoch 156, Train ] | Loss:0.00226 Time:0.082923\n",
      "[ Epoch 156, Val ] | Loss:0.26778 Time:0.005438\n",
      "[ Epoch 157, Train ] | Loss:0.00173 Time:0.083125\n",
      "[ Epoch 157, Val ] | Loss:0.26037 Time:0.005452\n",
      "[ Epoch 158, Train ] | Loss:0.00402 Time:0.083118\n",
      "[ Epoch 158, Val ] | Loss:0.25547 Time:0.005556\n",
      "[ Epoch 159, Train ] | Loss:0.00199 Time:0.083054\n",
      "[ Epoch 159, Val ] | Loss:0.49965 Time:0.005559\n",
      "[ Epoch 160, Train ] | Loss:0.00313 Time:0.082949\n",
      "[ Epoch 160, Val ] | Loss:0.29534 Time:0.005541\n",
      "[ Epoch 161, Train ] | Loss:0.00209 Time:0.083019\n",
      "[ Epoch 161, Val ] | Loss:0.24422 Time:0.005375\n",
      "[ Epoch 162, Train ] | Loss:0.00182 Time:0.082814\n",
      "[ Epoch 162, Val ] | Loss:0.26375 Time:0.005443\n",
      "[ Epoch 163, Train ] | Loss:0.00195 Time:0.082906\n",
      "[ Epoch 163, Val ] | Loss:0.25943 Time:0.005576\n",
      "[ Epoch 164, Train ] | Loss:0.00156 Time:0.083729\n",
      "[ Epoch 164, Val ] | Loss:0.25880 Time:0.005383\n",
      "[ Epoch 165, Train ] | Loss:0.00173 Time:0.082489\n",
      "[ Epoch 165, Val ] | Loss:0.21733 Time:0.005549\n",
      "save model with val loss 0.217\n",
      "[ Epoch 166, Train ] | Loss:0.00133 Time:0.083344\n",
      "[ Epoch 166, Val ] | Loss:0.20279 Time:0.005302\n",
      "save model with val loss 0.203\n",
      "[ Epoch 167, Train ] | Loss:0.00124 Time:0.084213\n",
      "[ Epoch 167, Val ] | Loss:0.19935 Time:0.005368\n",
      "save model with val loss 0.199\n",
      "[ Epoch 168, Train ] | Loss:0.00161 Time:0.084330\n",
      "[ Epoch 168, Val ] | Loss:0.21377 Time:0.005317\n",
      "[ Epoch 169, Train ] | Loss:0.00192 Time:0.082888\n",
      "[ Epoch 169, Val ] | Loss:0.21959 Time:0.005453\n",
      "[ Epoch 170, Train ] | Loss:0.00119 Time:0.082855\n",
      "[ Epoch 170, Val ] | Loss:0.20876 Time:0.005443\n",
      "[ Epoch 171, Train ] | Loss:0.00117 Time:0.082728\n",
      "[ Epoch 171, Val ] | Loss:0.19716 Time:0.005562\n",
      "save model with val loss 0.197\n",
      "[ Epoch 172, Train ] | Loss:0.00139 Time:0.084204\n",
      "[ Epoch 172, Val ] | Loss:0.18710 Time:0.005363\n",
      "save model with val loss 0.187\n",
      "[ Epoch 173, Train ] | Loss:0.00117 Time:0.084634\n",
      "[ Epoch 173, Val ] | Loss:0.18450 Time:0.005519\n",
      "save model with val loss 0.184\n",
      "[ Epoch 174, Train ] | Loss:0.00137 Time:0.084315\n",
      "[ Epoch 174, Val ] | Loss:0.18814 Time:0.005553\n",
      "[ Epoch 175, Train ] | Loss:0.00139 Time:0.082747\n",
      "[ Epoch 175, Val ] | Loss:0.19189 Time:0.005554\n",
      "[ Epoch 176, Train ] | Loss:0.00144 Time:0.082778\n",
      "[ Epoch 176, Val ] | Loss:0.20318 Time:0.005440\n",
      "[ Epoch 177, Train ] | Loss:0.00210 Time:0.082911\n",
      "[ Epoch 177, Val ] | Loss:0.19837 Time:0.005548\n",
      "[ Epoch 178, Train ] | Loss:0.00158 Time:0.082360\n",
      "[ Epoch 178, Val ] | Loss:0.21804 Time:0.005536\n",
      "[ Epoch 179, Train ] | Loss:0.00178 Time:0.082778\n",
      "[ Epoch 179, Val ] | Loss:0.23359 Time:0.005444\n",
      "[ Epoch 180, Train ] | Loss:0.00164 Time:0.082812\n",
      "[ Epoch 180, Val ] | Loss:0.23551 Time:0.005552\n",
      "[ Epoch 181, Train ] | Loss:0.00211 Time:0.084841\n",
      "[ Epoch 181, Val ] | Loss:0.21109 Time:0.005404\n",
      "[ Epoch 182, Train ] | Loss:0.00165 Time:0.082774\n",
      "[ Epoch 182, Val ] | Loss:0.19814 Time:0.005435\n",
      "[ Epoch 183, Train ] | Loss:0.00183 Time:0.082572\n",
      "[ Epoch 183, Val ] | Loss:0.19229 Time:0.005554\n",
      "[ Epoch 184, Train ] | Loss:0.00161 Time:0.082751\n",
      "[ Epoch 184, Val ] | Loss:0.20253 Time:0.005383\n",
      "[ Epoch 185, Train ] | Loss:0.00165 Time:0.082732\n",
      "[ Epoch 185, Val ] | Loss:0.21367 Time:0.005422\n",
      "[ Epoch 186, Train ] | Loss:0.00161 Time:0.082546\n",
      "[ Epoch 186, Val ] | Loss:0.22028 Time:0.005455\n",
      "[ Epoch 187, Train ] | Loss:0.00153 Time:0.082717\n",
      "[ Epoch 187, Val ] | Loss:0.20398 Time:0.005550\n",
      "[ Epoch 188, Train ] | Loss:0.00156 Time:0.082734\n",
      "[ Epoch 188, Val ] | Loss:0.20158 Time:0.005442\n",
      "[ Epoch 189, Train ] | Loss:0.00194 Time:0.082499\n",
      "[ Epoch 189, Val ] | Loss:0.22260 Time:0.005440\n",
      "[ Epoch 190, Train ] | Loss:0.00177 Time:0.084770\n",
      "[ Epoch 190, Val ] | Loss:0.24737 Time:0.005375\n",
      "[ Epoch 191, Train ] | Loss:0.00189 Time:0.084549\n",
      "[ Epoch 191, Val ] | Loss:0.23001 Time:0.005435\n",
      "[ Epoch 192, Train ] | Loss:0.00230 Time:0.084257\n",
      "[ Epoch 192, Val ] | Loss:0.21232 Time:0.005432\n",
      "[ Epoch 193, Train ] | Loss:0.00235 Time:0.084499\n",
      "[ Epoch 193, Val ] | Loss:0.25046 Time:0.005381\n",
      "[ Epoch 194, Train ] | Loss:0.00305 Time:0.084749\n",
      "[ Epoch 194, Val ] | Loss:0.25817 Time:0.005444\n",
      "[ Epoch 195, Train ] | Loss:0.00358 Time:0.084385\n",
      "[ Epoch 195, Val ] | Loss:0.23350 Time:0.005431\n",
      "[ Epoch 196, Train ] | Loss:0.00312 Time:0.084223\n",
      "[ Epoch 196, Val ] | Loss:0.21506 Time:0.005373\n",
      "[ Epoch 197, Train ] | Loss:0.00390 Time:0.084605\n",
      "[ Epoch 197, Val ] | Loss:0.21457 Time:0.005433\n",
      "[ Epoch 198, Train ] | Loss:0.00394 Time:0.084730\n",
      "[ Epoch 198, Val ] | Loss:0.18863 Time:0.005450\n",
      "[ Epoch 199, Train ] | Loss:0.00491 Time:0.084550\n",
      "[ Epoch 199, Val ] | Loss:0.24565 Time:0.005554\n",
      "[ Epoch 200, Train ] | Loss:0.01052 Time:0.082599\n",
      "[ Epoch 200, Val ] | Loss:0.36243 Time:0.005441\n",
      "[ Epoch 201, Train ] | Loss:0.01356 Time:0.084396\n",
      "[ Epoch 201, Val ] | Loss:0.25859 Time:0.005436\n",
      "[ Epoch 202, Train ] | Loss:0.01446 Time:0.084281\n",
      "[ Epoch 202, Val ] | Loss:0.35114 Time:0.005548\n",
      "[ Epoch 203, Train ] | Loss:0.00926 Time:0.084453\n",
      "[ Epoch 203, Val ] | Loss:0.25933 Time:0.005437\n",
      "[ Epoch 204, Train ] | Loss:0.00760 Time:0.084366\n",
      "[ Epoch 204, Val ] | Loss:0.17354 Time:0.005451\n",
      "save model with val loss 0.174\n",
      "[ Epoch 205, Train ] | Loss:0.00500 Time:0.082639\n",
      "[ Epoch 205, Val ] | Loss:0.23756 Time:0.005328\n",
      "[ Epoch 206, Train ] | Loss:0.00451 Time:0.084328\n",
      "[ Epoch 206, Val ] | Loss:0.26062 Time:0.005432\n",
      "[ Epoch 207, Train ] | Loss:0.00463 Time:0.084080\n",
      "[ Epoch 207, Val ] | Loss:0.34269 Time:0.005436\n",
      "[ Epoch 208, Train ] | Loss:0.00483 Time:0.084136\n",
      "[ Epoch 208, Val ] | Loss:0.19076 Time:0.005562\n",
      "[ Epoch 209, Train ] | Loss:0.00424 Time:0.084022\n",
      "[ Epoch 209, Val ] | Loss:0.17212 Time:0.005454\n",
      "save model with val loss 0.172\n",
      "[ Epoch 210, Train ] | Loss:0.02274 Time:0.083394\n",
      "[ Epoch 210, Val ] | Loss:0.92858 Time:0.005313\n",
      "[ Epoch 211, Train ] | Loss:0.14480 Time:0.084404\n",
      "[ Epoch 211, Val ] | Loss:11.59002 Time:0.005429\n",
      "[ Epoch 212, Train ] | Loss:0.36359 Time:0.084222\n",
      "[ Epoch 212, Val ] | Loss:3.35316 Time:0.005445\n",
      "[ Epoch 213, Train ] | Loss:0.16534 Time:0.084172\n",
      "[ Epoch 213, Val ] | Loss:2.03129 Time:0.005368\n",
      "[ Epoch 214, Train ] | Loss:0.07278 Time:0.084430\n",
      "[ Epoch 214, Val ] | Loss:0.68557 Time:0.005589\n",
      "[ Epoch 215, Train ] | Loss:0.04188 Time:0.084423\n",
      "[ Epoch 215, Val ] | Loss:1.15719 Time:0.005432\n",
      "[ Epoch 216, Train ] | Loss:0.02782 Time:0.084144\n",
      "[ Epoch 216, Val ] | Loss:0.24277 Time:0.005372\n",
      "[ Epoch 217, Train ] | Loss:0.02113 Time:0.084354\n",
      "[ Epoch 217, Val ] | Loss:0.57534 Time:0.005438\n",
      "[ Epoch 218, Train ] | Loss:0.02122 Time:0.084234\n",
      "[ Epoch 218, Val ] | Loss:0.50090 Time:0.005503\n",
      "[ Epoch 219, Train ] | Loss:0.01007 Time:0.083963\n",
      "[ Epoch 219, Val ] | Loss:0.58869 Time:0.005438\n",
      "[ Epoch 220, Train ] | Loss:0.00821 Time:0.084301\n",
      "[ Epoch 220, Val ] | Loss:0.32057 Time:0.005564\n",
      "[ Epoch 221, Train ] | Loss:0.00422 Time:0.084332\n",
      "[ Epoch 221, Val ] | Loss:0.28502 Time:0.005383\n",
      "[ Epoch 222, Train ] | Loss:0.00323 Time:0.084256\n",
      "[ Epoch 222, Val ] | Loss:0.21342 Time:0.005558\n",
      "[ Epoch 223, Train ] | Loss:0.00997 Time:0.084282\n",
      "[ Epoch 223, Val ] | Loss:0.24230 Time:0.005438\n",
      "[ Epoch 224, Train ] | Loss:0.01514 Time:0.083820\n",
      "[ Epoch 224, Val ] | Loss:0.57743 Time:0.005589\n",
      "[ Epoch 225, Train ] | Loss:0.00928 Time:0.084217\n",
      "[ Epoch 225, Val ] | Loss:1.26273 Time:0.005560\n",
      "[ Epoch 226, Train ] | Loss:0.00852 Time:0.084086\n",
      "[ Epoch 226, Val ] | Loss:0.46659 Time:0.005437\n",
      "[ Epoch 227, Train ] | Loss:0.00642 Time:0.083647\n",
      "[ Epoch 227, Val ] | Loss:0.32094 Time:0.005379\n",
      "[ Epoch 228, Train ] | Loss:0.00492 Time:0.084246\n",
      "[ Epoch 228, Val ] | Loss:0.40938 Time:0.005558\n",
      "[ Epoch 229, Train ] | Loss:0.00420 Time:0.084238\n",
      "[ Epoch 229, Val ] | Loss:0.33597 Time:0.005441\n",
      "[ Epoch 230, Train ] | Loss:0.00333 Time:0.083659\n",
      "[ Epoch 230, Val ] | Loss:0.28273 Time:0.005544\n",
      "[ Epoch 231, Train ] | Loss:0.00269 Time:0.082529\n",
      "[ Epoch 231, Val ] | Loss:0.24223 Time:0.005554\n",
      "[ Epoch 232, Train ] | Loss:0.00273 Time:0.084166\n",
      "[ Epoch 232, Val ] | Loss:0.20382 Time:0.005437\n",
      "[ Epoch 233, Train ] | Loss:0.00279 Time:0.083762\n",
      "[ Epoch 233, Val ] | Loss:0.21212 Time:0.005372\n",
      "[ Epoch 234, Train ] | Loss:0.00225 Time:0.084195\n",
      "[ Epoch 234, Val ] | Loss:0.21667 Time:0.005450\n",
      "[ Epoch 235, Train ] | Loss:0.00184 Time:0.084072\n",
      "[ Epoch 235, Val ] | Loss:0.19591 Time:0.005442\n",
      "[ Epoch 236, Train ] | Loss:0.00169 Time:0.083980\n",
      "[ Epoch 236, Val ] | Loss:0.19064 Time:0.005535\n",
      "[ Epoch 237, Train ] | Loss:0.00222 Time:0.084131\n",
      "[ Epoch 237, Val ] | Loss:0.18469 Time:0.005560\n",
      "[ Epoch 238, Train ] | Loss:0.00177 Time:0.083121\n",
      "[ Epoch 238, Val ] | Loss:0.22877 Time:0.005569\n",
      "[ Epoch 239, Train ] | Loss:0.00556 Time:0.084041\n",
      "[ Epoch 239, Val ] | Loss:0.19243 Time:0.005379\n",
      "[ Epoch 240, Train ] | Loss:0.00867 Time:0.084037\n",
      "[ Epoch 240, Val ] | Loss:0.28952 Time:0.005442\n",
      "[ Epoch 241, Train ] | Loss:0.00794 Time:0.083998\n",
      "[ Epoch 241, Val ] | Loss:0.32485 Time:0.005448\n",
      "[ Epoch 242, Train ] | Loss:0.01055 Time:0.083672\n",
      "[ Epoch 242, Val ] | Loss:0.29160 Time:0.005383\n",
      "[ Epoch 243, Train ] | Loss:0.00588 Time:0.084000\n",
      "[ Epoch 243, Val ] | Loss:0.26529 Time:0.005572\n",
      "[ Epoch 244, Train ] | Loss:0.00460 Time:0.083896\n",
      "[ Epoch 244, Val ] | Loss:0.25166 Time:0.005450\n",
      "[ Epoch 245, Train ] | Loss:0.00323 Time:0.083689\n",
      "[ Epoch 245, Val ] | Loss:0.19856 Time:0.005384\n",
      "[ Epoch 246, Train ] | Loss:0.00283 Time:0.083833\n",
      "[ Epoch 246, Val ] | Loss:0.20413 Time:0.005453\n",
      "[ Epoch 247, Train ] | Loss:0.00300 Time:0.083455\n",
      "[ Epoch 247, Val ] | Loss:0.20009 Time:0.005568\n",
      "[ Epoch 248, Train ] | Loss:0.00216 Time:0.083723\n",
      "[ Epoch 248, Val ] | Loss:0.22640 Time:0.005556\n",
      "[ Epoch 249, Train ] | Loss:0.00217 Time:0.083648\n",
      "[ Epoch 249, Val ] | Loss:0.21615 Time:0.005548\n",
      "[ Epoch 250, Train ] | Loss:0.00212 Time:0.083957\n",
      "[ Epoch 250, Val ] | Loss:0.19671 Time:0.005431\n",
      "[ Epoch 251, Train ] | Loss:0.00276 Time:0.083842\n",
      "[ Epoch 251, Val ] | Loss:0.18406 Time:0.005382\n",
      "[ Epoch 252, Train ] | Loss:0.00180 Time:0.083943\n",
      "[ Epoch 252, Val ] | Loss:0.17447 Time:0.005450\n",
      "[ Epoch 253, Train ] | Loss:0.00193 Time:0.083704\n",
      "[ Epoch 253, Val ] | Loss:0.16830 Time:0.005434\n",
      "save model with val loss 0.168\n",
      "[ Epoch 254, Train ] | Loss:0.00169 Time:0.083712\n",
      "[ Epoch 254, Val ] | Loss:0.16703 Time:0.005300\n",
      "save model with val loss 0.167\n",
      "[ Epoch 255, Train ] | Loss:0.00153 Time:0.082848\n",
      "[ Epoch 255, Val ] | Loss:0.17499 Time:0.005369\n",
      "[ Epoch 256, Train ] | Loss:0.00247 Time:0.085818\n",
      "[ Epoch 256, Val ] | Loss:0.18410 Time:0.005364\n",
      "[ Epoch 257, Train ] | Loss:0.00163 Time:0.083788\n",
      "[ Epoch 257, Val ] | Loss:0.19852 Time:0.005437\n",
      "[ Epoch 258, Train ] | Loss:0.00151 Time:0.083769\n",
      "[ Epoch 258, Val ] | Loss:0.20546 Time:0.005445\n",
      "[ Epoch 259, Train ] | Loss:0.00217 Time:0.083308\n",
      "[ Epoch 259, Val ] | Loss:0.20857 Time:0.005163\n",
      "[ Epoch 260, Train ] | Loss:0.00176 Time:0.083665\n",
      "[ Epoch 260, Val ] | Loss:0.21960 Time:0.005457\n",
      "[ Epoch 261, Train ] | Loss:0.00192 Time:0.083611\n",
      "[ Epoch 261, Val ] | Loss:0.22638 Time:0.005432\n",
      "[ Epoch 262, Train ] | Loss:0.00170 Time:0.083540\n",
      "[ Epoch 262, Val ] | Loss:0.22572 Time:0.005385\n",
      "[ Epoch 263, Train ] | Loss:0.00218 Time:0.083718\n",
      "[ Epoch 263, Val ] | Loss:0.21143 Time:0.005450\n",
      "[ Epoch 264, Train ] | Loss:0.00219 Time:0.083634\n",
      "[ Epoch 264, Val ] | Loss:0.20291 Time:0.005459\n",
      "[ Epoch 265, Train ] | Loss:0.00184 Time:0.083416\n",
      "[ Epoch 265, Val ] | Loss:0.22322 Time:0.005379\n",
      "[ Epoch 266, Train ] | Loss:0.00160 Time:0.083586\n",
      "[ Epoch 266, Val ] | Loss:0.20549 Time:0.005443\n",
      "[ Epoch 267, Train ] | Loss:0.00222 Time:0.083610\n",
      "[ Epoch 267, Val ] | Loss:0.20243 Time:0.005442\n",
      "[ Epoch 268, Train ] | Loss:0.00231 Time:0.082960\n",
      "[ Epoch 268, Val ] | Loss:0.21064 Time:0.005389\n",
      "[ Epoch 269, Train ] | Loss:0.00201 Time:0.083520\n",
      "[ Epoch 269, Val ] | Loss:0.20850 Time:0.005441\n",
      "[ Epoch 270, Train ] | Loss:0.00260 Time:0.083644\n",
      "[ Epoch 270, Val ] | Loss:0.23066 Time:0.005441\n",
      "[ Epoch 271, Train ] | Loss:0.00217 Time:0.082500\n",
      "[ Epoch 271, Val ] | Loss:0.20843 Time:0.005383\n",
      "[ Epoch 272, Train ] | Loss:0.00230 Time:0.083565\n",
      "[ Epoch 272, Val ] | Loss:0.21185 Time:0.005443\n",
      "[ Epoch 273, Train ] | Loss:0.00206 Time:0.083551\n",
      "[ Epoch 273, Val ] | Loss:0.21220 Time:0.005442\n",
      "[ Epoch 274, Train ] | Loss:0.00277 Time:0.082941\n",
      "[ Epoch 274, Val ] | Loss:0.19624 Time:0.005376\n",
      "[ Epoch 275, Train ] | Loss:0.00235 Time:0.083507\n",
      "[ Epoch 275, Val ] | Loss:0.20842 Time:0.005434\n",
      "[ Epoch 276, Train ] | Loss:0.00305 Time:0.083589\n",
      "[ Epoch 276, Val ] | Loss:0.25112 Time:0.005566\n",
      "[ Epoch 277, Train ] | Loss:0.00303 Time:0.082939\n",
      "[ Epoch 277, Val ] | Loss:0.26256 Time:0.005557\n",
      "[ Epoch 278, Train ] | Loss:0.00276 Time:0.083575\n",
      "[ Epoch 278, Val ] | Loss:0.23536 Time:0.005429\n",
      "[ Epoch 279, Train ] | Loss:0.00256 Time:0.083362\n",
      "[ Epoch 279, Val ] | Loss:0.22647 Time:0.005580\n",
      "[ Epoch 280, Train ] | Loss:0.00245 Time:0.084037\n",
      "[ Epoch 280, Val ] | Loss:0.22560 Time:0.005361\n",
      "[ Epoch 281, Train ] | Loss:0.00280 Time:0.083517\n",
      "[ Epoch 281, Val ] | Loss:0.20267 Time:0.005551\n",
      "[ Epoch 282, Train ] | Loss:0.00320 Time:0.083504\n",
      "[ Epoch 282, Val ] | Loss:0.19191 Time:0.005554\n",
      "[ Epoch 283, Train ] | Loss:0.00355 Time:0.083085\n",
      "[ Epoch 283, Val ] | Loss:0.24243 Time:0.005569\n",
      "[ Epoch 284, Train ] | Loss:0.00363 Time:0.083354\n",
      "[ Epoch 284, Val ] | Loss:0.22822 Time:0.005562\n",
      "[ Epoch 285, Train ] | Loss:0.00325 Time:0.083402\n",
      "[ Epoch 285, Val ] | Loss:0.25887 Time:0.005430\n",
      "[ Epoch 286, Train ] | Loss:0.00391 Time:0.082847\n",
      "[ Epoch 286, Val ] | Loss:0.18503 Time:0.005379\n",
      "[ Epoch 287, Train ] | Loss:0.00274 Time:0.083054\n",
      "[ Epoch 287, Val ] | Loss:0.16700 Time:0.005430\n",
      "save model with val loss 0.167\n",
      "[ Epoch 288, Train ] | Loss:0.00321 Time:0.085665\n",
      "[ Epoch 288, Val ] | Loss:0.19263 Time:0.005321\n",
      "[ Epoch 289, Train ] | Loss:0.00272 Time:0.083157\n",
      "[ Epoch 289, Val ] | Loss:0.22381 Time:0.005432\n",
      "[ Epoch 290, Train ] | Loss:0.00293 Time:0.083263\n",
      "[ Epoch 290, Val ] | Loss:0.27172 Time:0.005451\n",
      "[ Epoch 291, Train ] | Loss:0.00257 Time:0.082697\n",
      "[ Epoch 291, Val ] | Loss:0.21338 Time:0.005360\n",
      "[ Epoch 292, Train ] | Loss:0.00228 Time:0.083273\n",
      "[ Epoch 292, Val ] | Loss:0.23006 Time:0.005431\n",
      "[ Epoch 293, Train ] | Loss:0.00280 Time:0.083228\n",
      "[ Epoch 293, Val ] | Loss:0.25235 Time:0.005441\n",
      "[ Epoch 294, Train ] | Loss:0.00263 Time:0.083017\n",
      "[ Epoch 294, Val ] | Loss:0.20817 Time:0.005385\n",
      "[ Epoch 295, Train ] | Loss:0.00297 Time:0.083251\n",
      "[ Epoch 295, Val ] | Loss:0.20457 Time:0.005440\n",
      "[ Epoch 296, Train ] | Loss:0.00331 Time:0.082884\n",
      "[ Epoch 296, Val ] | Loss:0.19683 Time:0.005548\n",
      "[ Epoch 297, Train ] | Loss:0.00320 Time:0.082915\n",
      "[ Epoch 297, Val ] | Loss:0.25897 Time:0.005388\n",
      "[ Epoch 298, Train ] | Loss:0.00317 Time:0.083249\n",
      "[ Epoch 298, Val ] | Loss:0.20872 Time:0.005437\n",
      "[ Epoch 299, Train ] | Loss:0.00289 Time:0.082953\n",
      "[ Epoch 299, Val ] | Loss:0.17761 Time:0.005559\n",
      "[ Epoch 300, Train ] | Loss:0.00308 Time:0.082749\n",
      "[ Epoch 300, Val ] | Loss:0.23207 Time:0.005384\n",
      "Finish training!\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "d4c98f20-662c-431e-ad48-589c54ac8b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin testing\n",
      "Test loss: 0.22446653246879578\n",
      "——————Test——————\n",
      "           class precision  recall f1-score support\n",
      "0              0    0.8333  1.0000   0.9091       5\n",
      "1              1    0.6667  0.8000   0.7273       5\n",
      "2              2    1.0000  1.0000   1.0000       5\n",
      "3              3    0.8333  1.0000   0.9091       5\n",
      "4              4    1.0000  0.8000   0.8889       5\n",
      "5              5    1.0000  1.0000   1.0000       5\n",
      "6              6    1.0000  0.8000   0.8889       5\n",
      "7              7    1.0000  0.8000   0.8889       5\n",
      "8              8    1.0000  1.0000   1.0000       5\n",
      "9              9    0.8333  1.0000   0.9091       5\n",
      "10            10    1.0000  1.0000   1.0000       5\n",
      "11            11    1.0000  0.8000   0.8889       5\n",
      "12            12    1.0000  1.0000   1.0000       5\n",
      "13            13    1.0000  0.8000   0.8889       5\n",
      "14            14    1.0000  1.0000   1.0000       5\n",
      "15            15    1.0000  1.0000   1.0000       5\n",
      "16            16    1.0000  1.0000   1.0000       5\n",
      "17            17    1.0000  1.0000   1.0000       5\n",
      "18            18    1.0000  0.8000   0.8889       5\n",
      "19            19    0.8333  1.0000   0.9091       5\n",
      "20            20    0.8333  1.0000   0.9091       5\n",
      "21            21    1.0000  0.8000   0.8889       5\n",
      "22            22    1.0000  1.0000   1.0000       5\n",
      "23            23    0.8333  1.0000   0.9091       5\n",
      "24            24    1.0000  1.0000   1.0000       5\n",
      "25            25    1.0000  1.0000   1.0000       5\n",
      "26            26    1.0000  1.0000   1.0000       5\n",
      "27            27    1.0000  1.0000   1.0000       5\n",
      "28            28    0.8000  0.8000   0.8000       5\n",
      "29            29    0.8000  0.8000   0.8000       5\n",
      "30            30    1.0000  1.0000   1.0000       5\n",
      "31            31    1.0000  1.0000   1.0000       5\n",
      "32            32    1.0000  1.0000   1.0000       5\n",
      "33            33    1.0000  1.0000   1.0000       5\n",
      "34            34    1.0000  1.0000   1.0000       5\n",
      "35            35    1.0000  1.0000   1.0000       5\n",
      "36            36    1.0000  1.0000   1.0000       5\n",
      "37            37    1.0000  1.0000   1.0000       5\n",
      "38            38    1.0000  0.8000   0.8889       5\n",
      "39            39    1.0000  1.0000   1.0000       5\n",
      "40            40    0.8333  1.0000   0.9091       5\n",
      "41            41    1.0000  1.0000   1.0000       5\n",
      "42            42    1.0000  1.0000   1.0000       5\n",
      "43            43    1.0000  1.0000   1.0000       5\n",
      "44            44    0.6667  0.8000   0.7273       5\n",
      "45            45    0.8333  1.0000   0.9091       5\n",
      "46            46    0.7500  0.6000   0.6667       5\n",
      "47            47    1.0000  1.0000   1.0000       5\n",
      "48            48    1.0000  1.0000   1.0000       5\n",
      "49            49    1.0000  0.8000   0.8889       5\n",
      "50      accuracy                     0.9400     250\n",
      "51     macro avg    0.9470  0.9400   0.9399     250\n",
      "52  weighted avg    0.9470  0.9400   0.9399     250\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70f46f0-b8f2-4be9-b0aa-e2e400b277f4",
   "metadata": {},
   "source": [
    "# 7.2 Do not use dropout or batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "c6013d35-cc48-4dd6-86c3-3fd90ff1e8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vanilla_CNN_no_dropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Vanilla_CNN_no_dropout, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 1, 1), # input channel, ouput channel, filter size, stride, padding\n",
    "            nn.BatchNorm2d(32), # num_features\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, 1, 1), \n",
    "            nn.BatchNorm2d(32), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0), # kernel size, stride, padding\n",
    "        \n",
    "            nn.Conv2d(32, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64*7*7, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,50),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1) # resize, first axis is batch_num\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "0b8d826e-72e5-424e-98bd-4db10bc87f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_no_dropout = Vanilla_CNN_no_dropout()\n",
    "epochs = 200\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0\n",
    "optimizer = Adam(cnn_no_dropout.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_dir = './output'\n",
    "model_name = 'cnn_no_dropout'\n",
    "random_seed = 2022\n",
    "trainer = Trainer(model=cnn_no_dropout, \n",
    "                  epochs=epochs, \n",
    "                  train_dataloader=train_loader, \n",
    "                  val_dataloader=val_loader, \n",
    "                  test_dataloader=test_loader,\n",
    "                  criterion=criterion, \n",
    "                  optimizer=optimizer, \n",
    "                  lr=learning_rate, \n",
    "                  device=device, \n",
    "                  model_dir=model_dir, \n",
    "                  model_name=model_name, \n",
    "                  random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "3215ed2a-f3fc-406c-89c3-8a089086c79a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameters: 1698194, trainable parameters: 1698194\n",
      "\n",
      "        initial learning rate: 0.001, weight_decay: 0,\n",
      "        total epochs: 200\n",
      "        \n",
      "Begin training, total epochs: 200\n",
      "[ Epoch 1, Train ] | Loss:3.82962 Time:0.072912\n",
      "[ Epoch 1, Val ] | Loss:3.91359 Time:0.005265\n",
      "save model with val loss 3.914\n",
      "[ Epoch 2, Train ] | Loss:3.60645 Time:0.065418\n",
      "[ Epoch 2, Val ] | Loss:3.91462 Time:0.005282\n",
      "[ Epoch 3, Train ] | Loss:3.45030 Time:0.065121\n",
      "[ Epoch 3, Val ] | Loss:3.91291 Time:0.005289\n",
      "save model with val loss 3.913\n",
      "[ Epoch 4, Train ] | Loss:3.30741 Time:0.065981\n",
      "[ Epoch 4, Val ] | Loss:3.90647 Time:0.005307\n",
      "save model with val loss 3.906\n",
      "[ Epoch 5, Train ] | Loss:3.24639 Time:0.066688\n",
      "[ Epoch 5, Val ] | Loss:3.86650 Time:0.005274\n",
      "save model with val loss 3.866\n",
      "[ Epoch 6, Train ] | Loss:3.17069 Time:0.066747\n",
      "[ Epoch 6, Val ] | Loss:3.81868 Time:0.005289\n",
      "save model with val loss 3.819\n",
      "[ Epoch 7, Train ] | Loss:3.12723 Time:0.066566\n",
      "[ Epoch 7, Val ] | Loss:3.63911 Time:0.005387\n",
      "save model with val loss 3.639\n",
      "[ Epoch 8, Train ] | Loss:3.08124 Time:0.073762\n",
      "[ Epoch 8, Val ] | Loss:3.48315 Time:0.005186\n",
      "save model with val loss 3.483\n",
      "[ Epoch 9, Train ] | Loss:3.04427 Time:0.070914\n",
      "[ Epoch 9, Val ] | Loss:3.33447 Time:0.004981\n",
      "save model with val loss 3.334\n",
      "[ Epoch 10, Train ] | Loss:3.02564 Time:0.066426\n",
      "[ Epoch 10, Val ] | Loss:3.29847 Time:0.005385\n",
      "save model with val loss 3.298\n",
      "[ Epoch 11, Train ] | Loss:3.00846 Time:0.065195\n",
      "[ Epoch 11, Val ] | Loss:3.27025 Time:0.005303\n",
      "save model with val loss 3.270\n",
      "[ Epoch 12, Train ] | Loss:2.99014 Time:0.069036\n",
      "[ Epoch 12, Val ] | Loss:3.23252 Time:0.005265\n",
      "save model with val loss 3.233\n",
      "[ Epoch 13, Train ] | Loss:2.97038 Time:0.070795\n",
      "[ Epoch 13, Val ] | Loss:3.24922 Time:0.004994\n",
      "[ Epoch 14, Train ] | Loss:2.96570 Time:0.069105\n",
      "[ Epoch 14, Val ] | Loss:3.22697 Time:0.005238\n",
      "save model with val loss 3.227\n",
      "[ Epoch 15, Train ] | Loss:2.96672 Time:0.067697\n",
      "[ Epoch 15, Val ] | Loss:3.20040 Time:0.005183\n",
      "save model with val loss 3.200\n",
      "[ Epoch 16, Train ] | Loss:2.95889 Time:0.065699\n",
      "[ Epoch 16, Val ] | Loss:3.17653 Time:0.005288\n",
      "save model with val loss 3.177\n",
      "[ Epoch 17, Train ] | Loss:2.95006 Time:0.068554\n",
      "[ Epoch 17, Val ] | Loss:3.18130 Time:0.004985\n",
      "[ Epoch 18, Train ] | Loss:2.94877 Time:0.065163\n",
      "[ Epoch 18, Val ] | Loss:3.16437 Time:0.005308\n",
      "save model with val loss 3.164\n",
      "[ Epoch 19, Train ] | Loss:2.94768 Time:0.071230\n",
      "[ Epoch 19, Val ] | Loss:3.16879 Time:0.005225\n",
      "[ Epoch 20, Train ] | Loss:2.94734 Time:0.065320\n",
      "[ Epoch 20, Val ] | Loss:3.16901 Time:0.005316\n",
      "[ Epoch 21, Train ] | Loss:2.94710 Time:0.067448\n",
      "[ Epoch 21, Val ] | Loss:3.15906 Time:0.005309\n",
      "save model with val loss 3.159\n",
      "[ Epoch 22, Train ] | Loss:2.94688 Time:0.065820\n",
      "[ Epoch 22, Val ] | Loss:3.15640 Time:0.005290\n",
      "save model with val loss 3.156\n",
      "[ Epoch 23, Train ] | Loss:2.94667 Time:0.065959\n",
      "[ Epoch 23, Val ] | Loss:3.15152 Time:0.005285\n",
      "save model with val loss 3.152\n",
      "[ Epoch 24, Train ] | Loss:2.94658 Time:0.066377\n",
      "[ Epoch 24, Val ] | Loss:3.15074 Time:0.005301\n",
      "save model with val loss 3.151\n",
      "[ Epoch 25, Train ] | Loss:2.94658 Time:0.066225\n",
      "[ Epoch 25, Val ] | Loss:3.14929 Time:0.005277\n",
      "save model with val loss 3.149\n",
      "[ Epoch 26, Train ] | Loss:2.94647 Time:0.066344\n",
      "[ Epoch 26, Val ] | Loss:3.14769 Time:0.005295\n",
      "save model with val loss 3.148\n",
      "[ Epoch 27, Train ] | Loss:2.94648 Time:0.066103\n",
      "[ Epoch 27, Val ] | Loss:3.14672 Time:0.005305\n",
      "save model with val loss 3.147\n",
      "[ Epoch 28, Train ] | Loss:2.94641 Time:0.066346\n",
      "[ Epoch 28, Val ] | Loss:3.14942 Time:0.005300\n",
      "[ Epoch 29, Train ] | Loss:2.94636 Time:0.065032\n",
      "[ Epoch 29, Val ] | Loss:3.15150 Time:0.005142\n",
      "[ Epoch 30, Train ] | Loss:2.94633 Time:0.064896\n",
      "[ Epoch 30, Val ] | Loss:3.15090 Time:0.005313\n",
      "[ Epoch 31, Train ] | Loss:2.94629 Time:0.065045\n",
      "[ Epoch 31, Val ] | Loss:3.14808 Time:0.005311\n",
      "[ Epoch 32, Train ] | Loss:2.94626 Time:0.064926\n",
      "[ Epoch 32, Val ] | Loss:3.14554 Time:0.005320\n",
      "save model with val loss 3.146\n",
      "[ Epoch 33, Train ] | Loss:2.94622 Time:0.066412\n",
      "[ Epoch 33, Val ] | Loss:3.14334 Time:0.005300\n",
      "save model with val loss 3.143\n",
      "[ Epoch 34, Train ] | Loss:2.94623 Time:0.064603\n",
      "[ Epoch 34, Val ] | Loss:3.14360 Time:0.005304\n",
      "[ Epoch 35, Train ] | Loss:2.94619 Time:0.064963\n",
      "[ Epoch 35, Val ] | Loss:3.14276 Time:0.005326\n",
      "save model with val loss 3.143\n",
      "[ Epoch 36, Train ] | Loss:2.94619 Time:0.066394\n",
      "[ Epoch 36, Val ] | Loss:3.14255 Time:0.005287\n",
      "save model with val loss 3.143\n",
      "[ Epoch 37, Train ] | Loss:2.94618 Time:0.066313\n",
      "[ Epoch 37, Val ] | Loss:3.14259 Time:0.005280\n",
      "[ Epoch 38, Train ] | Loss:2.94615 Time:0.065084\n",
      "[ Epoch 38, Val ] | Loss:3.14223 Time:0.005330\n",
      "save model with val loss 3.142\n",
      "[ Epoch 39, Train ] | Loss:2.94617 Time:0.064458\n",
      "[ Epoch 39, Val ] | Loss:3.14185 Time:0.005309\n",
      "save model with val loss 3.142\n",
      "[ Epoch 40, Train ] | Loss:2.94614 Time:0.066806\n",
      "[ Epoch 40, Val ] | Loss:3.14111 Time:0.005317\n",
      "save model with val loss 3.141\n",
      "[ Epoch 41, Train ] | Loss:2.94611 Time:0.066546\n",
      "[ Epoch 41, Val ] | Loss:3.14213 Time:0.005304\n",
      "[ Epoch 42, Train ] | Loss:2.94612 Time:0.065006\n",
      "[ Epoch 42, Val ] | Loss:3.14008 Time:0.005283\n",
      "save model with val loss 3.140\n",
      "[ Epoch 43, Train ] | Loss:2.94611 Time:0.066490\n",
      "[ Epoch 43, Val ] | Loss:3.13990 Time:0.005311\n",
      "save model with val loss 3.140\n",
      "[ Epoch 44, Train ] | Loss:2.94610 Time:0.064617\n",
      "[ Epoch 44, Val ] | Loss:3.13978 Time:0.005308\n",
      "save model with val loss 3.140\n",
      "[ Epoch 45, Train ] | Loss:2.94615 Time:0.064598\n",
      "[ Epoch 45, Val ] | Loss:3.13866 Time:0.005284\n",
      "save model with val loss 3.139\n",
      "[ Epoch 46, Train ] | Loss:2.94609 Time:0.064599\n",
      "[ Epoch 46, Val ] | Loss:3.14098 Time:0.005284\n",
      "[ Epoch 47, Train ] | Loss:2.94608 Time:0.064939\n",
      "[ Epoch 47, Val ] | Loss:3.14205 Time:0.005321\n",
      "[ Epoch 48, Train ] | Loss:2.94606 Time:0.064814\n",
      "[ Epoch 48, Val ] | Loss:3.14064 Time:0.005302\n",
      "[ Epoch 49, Train ] | Loss:2.94607 Time:0.064884\n",
      "[ Epoch 49, Val ] | Loss:3.13861 Time:0.005313\n",
      "save model with val loss 3.139\n",
      "[ Epoch 50, Train ] | Loss:2.94605 Time:0.066530\n",
      "[ Epoch 50, Val ] | Loss:3.13796 Time:0.005284\n",
      "save model with val loss 3.138\n",
      "[ Epoch 51, Train ] | Loss:2.94604 Time:0.066496\n",
      "[ Epoch 51, Val ] | Loss:3.13726 Time:0.005305\n",
      "save model with val loss 3.137\n",
      "[ Epoch 52, Train ] | Loss:2.94605 Time:0.066393\n",
      "[ Epoch 52, Val ] | Loss:3.13610 Time:0.005299\n",
      "save model with val loss 3.136\n",
      "[ Epoch 53, Train ] | Loss:2.94603 Time:0.066583\n",
      "[ Epoch 53, Val ] | Loss:3.13838 Time:0.005309\n",
      "[ Epoch 54, Train ] | Loss:2.94603 Time:0.064771\n",
      "[ Epoch 54, Val ] | Loss:3.13660 Time:0.005323\n",
      "[ Epoch 55, Train ] | Loss:2.94603 Time:0.064687\n",
      "[ Epoch 55, Val ] | Loss:3.13561 Time:0.005204\n",
      "save model with val loss 3.136\n",
      "[ Epoch 56, Train ] | Loss:2.94603 Time:0.066113\n",
      "[ Epoch 56, Val ] | Loss:3.13526 Time:0.005310\n",
      "save model with val loss 3.135\n",
      "[ Epoch 57, Train ] | Loss:2.94601 Time:0.066039\n",
      "[ Epoch 57, Val ] | Loss:3.13692 Time:0.005314\n",
      "[ Epoch 58, Train ] | Loss:2.94601 Time:0.064514\n",
      "[ Epoch 58, Val ] | Loss:3.13761 Time:0.005329\n",
      "[ Epoch 59, Train ] | Loss:2.94601 Time:0.064686\n",
      "[ Epoch 59, Val ] | Loss:3.13718 Time:0.005321\n",
      "[ Epoch 60, Train ] | Loss:2.94600 Time:0.064620\n",
      "[ Epoch 60, Val ] | Loss:3.13788 Time:0.005329\n",
      "[ Epoch 61, Train ] | Loss:2.94600 Time:0.064822\n",
      "[ Epoch 61, Val ] | Loss:3.13843 Time:0.005322\n",
      "[ Epoch 62, Train ] | Loss:2.94599 Time:0.064666\n",
      "[ Epoch 62, Val ] | Loss:3.13495 Time:0.005312\n",
      "save model with val loss 3.135\n",
      "[ Epoch 63, Train ] | Loss:2.94599 Time:0.067225\n",
      "[ Epoch 63, Val ] | Loss:3.13445 Time:0.005067\n",
      "save model with val loss 3.134\n",
      "[ Epoch 64, Train ] | Loss:2.94599 Time:0.066182\n",
      "[ Epoch 64, Val ] | Loss:3.13499 Time:0.005290\n",
      "[ Epoch 65, Train ] | Loss:2.94598 Time:0.064631\n",
      "[ Epoch 65, Val ] | Loss:3.13614 Time:0.005304\n",
      "[ Epoch 66, Train ] | Loss:2.94597 Time:0.066773\n",
      "[ Epoch 66, Val ] | Loss:3.13805 Time:0.005307\n",
      "[ Epoch 67, Train ] | Loss:2.94598 Time:0.064620\n",
      "[ Epoch 67, Val ] | Loss:3.13845 Time:0.005313\n",
      "[ Epoch 68, Train ] | Loss:2.94600 Time:0.067339\n",
      "[ Epoch 68, Val ] | Loss:3.13621 Time:0.005217\n",
      "[ Epoch 69, Train ] | Loss:2.94597 Time:0.066454\n",
      "[ Epoch 69, Val ] | Loss:3.13666 Time:0.005318\n",
      "[ Epoch 70, Train ] | Loss:2.94597 Time:0.064612\n",
      "[ Epoch 70, Val ] | Loss:3.13743 Time:0.005139\n",
      "[ Epoch 71, Train ] | Loss:2.94597 Time:0.064263\n",
      "[ Epoch 71, Val ] | Loss:3.13406 Time:0.005105\n",
      "save model with val loss 3.134\n",
      "[ Epoch 72, Train ] | Loss:2.94597 Time:0.065411\n",
      "[ Epoch 72, Val ] | Loss:3.13487 Time:0.005055\n",
      "[ Epoch 73, Train ] | Loss:2.94598 Time:0.066634\n",
      "[ Epoch 73, Val ] | Loss:3.13368 Time:0.005314\n",
      "save model with val loss 3.134\n",
      "[ Epoch 74, Train ] | Loss:2.94596 Time:0.065518\n",
      "[ Epoch 74, Val ] | Loss:3.13304 Time:0.005283\n",
      "save model with val loss 3.133\n",
      "[ Epoch 75, Train ] | Loss:2.94595 Time:0.065367\n",
      "[ Epoch 75, Val ] | Loss:3.13268 Time:0.005258\n",
      "save model with val loss 3.133\n",
      "[ Epoch 76, Train ] | Loss:2.94594 Time:0.069780\n",
      "[ Epoch 76, Val ] | Loss:3.13366 Time:0.005267\n",
      "[ Epoch 77, Train ] | Loss:2.94594 Time:0.066387\n",
      "[ Epoch 77, Val ] | Loss:3.13482 Time:0.005326\n",
      "[ Epoch 78, Train ] | Loss:2.94595 Time:0.068668\n",
      "[ Epoch 78, Val ] | Loss:3.13436 Time:0.005292\n",
      "[ Epoch 79, Train ] | Loss:2.94593 Time:0.064591\n",
      "[ Epoch 79, Val ] | Loss:3.13391 Time:0.005313\n",
      "[ Epoch 80, Train ] | Loss:2.94594 Time:0.064564\n",
      "[ Epoch 80, Val ] | Loss:3.13430 Time:0.005323\n",
      "[ Epoch 81, Train ] | Loss:2.94594 Time:0.064629\n",
      "[ Epoch 81, Val ] | Loss:3.13303 Time:0.005318\n",
      "[ Epoch 82, Train ] | Loss:2.94592 Time:0.064273\n",
      "[ Epoch 82, Val ] | Loss:3.13215 Time:0.005321\n",
      "save model with val loss 3.132\n",
      "[ Epoch 83, Train ] | Loss:2.94592 Time:0.066564\n",
      "[ Epoch 83, Val ] | Loss:3.13256 Time:0.005281\n",
      "[ Epoch 84, Train ] | Loss:2.94593 Time:0.066554\n",
      "[ Epoch 84, Val ] | Loss:3.13170 Time:0.005316\n",
      "save model with val loss 3.132\n",
      "[ Epoch 85, Train ] | Loss:2.94592 Time:0.065091\n",
      "[ Epoch 85, Val ] | Loss:3.13087 Time:0.005295\n",
      "save model with val loss 3.131\n",
      "[ Epoch 86, Train ] | Loss:2.94593 Time:0.066053\n",
      "[ Epoch 86, Val ] | Loss:3.13242 Time:0.005305\n",
      "[ Epoch 87, Train ] | Loss:2.94593 Time:0.066496\n",
      "[ Epoch 87, Val ] | Loss:3.13113 Time:0.005308\n",
      "[ Epoch 88, Train ] | Loss:2.94592 Time:0.068792\n",
      "[ Epoch 88, Val ] | Loss:3.12984 Time:0.005282\n",
      "save model with val loss 3.130\n",
      "[ Epoch 89, Train ] | Loss:2.94592 Time:0.065154\n",
      "[ Epoch 89, Val ] | Loss:3.12991 Time:0.005140\n",
      "[ Epoch 90, Train ] | Loss:2.94592 Time:0.066461\n",
      "[ Epoch 90, Val ] | Loss:3.13070 Time:0.005307\n",
      "[ Epoch 91, Train ] | Loss:2.94591 Time:0.066448\n",
      "[ Epoch 91, Val ] | Loss:3.13004 Time:0.005324\n",
      "[ Epoch 92, Train ] | Loss:2.94591 Time:0.066597\n",
      "[ Epoch 92, Val ] | Loss:3.13016 Time:0.005323\n",
      "[ Epoch 93, Train ] | Loss:2.94594 Time:0.066401\n",
      "[ Epoch 93, Val ] | Loss:3.13171 Time:0.005324\n",
      "[ Epoch 94, Train ] | Loss:2.94592 Time:0.066520\n",
      "[ Epoch 94, Val ] | Loss:3.13415 Time:0.005307\n",
      "[ Epoch 95, Train ] | Loss:2.94591 Time:0.066198\n",
      "[ Epoch 95, Val ] | Loss:3.13385 Time:0.005325\n",
      "[ Epoch 96, Train ] | Loss:2.94591 Time:0.066640\n",
      "[ Epoch 96, Val ] | Loss:3.12931 Time:0.005280\n",
      "save model with val loss 3.129\n",
      "[ Epoch 97, Train ] | Loss:2.94591 Time:0.065015\n",
      "[ Epoch 97, Val ] | Loss:3.13092 Time:0.005301\n",
      "[ Epoch 98, Train ] | Loss:2.94591 Time:0.066504\n",
      "[ Epoch 98, Val ] | Loss:3.13187 Time:0.005300\n",
      "[ Epoch 99, Train ] | Loss:2.94590 Time:0.066440\n",
      "[ Epoch 99, Val ] | Loss:3.13039 Time:0.005307\n",
      "[ Epoch 100, Train ] | Loss:2.94590 Time:0.066449\n",
      "[ Epoch 100, Val ] | Loss:3.13030 Time:0.005324\n",
      "[ Epoch 101, Train ] | Loss:2.94590 Time:0.066390\n",
      "[ Epoch 101, Val ] | Loss:3.12913 Time:0.005310\n",
      "save model with val loss 3.129\n",
      "[ Epoch 102, Train ] | Loss:2.94589 Time:0.065459\n",
      "[ Epoch 102, Val ] | Loss:3.12864 Time:0.005311\n",
      "save model with val loss 3.129\n",
      "[ Epoch 103, Train ] | Loss:2.94590 Time:0.065563\n",
      "[ Epoch 103, Val ] | Loss:3.12757 Time:0.005245\n",
      "save model with val loss 3.128\n",
      "[ Epoch 104, Train ] | Loss:2.94590 Time:0.065035\n",
      "[ Epoch 104, Val ] | Loss:3.12861 Time:0.005284\n",
      "[ Epoch 105, Train ] | Loss:2.94590 Time:0.066302\n",
      "[ Epoch 105, Val ] | Loss:3.12689 Time:0.005317\n",
      "save model with val loss 3.127\n",
      "[ Epoch 106, Train ] | Loss:2.94590 Time:0.065556\n",
      "[ Epoch 106, Val ] | Loss:3.12824 Time:0.005294\n",
      "[ Epoch 107, Train ] | Loss:2.94589 Time:0.066471\n",
      "[ Epoch 107, Val ] | Loss:3.12866 Time:0.005311\n",
      "[ Epoch 108, Train ] | Loss:2.94589 Time:0.066357\n",
      "[ Epoch 108, Val ] | Loss:3.12975 Time:0.005302\n",
      "[ Epoch 109, Train ] | Loss:2.94589 Time:0.066278\n",
      "[ Epoch 109, Val ] | Loss:3.13021 Time:0.005319\n",
      "[ Epoch 110, Train ] | Loss:2.94589 Time:0.066352\n",
      "[ Epoch 110, Val ] | Loss:3.13034 Time:0.005324\n",
      "[ Epoch 111, Train ] | Loss:2.94589 Time:0.066223\n",
      "[ Epoch 111, Val ] | Loss:3.12875 Time:0.005287\n",
      "[ Epoch 112, Train ] | Loss:2.94589 Time:0.066262\n",
      "[ Epoch 112, Val ] | Loss:3.12875 Time:0.005324\n",
      "[ Epoch 113, Train ] | Loss:2.94589 Time:0.066175\n",
      "[ Epoch 113, Val ] | Loss:3.12866 Time:0.005324\n",
      "[ Epoch 114, Train ] | Loss:2.94588 Time:0.066379\n",
      "[ Epoch 114, Val ] | Loss:3.12862 Time:0.005304\n",
      "[ Epoch 115, Train ] | Loss:2.94588 Time:0.066337\n",
      "[ Epoch 115, Val ] | Loss:3.12910 Time:0.005299\n",
      "[ Epoch 116, Train ] | Loss:2.94588 Time:0.066232\n",
      "[ Epoch 116, Val ] | Loss:3.12921 Time:0.005328\n",
      "[ Epoch 117, Train ] | Loss:2.94588 Time:0.066237\n",
      "[ Epoch 117, Val ] | Loss:3.12880 Time:0.005315\n",
      "[ Epoch 118, Train ] | Loss:2.94589 Time:0.066350\n",
      "[ Epoch 118, Val ] | Loss:3.12550 Time:0.005312\n",
      "save model with val loss 3.125\n",
      "[ Epoch 119, Train ] | Loss:2.94589 Time:0.065156\n",
      "[ Epoch 119, Val ] | Loss:3.12659 Time:0.005286\n",
      "[ Epoch 120, Train ] | Loss:2.94588 Time:0.066228\n",
      "[ Epoch 120, Val ] | Loss:3.12758 Time:0.005307\n",
      "[ Epoch 121, Train ] | Loss:2.94587 Time:0.066107\n",
      "[ Epoch 121, Val ] | Loss:3.12546 Time:0.005316\n",
      "save model with val loss 3.125\n",
      "[ Epoch 122, Train ] | Loss:2.94588 Time:0.065466\n",
      "[ Epoch 122, Val ] | Loss:3.12714 Time:0.005326\n",
      "[ Epoch 123, Train ] | Loss:2.94588 Time:0.066252\n",
      "[ Epoch 123, Val ] | Loss:3.12619 Time:0.005311\n",
      "[ Epoch 124, Train ] | Loss:2.94588 Time:0.066101\n",
      "[ Epoch 124, Val ] | Loss:3.12623 Time:0.005323\n",
      "[ Epoch 125, Train ] | Loss:2.94588 Time:0.066127\n",
      "[ Epoch 125, Val ] | Loss:3.12642 Time:0.005316\n",
      "[ Epoch 126, Train ] | Loss:2.94587 Time:0.066148\n",
      "[ Epoch 126, Val ] | Loss:3.12535 Time:0.005095\n",
      "save model with val loss 3.125\n",
      "[ Epoch 127, Train ] | Loss:2.94588 Time:0.066020\n",
      "[ Epoch 127, Val ] | Loss:3.12620 Time:0.005290\n",
      "[ Epoch 128, Train ] | Loss:2.94588 Time:0.066135\n",
      "[ Epoch 128, Val ] | Loss:3.12752 Time:0.005328\n",
      "[ Epoch 129, Train ] | Loss:2.94588 Time:0.066194\n",
      "[ Epoch 129, Val ] | Loss:3.12738 Time:0.005302\n",
      "[ Epoch 130, Train ] | Loss:2.94587 Time:0.066133\n",
      "[ Epoch 130, Val ] | Loss:3.12867 Time:0.005336\n",
      "[ Epoch 131, Train ] | Loss:2.94587 Time:0.066267\n",
      "[ Epoch 131, Val ] | Loss:3.12750 Time:0.005323\n",
      "[ Epoch 132, Train ] | Loss:2.94587 Time:0.066052\n",
      "[ Epoch 132, Val ] | Loss:3.12709 Time:0.005312\n",
      "[ Epoch 133, Train ] | Loss:2.94587 Time:0.066067\n",
      "[ Epoch 133, Val ] | Loss:3.12663 Time:0.005343\n",
      "[ Epoch 134, Train ] | Loss:2.94587 Time:0.066330\n",
      "[ Epoch 134, Val ] | Loss:3.12499 Time:0.005334\n",
      "save model with val loss 3.125\n",
      "[ Epoch 135, Train ] | Loss:2.94586 Time:0.065192\n",
      "[ Epoch 135, Val ] | Loss:3.12564 Time:0.005293\n",
      "[ Epoch 136, Train ] | Loss:2.94587 Time:0.066017\n",
      "[ Epoch 136, Val ] | Loss:3.12502 Time:0.005316\n",
      "[ Epoch 137, Train ] | Loss:2.94588 Time:0.065878\n",
      "[ Epoch 137, Val ] | Loss:3.12529 Time:0.005313\n",
      "[ Epoch 138, Train ] | Loss:2.94587 Time:0.065832\n",
      "[ Epoch 138, Val ] | Loss:3.12542 Time:0.005328\n",
      "[ Epoch 139, Train ] | Loss:2.94586 Time:0.066122\n",
      "[ Epoch 139, Val ] | Loss:3.12654 Time:0.005316\n",
      "[ Epoch 140, Train ] | Loss:2.94587 Time:0.065974\n",
      "[ Epoch 140, Val ] | Loss:3.12695 Time:0.005332\n",
      "[ Epoch 141, Train ] | Loss:2.94586 Time:0.065938\n",
      "[ Epoch 141, Val ] | Loss:3.12566 Time:0.005316\n",
      "[ Epoch 142, Train ] | Loss:2.94586 Time:0.065990\n",
      "[ Epoch 142, Val ] | Loss:3.12599 Time:0.005312\n",
      "[ Epoch 143, Train ] | Loss:2.94586 Time:0.066033\n",
      "[ Epoch 143, Val ] | Loss:3.12618 Time:0.005306\n",
      "[ Epoch 144, Train ] | Loss:2.94586 Time:0.065989\n",
      "[ Epoch 144, Val ] | Loss:3.12588 Time:0.005307\n",
      "[ Epoch 145, Train ] | Loss:2.94586 Time:0.066006\n",
      "[ Epoch 145, Val ] | Loss:3.12522 Time:0.005328\n",
      "[ Epoch 146, Train ] | Loss:2.94586 Time:0.066013\n",
      "[ Epoch 146, Val ] | Loss:3.12446 Time:0.005324\n",
      "save model with val loss 3.124\n",
      "[ Epoch 147, Train ] | Loss:2.94586 Time:0.064947\n",
      "[ Epoch 147, Val ] | Loss:3.12423 Time:0.005286\n",
      "save model with val loss 3.124\n",
      "[ Epoch 148, Train ] | Loss:2.94586 Time:0.065236\n",
      "[ Epoch 148, Val ] | Loss:3.12411 Time:0.005292\n",
      "save model with val loss 3.124\n",
      "[ Epoch 149, Train ] | Loss:2.94586 Time:0.065356\n",
      "[ Epoch 149, Val ] | Loss:3.12482 Time:0.005296\n",
      "[ Epoch 150, Train ] | Loss:2.94587 Time:0.065820\n",
      "[ Epoch 150, Val ] | Loss:3.12523 Time:0.005302\n",
      "[ Epoch 151, Train ] | Loss:2.94586 Time:0.065772\n",
      "[ Epoch 151, Val ] | Loss:3.12284 Time:0.005334\n",
      "save model with val loss 3.123\n",
      "[ Epoch 152, Train ] | Loss:2.94586 Time:0.065055\n",
      "[ Epoch 152, Val ] | Loss:3.12414 Time:0.005288\n",
      "[ Epoch 153, Train ] | Loss:2.94586 Time:0.065993\n",
      "[ Epoch 153, Val ] | Loss:3.12365 Time:0.005167\n",
      "[ Epoch 154, Train ] | Loss:2.94586 Time:0.065884\n",
      "[ Epoch 154, Val ] | Loss:3.12499 Time:0.005325\n",
      "[ Epoch 155, Train ] | Loss:2.94586 Time:0.065881\n",
      "[ Epoch 155, Val ] | Loss:3.12470 Time:0.005329\n",
      "[ Epoch 156, Train ] | Loss:2.94586 Time:0.068141\n",
      "[ Epoch 156, Val ] | Loss:3.12360 Time:0.005313\n",
      "[ Epoch 157, Train ] | Loss:2.94585 Time:0.065963\n",
      "[ Epoch 157, Val ] | Loss:3.12369 Time:0.005326\n",
      "[ Epoch 158, Train ] | Loss:2.94586 Time:0.065685\n",
      "[ Epoch 158, Val ] | Loss:3.12390 Time:0.005320\n",
      "[ Epoch 159, Train ] | Loss:2.94586 Time:0.065706\n",
      "[ Epoch 159, Val ] | Loss:3.12279 Time:0.005316\n",
      "save model with val loss 3.123\n",
      "[ Epoch 160, Train ] | Loss:2.94586 Time:0.065043\n",
      "[ Epoch 160, Val ] | Loss:3.12269 Time:0.005299\n",
      "save model with val loss 3.123\n",
      "[ Epoch 161, Train ] | Loss:2.94586 Time:0.065313\n",
      "[ Epoch 161, Val ] | Loss:3.12389 Time:0.005283\n",
      "[ Epoch 162, Train ] | Loss:2.94585 Time:0.065881\n",
      "[ Epoch 162, Val ] | Loss:3.12393 Time:0.005328\n",
      "[ Epoch 163, Train ] | Loss:2.94585 Time:0.065011\n",
      "[ Epoch 163, Val ] | Loss:3.12211 Time:0.005276\n",
      "save model with val loss 3.122\n",
      "[ Epoch 164, Train ] | Loss:2.94585 Time:0.066191\n",
      "[ Epoch 164, Val ] | Loss:3.12165 Time:0.005296\n",
      "save model with val loss 3.122\n",
      "[ Epoch 165, Train ] | Loss:2.94585 Time:0.067316\n",
      "[ Epoch 165, Val ] | Loss:3.12218 Time:0.005297\n",
      "[ Epoch 166, Train ] | Loss:2.94585 Time:0.065679\n",
      "[ Epoch 166, Val ] | Loss:3.12192 Time:0.005307\n",
      "[ Epoch 167, Train ] | Loss:2.94585 Time:0.065891\n",
      "[ Epoch 167, Val ] | Loss:3.12292 Time:0.005317\n",
      "[ Epoch 168, Train ] | Loss:2.94585 Time:0.065953\n",
      "[ Epoch 168, Val ] | Loss:3.12324 Time:0.005309\n",
      "[ Epoch 169, Train ] | Loss:2.94585 Time:0.065756\n",
      "[ Epoch 169, Val ] | Loss:3.12303 Time:0.005333\n",
      "[ Epoch 170, Train ] | Loss:2.94585 Time:0.065640\n",
      "[ Epoch 170, Val ] | Loss:3.12253 Time:0.005326\n",
      "[ Epoch 171, Train ] | Loss:2.94585 Time:0.065752\n",
      "[ Epoch 171, Val ] | Loss:3.12196 Time:0.005321\n",
      "[ Epoch 172, Train ] | Loss:2.94586 Time:0.065687\n",
      "[ Epoch 172, Val ] | Loss:3.12250 Time:0.005339\n",
      "[ Epoch 173, Train ] | Loss:2.94585 Time:0.065623\n",
      "[ Epoch 173, Val ] | Loss:3.12321 Time:0.005312\n",
      "[ Epoch 174, Train ] | Loss:2.94585 Time:0.066482\n",
      "[ Epoch 174, Val ] | Loss:3.12336 Time:0.004341\n",
      "[ Epoch 175, Train ] | Loss:2.94585 Time:0.065722\n",
      "[ Epoch 175, Val ] | Loss:3.12239 Time:0.005320\n",
      "[ Epoch 176, Train ] | Loss:2.94585 Time:0.065650\n",
      "[ Epoch 176, Val ] | Loss:3.12283 Time:0.005314\n",
      "[ Epoch 177, Train ] | Loss:2.94585 Time:0.065599\n",
      "[ Epoch 177, Val ] | Loss:3.12137 Time:0.005318\n",
      "save model with val loss 3.121\n",
      "[ Epoch 178, Train ] | Loss:2.94585 Time:0.064739\n",
      "[ Epoch 178, Val ] | Loss:3.12073 Time:0.005289\n",
      "save model with val loss 3.121\n",
      "[ Epoch 179, Train ] | Loss:2.94585 Time:0.064838\n",
      "[ Epoch 179, Val ] | Loss:3.12101 Time:0.005310\n",
      "[ Epoch 180, Train ] | Loss:2.94585 Time:0.065381\n",
      "[ Epoch 180, Val ] | Loss:3.12117 Time:0.006952\n",
      "[ Epoch 181, Train ] | Loss:2.94585 Time:0.065647\n",
      "[ Epoch 181, Val ] | Loss:3.12141 Time:0.005306\n",
      "[ Epoch 182, Train ] | Loss:2.94585 Time:0.065526\n",
      "[ Epoch 182, Val ] | Loss:3.12198 Time:0.005317\n",
      "[ Epoch 183, Train ] | Loss:2.94585 Time:0.065584\n",
      "[ Epoch 183, Val ] | Loss:3.12167 Time:0.005309\n",
      "[ Epoch 184, Train ] | Loss:2.94585 Time:0.065617\n",
      "[ Epoch 184, Val ] | Loss:3.12243 Time:0.005150\n",
      "[ Epoch 185, Train ] | Loss:2.94585 Time:0.065605\n",
      "[ Epoch 185, Val ] | Loss:3.12208 Time:0.005320\n",
      "[ Epoch 186, Train ] | Loss:2.94584 Time:0.065785\n",
      "[ Epoch 186, Val ] | Loss:3.12362 Time:0.005304\n",
      "[ Epoch 187, Train ] | Loss:2.94584 Time:0.065441\n",
      "[ Epoch 187, Val ] | Loss:3.12204 Time:0.005328\n",
      "[ Epoch 188, Train ] | Loss:2.94584 Time:0.065379\n",
      "[ Epoch 188, Val ] | Loss:3.11960 Time:0.005317\n",
      "save model with val loss 3.120\n",
      "[ Epoch 189, Train ] | Loss:2.94584 Time:0.065552\n",
      "[ Epoch 189, Val ] | Loss:3.11941 Time:0.005281\n",
      "save model with val loss 3.119\n",
      "[ Epoch 190, Train ] | Loss:2.94585 Time:0.066714\n",
      "[ Epoch 190, Val ] | Loss:3.12069 Time:0.005293\n",
      "[ Epoch 191, Train ] | Loss:2.94584 Time:0.065561\n",
      "[ Epoch 191, Val ] | Loss:3.12113 Time:0.005317\n",
      "[ Epoch 192, Train ] | Loss:2.94584 Time:0.065527\n",
      "[ Epoch 192, Val ] | Loss:3.12104 Time:0.005315\n",
      "[ Epoch 193, Train ] | Loss:2.94584 Time:0.065496\n",
      "[ Epoch 193, Val ] | Loss:3.12089 Time:0.005319\n",
      "[ Epoch 194, Train ] | Loss:2.94584 Time:0.065423\n",
      "[ Epoch 194, Val ] | Loss:3.11952 Time:0.005316\n",
      "[ Epoch 195, Train ] | Loss:2.94584 Time:0.065449\n",
      "[ Epoch 195, Val ] | Loss:3.11982 Time:0.005307\n",
      "[ Epoch 196, Train ] | Loss:2.94584 Time:0.065360\n",
      "[ Epoch 196, Val ] | Loss:3.11926 Time:0.005315\n",
      "save model with val loss 3.119\n",
      "[ Epoch 197, Train ] | Loss:2.94584 Time:0.066660\n",
      "[ Epoch 197, Val ] | Loss:3.12093 Time:0.005292\n",
      "[ Epoch 198, Train ] | Loss:2.94584 Time:0.065399\n",
      "[ Epoch 198, Val ] | Loss:3.12249 Time:0.005296\n",
      "[ Epoch 199, Train ] | Loss:2.94584 Time:0.065334\n",
      "[ Epoch 199, Val ] | Loss:3.12223 Time:0.005311\n",
      "[ Epoch 200, Train ] | Loss:2.94584 Time:0.065398\n",
      "[ Epoch 200, Val ] | Loss:3.12190 Time:0.005318\n",
      "Finish training!\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "a495831a-6773-4160-b8f9-4a0aba79eec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin testing\n",
      "Test loss: 3.115818977355957\n",
      "——————Test——————\n",
      "           class precision  recall f1-score support\n",
      "0              0    1.0000  1.0000   1.0000       5\n",
      "1              1    0.6000  0.6000   0.6000       5\n",
      "2              2    1.0000  1.0000   1.0000       5\n",
      "3              3    1.0000  1.0000   1.0000       5\n",
      "4              4    0.6667  0.4000   0.5000       5\n",
      "5              5    0.7500  0.6000   0.6667       5\n",
      "6              6    0.7500  0.6000   0.6667       5\n",
      "7              7    0.6667  0.8000   0.7273       5\n",
      "8              8    1.0000  0.8000   0.8889       5\n",
      "9              9    1.0000  0.8000   0.8889       5\n",
      "10            10    0.8333  1.0000   0.9091       5\n",
      "11            11    0.7143  1.0000   0.8333       5\n",
      "12            12    0.8333  1.0000   0.9091       5\n",
      "13            13    1.0000  1.0000   1.0000       5\n",
      "14            14    0.6667  0.8000   0.7273       5\n",
      "15            15    1.0000  0.8000   0.8889       5\n",
      "16            16    0.8333  1.0000   0.9091       5\n",
      "17            17    1.0000  1.0000   1.0000       5\n",
      "18            18    1.0000  0.6000   0.7500       5\n",
      "19            19    0.7143  1.0000   0.8333       5\n",
      "20            20    1.0000  0.8000   0.8889       5\n",
      "21            21    0.8000  0.8000   0.8000       5\n",
      "22            22    1.0000  0.6000   0.7500       5\n",
      "23            23    1.0000  0.6000   0.7500       5\n",
      "24            24    1.0000  1.0000   1.0000       5\n",
      "25            25    1.0000  1.0000   1.0000       5\n",
      "26            26    0.8000  0.8000   0.8000       5\n",
      "27            27    1.0000  1.0000   1.0000       5\n",
      "28            28    0.8333  1.0000   0.9091       5\n",
      "29            29    1.0000  1.0000   1.0000       5\n",
      "30            30    0.8333  1.0000   0.9091       5\n",
      "31            31    1.0000  1.0000   1.0000       5\n",
      "32            32    0.8000  0.8000   0.8000       5\n",
      "33            33    1.0000  1.0000   1.0000       5\n",
      "34            34    0.8333  1.0000   0.9091       5\n",
      "35            35    1.0000  1.0000   1.0000       5\n",
      "36            36    1.0000  1.0000   1.0000       5\n",
      "37            37    0.8333  1.0000   0.9091       5\n",
      "38            38    1.0000  1.0000   1.0000       5\n",
      "39            39    0.7143  1.0000   0.8333       5\n",
      "40            40    1.0000  0.8000   0.8889       5\n",
      "41            41    1.0000  0.8000   0.8889       5\n",
      "42            42    1.0000  1.0000   1.0000       5\n",
      "43            43    0.8333  1.0000   0.9091       5\n",
      "44            44    0.7500  0.6000   0.6667       5\n",
      "45            45    1.0000  0.8000   0.8889       5\n",
      "46            46    0.6667  0.8000   0.7273       5\n",
      "47            47    1.0000  1.0000   1.0000       5\n",
      "48            48    1.0000  1.0000   1.0000       5\n",
      "49            49    0.8333  1.0000   0.9091       5\n",
      "50      accuracy                     0.8800     250\n",
      "51     macro avg    0.8912  0.8800   0.8767     250\n",
      "52  weighted avg    0.8912  0.8800   0.8767     250\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "06b2cfdc-f621-47cd-858a-ba88339e131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vanilla_CNN_no_BN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Vanilla_CNN_no_BN, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 1, 1), # input channel, ouput channel, filter size, stride, padding\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, 1, 1), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0), # kernel size, stride, padding\n",
    "        \n",
    "            nn.Conv2d(32, 64, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64*7*7, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,50),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1) # resize, first axis is batch_num\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "27f5cb10-24b8-44a7-96fe-98d10a7188e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_no_BN = Vanilla_CNN_no_BN()\n",
    "epochs = 300\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4\n",
    "optimizer = Adam(cnn_no_BN.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_dir = './output'\n",
    "model_name = 'cnn_no_BN'\n",
    "random_seed = 2022\n",
    "trainer = Trainer(model=cnn_no_BN, \n",
    "                  epochs=epochs, \n",
    "                  train_dataloader=train_loader, \n",
    "                  val_dataloader=val_loader, \n",
    "                  test_dataloader=test_loader,\n",
    "                  criterion=criterion, \n",
    "                  optimizer=optimizer, \n",
    "                  lr=learning_rate, \n",
    "                  device=device, \n",
    "                  model_dir=model_dir, \n",
    "                  model_name=model_name, \n",
    "                  random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "21cfe3bb-f160-4fd5-b9fd-9f6c0bc5faa4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameters: 1696786, trainable parameters: 1696786\n",
      "\n",
      "        initial learning rate: 0.001, weight_decay: 0.0001,\n",
      "        total epochs: 300\n",
      "        \n",
      "Begin training, total epochs: 300\n",
      "[ Epoch 1, Train ] | Loss:3.91201 Time:0.064387\n",
      "[ Epoch 1, Val ] | Loss:3.91222 Time:0.005049\n",
      "save model with val loss 3.912\n",
      "[ Epoch 2, Train ] | Loss:3.90731 Time:0.061587\n",
      "[ Epoch 2, Val ] | Loss:3.93015 Time:0.005001\n",
      "[ Epoch 3, Train ] | Loss:3.90791 Time:0.060946\n",
      "[ Epoch 3, Val ] | Loss:3.91050 Time:0.004984\n",
      "save model with val loss 3.910\n",
      "[ Epoch 4, Train ] | Loss:3.87875 Time:0.059920\n",
      "[ Epoch 4, Val ] | Loss:3.88989 Time:0.005018\n",
      "save model with val loss 3.890\n",
      "[ Epoch 5, Train ] | Loss:3.84631 Time:0.059920\n",
      "[ Epoch 5, Val ] | Loss:3.86144 Time:0.005011\n",
      "save model with val loss 3.861\n",
      "[ Epoch 6, Train ] | Loss:3.81142 Time:0.062772\n",
      "[ Epoch 6, Val ] | Loss:3.79741 Time:0.004947\n",
      "save model with val loss 3.797\n",
      "[ Epoch 7, Train ] | Loss:3.77727 Time:0.060597\n",
      "[ Epoch 7, Val ] | Loss:3.77851 Time:0.005009\n",
      "save model with val loss 3.779\n",
      "[ Epoch 8, Train ] | Loss:3.72347 Time:0.060154\n",
      "[ Epoch 8, Val ] | Loss:3.76225 Time:0.005008\n",
      "save model with val loss 3.762\n",
      "[ Epoch 9, Train ] | Loss:3.72835 Time:0.062330\n",
      "[ Epoch 9, Val ] | Loss:3.75407 Time:0.004949\n",
      "save model with val loss 3.754\n",
      "[ Epoch 10, Train ] | Loss:3.73602 Time:0.060392\n",
      "[ Epoch 10, Val ] | Loss:3.77739 Time:0.008022\n",
      "[ Epoch 11, Train ] | Loss:3.72407 Time:0.060390\n",
      "[ Epoch 11, Val ] | Loss:3.74749 Time:0.005053\n",
      "save model with val loss 3.747\n",
      "[ Epoch 12, Train ] | Loss:3.73210 Time:0.060242\n",
      "[ Epoch 12, Val ] | Loss:3.76257 Time:0.004930\n",
      "[ Epoch 13, Train ] | Loss:3.69673 Time:0.061320\n",
      "[ Epoch 13, Val ] | Loss:3.74063 Time:0.005082\n",
      "save model with val loss 3.741\n",
      "[ Epoch 14, Train ] | Loss:3.69512 Time:0.060078\n",
      "[ Epoch 14, Val ] | Loss:3.69234 Time:0.004997\n",
      "save model with val loss 3.692\n",
      "[ Epoch 15, Train ] | Loss:3.71669 Time:0.062442\n",
      "[ Epoch 15, Val ] | Loss:3.70005 Time:0.004935\n",
      "[ Epoch 16, Train ] | Loss:3.72399 Time:0.061265\n",
      "[ Epoch 16, Val ] | Loss:3.69776 Time:0.005201\n",
      "[ Epoch 17, Train ] | Loss:3.68785 Time:0.061754\n",
      "[ Epoch 17, Val ] | Loss:3.68924 Time:0.005083\n",
      "save model with val loss 3.689\n",
      "[ Epoch 18, Train ] | Loss:3.65269 Time:0.062131\n",
      "[ Epoch 18, Val ] | Loss:3.73119 Time:0.004928\n",
      "[ Epoch 19, Train ] | Loss:3.64115 Time:0.061703\n",
      "[ Epoch 19, Val ] | Loss:3.69819 Time:0.005080\n",
      "[ Epoch 20, Train ] | Loss:3.65271 Time:0.061364\n",
      "[ Epoch 20, Val ] | Loss:3.68205 Time:0.005074\n",
      "save model with val loss 3.682\n",
      "[ Epoch 21, Train ] | Loss:3.65651 Time:0.062059\n",
      "[ Epoch 21, Val ] | Loss:3.71421 Time:0.004945\n",
      "[ Epoch 22, Train ] | Loss:3.65082 Time:0.061478\n",
      "[ Epoch 22, Val ] | Loss:3.67911 Time:0.005082\n",
      "save model with val loss 3.679\n",
      "[ Epoch 23, Train ] | Loss:3.62500 Time:0.059895\n",
      "[ Epoch 23, Val ] | Loss:3.71330 Time:0.007380\n",
      "[ Epoch 24, Train ] | Loss:3.64689 Time:0.063421\n",
      "[ Epoch 24, Val ] | Loss:3.72768 Time:0.005213\n",
      "[ Epoch 25, Train ] | Loss:3.62171 Time:0.059238\n",
      "[ Epoch 25, Val ] | Loss:3.71732 Time:0.005098\n",
      "[ Epoch 26, Train ] | Loss:3.62472 Time:0.059317\n",
      "[ Epoch 26, Val ] | Loss:3.68841 Time:0.005084\n",
      "[ Epoch 27, Train ] | Loss:3.58383 Time:0.057302\n",
      "[ Epoch 27, Val ] | Loss:3.69504 Time:0.005099\n",
      "[ Epoch 28, Train ] | Loss:3.60480 Time:0.061025\n",
      "[ Epoch 28, Val ] | Loss:3.71716 Time:0.005024\n",
      "[ Epoch 29, Train ] | Loss:3.58946 Time:0.059114\n",
      "[ Epoch 29, Val ] | Loss:3.73115 Time:0.005092\n",
      "[ Epoch 30, Train ] | Loss:3.59974 Time:0.059367\n",
      "[ Epoch 30, Val ] | Loss:3.71577 Time:0.005095\n",
      "[ Epoch 31, Train ] | Loss:3.62245 Time:0.059279\n",
      "[ Epoch 31, Val ] | Loss:3.70615 Time:0.006232\n",
      "[ Epoch 32, Train ] | Loss:3.61144 Time:0.059846\n",
      "[ Epoch 32, Val ] | Loss:3.70851 Time:0.005223\n",
      "[ Epoch 33, Train ] | Loss:3.61621 Time:0.059265\n",
      "[ Epoch 33, Val ] | Loss:3.72783 Time:0.005206\n",
      "[ Epoch 34, Train ] | Loss:3.61898 Time:0.056461\n",
      "[ Epoch 34, Val ] | Loss:3.72345 Time:0.005098\n",
      "[ Epoch 35, Train ] | Loss:3.62276 Time:0.059096\n",
      "[ Epoch 35, Val ] | Loss:3.72522 Time:0.005035\n",
      "[ Epoch 36, Train ] | Loss:3.60735 Time:0.059237\n",
      "[ Epoch 36, Val ] | Loss:3.70248 Time:0.005113\n",
      "[ Epoch 37, Train ] | Loss:3.60100 Time:0.059214\n",
      "[ Epoch 37, Val ] | Loss:3.69715 Time:0.005220\n",
      "[ Epoch 38, Train ] | Loss:3.61246 Time:0.059034\n",
      "[ Epoch 38, Val ] | Loss:3.70409 Time:0.005131\n",
      "[ Epoch 39, Train ] | Loss:3.58657 Time:0.060952\n",
      "[ Epoch 39, Val ] | Loss:3.71790 Time:0.005087\n",
      "[ Epoch 40, Train ] | Loss:3.60374 Time:0.061132\n",
      "[ Epoch 40, Val ] | Loss:3.69761 Time:0.005101\n",
      "[ Epoch 41, Train ] | Loss:3.57058 Time:0.059309\n",
      "[ Epoch 41, Val ] | Loss:3.68378 Time:0.005086\n",
      "[ Epoch 42, Train ] | Loss:3.61409 Time:0.061717\n",
      "[ Epoch 42, Val ] | Loss:3.67775 Time:0.005215\n",
      "save model with val loss 3.678\n",
      "[ Epoch 43, Train ] | Loss:3.59709 Time:0.060894\n",
      "[ Epoch 43, Val ] | Loss:3.70535 Time:0.005025\n",
      "[ Epoch 44, Train ] | Loss:3.60467 Time:0.061193\n",
      "[ Epoch 44, Val ] | Loss:3.72050 Time:0.005083\n",
      "[ Epoch 45, Train ] | Loss:3.61362 Time:0.063214\n",
      "[ Epoch 45, Val ] | Loss:3.72962 Time:0.005013\n",
      "[ Epoch 46, Train ] | Loss:3.58836 Time:0.060983\n",
      "[ Epoch 46, Val ] | Loss:3.69138 Time:0.005111\n",
      "[ Epoch 47, Train ] | Loss:3.59631 Time:0.060945\n",
      "[ Epoch 47, Val ] | Loss:3.66625 Time:0.005099\n",
      "save model with val loss 3.666\n",
      "[ Epoch 48, Train ] | Loss:3.60522 Time:0.061648\n",
      "[ Epoch 48, Val ] | Loss:3.66471 Time:0.004973\n",
      "save model with val loss 3.665\n",
      "[ Epoch 49, Train ] | Loss:3.59046 Time:0.059480\n",
      "[ Epoch 49, Val ] | Loss:3.66810 Time:0.005042\n",
      "[ Epoch 50, Train ] | Loss:3.59758 Time:0.061128\n",
      "[ Epoch 50, Val ] | Loss:3.67725 Time:0.005099\n",
      "[ Epoch 51, Train ] | Loss:3.58996 Time:0.065366\n",
      "[ Epoch 51, Val ] | Loss:3.69455 Time:0.005044\n",
      "[ Epoch 52, Train ] | Loss:3.58363 Time:0.060978\n",
      "[ Epoch 52, Val ] | Loss:3.70523 Time:0.005224\n",
      "[ Epoch 53, Train ] | Loss:3.60129 Time:0.061219\n",
      "[ Epoch 53, Val ] | Loss:3.69751 Time:0.005103\n",
      "[ Epoch 54, Train ] | Loss:3.57765 Time:0.061391\n",
      "[ Epoch 54, Val ] | Loss:3.69610 Time:0.004144\n",
      "[ Epoch 55, Train ] | Loss:3.55469 Time:0.060741\n",
      "[ Epoch 55, Val ] | Loss:3.68336 Time:0.005097\n",
      "[ Epoch 56, Train ] | Loss:3.59042 Time:0.060611\n",
      "[ Epoch 56, Val ] | Loss:3.67584 Time:0.005105\n",
      "[ Epoch 57, Train ] | Loss:3.59084 Time:0.061059\n",
      "[ Epoch 57, Val ] | Loss:3.66578 Time:0.005206\n",
      "[ Epoch 58, Train ] | Loss:3.56555 Time:0.060978\n",
      "[ Epoch 58, Val ] | Loss:3.67396 Time:0.005101\n",
      "[ Epoch 59, Train ] | Loss:3.57845 Time:0.060653\n",
      "[ Epoch 59, Val ] | Loss:3.65457 Time:0.005098\n",
      "save model with val loss 3.655\n",
      "[ Epoch 60, Train ] | Loss:3.59050 Time:0.061352\n",
      "[ Epoch 60, Val ] | Loss:3.65525 Time:0.004971\n",
      "[ Epoch 61, Train ] | Loss:3.57189 Time:0.060987\n",
      "[ Epoch 61, Val ] | Loss:3.65276 Time:0.005070\n",
      "save model with val loss 3.653\n",
      "[ Epoch 62, Train ] | Loss:3.56265 Time:0.059419\n",
      "[ Epoch 62, Val ] | Loss:3.62523 Time:0.005026\n",
      "save model with val loss 3.625\n",
      "[ Epoch 63, Train ] | Loss:3.53922 Time:0.059640\n",
      "[ Epoch 63, Val ] | Loss:3.60986 Time:0.004982\n",
      "save model with val loss 3.610\n",
      "[ Epoch 64, Train ] | Loss:3.54939 Time:0.059017\n",
      "[ Epoch 64, Val ] | Loss:3.66598 Time:0.005030\n",
      "[ Epoch 65, Train ] | Loss:3.58069 Time:0.060329\n",
      "[ Epoch 65, Val ] | Loss:3.68408 Time:0.005087\n",
      "[ Epoch 66, Train ] | Loss:3.58152 Time:0.063086\n",
      "[ Epoch 66, Val ] | Loss:3.71358 Time:0.005039\n",
      "[ Epoch 67, Train ] | Loss:3.57099 Time:0.060929\n",
      "[ Epoch 67, Val ] | Loss:3.65642 Time:0.005103\n",
      "[ Epoch 68, Train ] | Loss:3.54386 Time:0.060906\n",
      "[ Epoch 68, Val ] | Loss:3.62857 Time:0.005099\n",
      "[ Epoch 69, Train ] | Loss:3.52716 Time:0.060696\n",
      "[ Epoch 69, Val ] | Loss:3.63170 Time:0.004934\n",
      "[ Epoch 70, Train ] | Loss:3.54163 Time:0.060777\n",
      "[ Epoch 70, Val ] | Loss:3.64679 Time:0.005103\n",
      "[ Epoch 71, Train ] | Loss:3.54165 Time:0.060743\n",
      "[ Epoch 71, Val ] | Loss:3.63881 Time:0.005098\n",
      "[ Epoch 72, Train ] | Loss:3.53759 Time:0.060949\n",
      "[ Epoch 72, Val ] | Loss:3.64089 Time:0.004565\n",
      "[ Epoch 73, Train ] | Loss:3.55924 Time:0.061003\n",
      "[ Epoch 73, Val ] | Loss:3.64248 Time:0.005090\n",
      "[ Epoch 74, Train ] | Loss:3.53439 Time:0.060649\n",
      "[ Epoch 74, Val ] | Loss:3.62514 Time:0.005104\n",
      "[ Epoch 75, Train ] | Loss:3.56011 Time:0.061878\n",
      "[ Epoch 75, Val ] | Loss:3.62455 Time:0.006166\n",
      "[ Epoch 76, Train ] | Loss:3.50651 Time:0.060814\n",
      "[ Epoch 76, Val ] | Loss:3.61968 Time:0.005093\n",
      "[ Epoch 77, Train ] | Loss:3.52639 Time:0.060544\n",
      "[ Epoch 77, Val ] | Loss:3.62932 Time:0.005099\n",
      "[ Epoch 78, Train ] | Loss:3.51181 Time:0.060831\n",
      "[ Epoch 78, Val ] | Loss:3.63026 Time:0.004735\n",
      "[ Epoch 79, Train ] | Loss:3.53084 Time:0.060792\n",
      "[ Epoch 79, Val ] | Loss:3.62543 Time:0.005098\n",
      "[ Epoch 80, Train ] | Loss:3.54679 Time:0.060726\n",
      "[ Epoch 80, Val ] | Loss:3.65924 Time:0.005117\n",
      "[ Epoch 81, Train ] | Loss:3.52036 Time:0.060724\n",
      "[ Epoch 81, Val ] | Loss:3.61263 Time:0.005178\n",
      "[ Epoch 82, Train ] | Loss:3.52120 Time:0.060743\n",
      "[ Epoch 82, Val ] | Loss:3.62289 Time:0.005087\n",
      "[ Epoch 83, Train ] | Loss:3.50722 Time:0.060621\n",
      "[ Epoch 83, Val ] | Loss:3.61618 Time:0.005095\n",
      "[ Epoch 84, Train ] | Loss:3.50457 Time:0.060554\n",
      "[ Epoch 84, Val ] | Loss:3.62487 Time:0.004954\n",
      "[ Epoch 85, Train ] | Loss:3.53696 Time:0.058149\n",
      "[ Epoch 85, Val ] | Loss:3.66704 Time:0.005224\n",
      "[ Epoch 86, Train ] | Loss:3.52270 Time:0.058812\n",
      "[ Epoch 86, Val ] | Loss:3.63796 Time:0.005112\n",
      "[ Epoch 87, Train ] | Loss:3.53423 Time:0.060606\n",
      "[ Epoch 87, Val ] | Loss:3.61698 Time:0.005126\n",
      "[ Epoch 88, Train ] | Loss:3.49673 Time:0.060200\n",
      "[ Epoch 88, Val ] | Loss:3.63546 Time:0.005100\n",
      "[ Epoch 89, Train ] | Loss:3.52160 Time:0.060452\n",
      "[ Epoch 89, Val ] | Loss:3.64026 Time:0.005104\n",
      "[ Epoch 90, Train ] | Loss:3.51401 Time:0.060412\n",
      "[ Epoch 90, Val ] | Loss:3.62136 Time:0.006020\n",
      "[ Epoch 91, Train ] | Loss:3.52836 Time:0.058972\n",
      "[ Epoch 91, Val ] | Loss:3.60008 Time:0.005098\n",
      "save model with val loss 3.600\n",
      "[ Epoch 92, Train ] | Loss:3.50110 Time:0.060610\n",
      "[ Epoch 92, Val ] | Loss:3.61062 Time:0.004999\n",
      "[ Epoch 93, Train ] | Loss:3.56017 Time:0.060255\n",
      "[ Epoch 93, Val ] | Loss:3.62703 Time:0.005013\n",
      "[ Epoch 94, Train ] | Loss:3.49389 Time:0.060459\n",
      "[ Epoch 94, Val ] | Loss:3.59792 Time:0.005122\n",
      "save model with val loss 3.598\n",
      "[ Epoch 95, Train ] | Loss:3.52775 Time:0.061271\n",
      "[ Epoch 95, Val ] | Loss:3.61051 Time:0.005039\n",
      "[ Epoch 96, Train ] | Loss:3.50350 Time:0.062503\n",
      "[ Epoch 96, Val ] | Loss:3.59406 Time:0.005032\n",
      "save model with val loss 3.594\n",
      "[ Epoch 97, Train ] | Loss:3.51109 Time:0.061268\n",
      "[ Epoch 97, Val ] | Loss:3.59886 Time:0.005041\n",
      "[ Epoch 98, Train ] | Loss:3.52724 Time:0.060358\n",
      "[ Epoch 98, Val ] | Loss:3.60805 Time:0.005209\n",
      "[ Epoch 99, Train ] | Loss:3.53170 Time:0.060222\n",
      "[ Epoch 99, Val ] | Loss:3.63352 Time:0.005014\n",
      "[ Epoch 100, Train ] | Loss:3.49607 Time:0.060231\n",
      "[ Epoch 100, Val ] | Loss:3.62163 Time:0.005101\n",
      "[ Epoch 101, Train ] | Loss:3.48833 Time:0.060200\n",
      "[ Epoch 101, Val ] | Loss:3.60961 Time:0.005227\n",
      "[ Epoch 102, Train ] | Loss:3.50531 Time:0.060349\n",
      "[ Epoch 102, Val ] | Loss:3.60624 Time:0.005147\n",
      "[ Epoch 103, Train ] | Loss:3.52530 Time:0.060181\n",
      "[ Epoch 103, Val ] | Loss:3.61115 Time:0.005093\n",
      "[ Epoch 104, Train ] | Loss:3.46704 Time:0.060384\n",
      "[ Epoch 104, Val ] | Loss:3.61888 Time:0.005221\n",
      "[ Epoch 105, Train ] | Loss:3.53444 Time:0.061413\n",
      "[ Epoch 105, Val ] | Loss:3.61964 Time:0.006537\n",
      "[ Epoch 106, Train ] | Loss:3.51649 Time:0.060007\n",
      "[ Epoch 106, Val ] | Loss:3.61713 Time:0.005097\n",
      "[ Epoch 107, Train ] | Loss:3.50168 Time:0.060251\n",
      "[ Epoch 107, Val ] | Loss:3.62249 Time:0.005097\n",
      "[ Epoch 108, Train ] | Loss:3.52043 Time:0.060375\n",
      "[ Epoch 108, Val ] | Loss:3.61705 Time:0.004979\n",
      "[ Epoch 109, Train ] | Loss:3.50853 Time:0.059867\n",
      "[ Epoch 109, Val ] | Loss:3.60956 Time:0.005104\n",
      "[ Epoch 110, Train ] | Loss:3.51264 Time:0.060094\n",
      "[ Epoch 110, Val ] | Loss:3.62321 Time:0.005109\n",
      "[ Epoch 111, Train ] | Loss:3.50117 Time:0.061335\n",
      "[ Epoch 111, Val ] | Loss:3.62759 Time:0.005461\n",
      "[ Epoch 112, Train ] | Loss:3.49807 Time:0.060518\n",
      "[ Epoch 112, Val ] | Loss:3.62630 Time:0.005215\n",
      "[ Epoch 113, Train ] | Loss:3.48418 Time:0.059924\n",
      "[ Epoch 113, Val ] | Loss:3.58902 Time:0.005104\n",
      "save model with val loss 3.589\n",
      "[ Epoch 114, Train ] | Loss:3.49837 Time:0.062402\n",
      "[ Epoch 114, Val ] | Loss:3.57287 Time:0.004999\n",
      "save model with val loss 3.573\n",
      "[ Epoch 115, Train ] | Loss:3.49116 Time:0.060775\n",
      "[ Epoch 115, Val ] | Loss:3.58137 Time:0.005036\n",
      "[ Epoch 116, Train ] | Loss:3.48949 Time:0.060012\n",
      "[ Epoch 116, Val ] | Loss:3.56700 Time:0.005100\n",
      "save model with val loss 3.567\n",
      "[ Epoch 117, Train ] | Loss:3.45732 Time:0.061039\n",
      "[ Epoch 117, Val ] | Loss:3.55606 Time:0.004968\n",
      "save model with val loss 3.556\n",
      "[ Epoch 118, Train ] | Loss:3.44883 Time:0.062026\n",
      "[ Epoch 118, Val ] | Loss:3.53318 Time:0.005908\n",
      "save model with val loss 3.533\n",
      "[ Epoch 119, Train ] | Loss:3.47737 Time:0.060660\n",
      "[ Epoch 119, Val ] | Loss:3.53389 Time:0.005013\n",
      "[ Epoch 120, Train ] | Loss:3.46425 Time:0.062416\n",
      "[ Epoch 120, Val ] | Loss:3.53639 Time:0.005030\n",
      "[ Epoch 121, Train ] | Loss:3.42845 Time:0.060241\n",
      "[ Epoch 121, Val ] | Loss:3.51806 Time:0.005108\n",
      "save model with val loss 3.518\n",
      "[ Epoch 122, Train ] | Loss:3.42398 Time:0.060747\n",
      "[ Epoch 122, Val ] | Loss:3.52915 Time:0.005032\n",
      "[ Epoch 123, Train ] | Loss:3.44308 Time:0.061887\n",
      "[ Epoch 123, Val ] | Loss:3.54185 Time:0.005034\n",
      "[ Epoch 124, Train ] | Loss:3.44164 Time:0.060073\n",
      "[ Epoch 124, Val ] | Loss:3.51780 Time:0.005105\n",
      "save model with val loss 3.518\n",
      "[ Epoch 125, Train ] | Loss:3.44257 Time:0.060493\n",
      "[ Epoch 125, Val ] | Loss:3.49567 Time:0.005019\n",
      "save model with val loss 3.496\n",
      "[ Epoch 126, Train ] | Loss:3.43196 Time:0.061185\n",
      "[ Epoch 126, Val ] | Loss:3.49024 Time:0.004992\n",
      "save model with val loss 3.490\n",
      "[ Epoch 127, Train ] | Loss:3.42683 Time:0.061450\n",
      "[ Epoch 127, Val ] | Loss:3.49610 Time:0.005023\n",
      "[ Epoch 128, Train ] | Loss:3.44068 Time:0.060307\n",
      "[ Epoch 128, Val ] | Loss:3.48460 Time:0.005077\n",
      "save model with val loss 3.485\n",
      "[ Epoch 129, Train ] | Loss:3.45730 Time:0.060861\n",
      "[ Epoch 129, Val ] | Loss:3.48093 Time:0.004985\n",
      "save model with val loss 3.481\n",
      "[ Epoch 130, Train ] | Loss:3.44941 Time:0.062957\n",
      "[ Epoch 130, Val ] | Loss:3.49211 Time:0.005003\n",
      "[ Epoch 131, Train ] | Loss:3.43112 Time:0.059917\n",
      "[ Epoch 131, Val ] | Loss:3.47874 Time:0.005092\n",
      "save model with val loss 3.479\n",
      "[ Epoch 132, Train ] | Loss:3.43227 Time:0.062782\n",
      "[ Epoch 132, Val ] | Loss:3.47405 Time:0.004972\n",
      "save model with val loss 3.474\n",
      "[ Epoch 133, Train ] | Loss:3.45657 Time:0.060911\n",
      "[ Epoch 133, Val ] | Loss:3.47172 Time:0.005023\n",
      "save model with val loss 3.472\n",
      "[ Epoch 134, Train ] | Loss:3.45097 Time:0.060585\n",
      "[ Epoch 134, Val ] | Loss:3.47149 Time:0.005037\n",
      "save model with val loss 3.471\n",
      "[ Epoch 135, Train ] | Loss:3.42706 Time:0.061000\n",
      "[ Epoch 135, Val ] | Loss:3.46829 Time:0.004988\n",
      "save model with val loss 3.468\n",
      "[ Epoch 136, Train ] | Loss:3.42132 Time:0.060876\n",
      "[ Epoch 136, Val ] | Loss:3.50645 Time:0.005188\n",
      "[ Epoch 137, Train ] | Loss:3.42897 Time:0.059732\n",
      "[ Epoch 137, Val ] | Loss:3.46330 Time:0.005086\n",
      "save model with val loss 3.463\n",
      "[ Epoch 138, Train ] | Loss:3.41986 Time:0.058172\n",
      "[ Epoch 138, Val ] | Loss:3.45205 Time:0.005196\n",
      "save model with val loss 3.452\n",
      "[ Epoch 139, Train ] | Loss:3.43285 Time:0.060702\n",
      "[ Epoch 139, Val ] | Loss:3.47542 Time:0.005048\n",
      "[ Epoch 140, Train ] | Loss:3.40915 Time:0.059896\n",
      "[ Epoch 140, Val ] | Loss:3.46693 Time:0.005087\n",
      "[ Epoch 141, Train ] | Loss:3.40813 Time:0.062653\n",
      "[ Epoch 141, Val ] | Loss:3.43360 Time:0.005026\n",
      "save model with val loss 3.434\n",
      "[ Epoch 142, Train ] | Loss:3.40697 Time:0.061937\n",
      "[ Epoch 142, Val ] | Loss:3.44359 Time:0.004698\n",
      "[ Epoch 143, Train ] | Loss:3.40371 Time:0.059984\n",
      "[ Epoch 143, Val ] | Loss:3.44807 Time:0.005097\n",
      "[ Epoch 144, Train ] | Loss:3.41763 Time:0.057425\n",
      "[ Epoch 144, Val ] | Loss:3.43639 Time:0.004968\n",
      "[ Epoch 145, Train ] | Loss:3.43439 Time:0.059420\n",
      "[ Epoch 145, Val ] | Loss:3.43509 Time:0.005216\n",
      "[ Epoch 146, Train ] | Loss:3.41690 Time:0.059215\n",
      "[ Epoch 146, Val ] | Loss:3.44367 Time:0.005080\n",
      "[ Epoch 147, Train ] | Loss:3.41261 Time:0.059064\n",
      "[ Epoch 147, Val ] | Loss:3.44192 Time:0.004958\n",
      "[ Epoch 148, Train ] | Loss:3.41156 Time:0.059402\n",
      "[ Epoch 148, Val ] | Loss:3.43967 Time:0.005077\n",
      "[ Epoch 149, Train ] | Loss:3.41732 Time:0.059357\n",
      "[ Epoch 149, Val ] | Loss:3.44011 Time:0.005101\n",
      "[ Epoch 150, Train ] | Loss:3.40903 Time:0.059496\n",
      "[ Epoch 150, Val ] | Loss:3.43932 Time:0.005146\n",
      "[ Epoch 151, Train ] | Loss:3.39341 Time:0.059588\n",
      "[ Epoch 151, Val ] | Loss:3.44286 Time:0.005091\n",
      "[ Epoch 152, Train ] | Loss:3.41393 Time:0.059925\n",
      "[ Epoch 152, Val ] | Loss:3.45576 Time:0.005094\n",
      "[ Epoch 153, Train ] | Loss:3.41194 Time:0.059493\n",
      "[ Epoch 153, Val ] | Loss:3.45176 Time:0.005062\n",
      "[ Epoch 154, Train ] | Loss:3.42871 Time:0.059475\n",
      "[ Epoch 154, Val ] | Loss:3.44601 Time:0.005099\n",
      "[ Epoch 155, Train ] | Loss:3.39919 Time:0.059537\n",
      "[ Epoch 155, Val ] | Loss:3.43797 Time:0.005103\n",
      "[ Epoch 156, Train ] | Loss:3.39611 Time:0.059750\n",
      "[ Epoch 156, Val ] | Loss:3.45651 Time:0.004941\n",
      "[ Epoch 157, Train ] | Loss:3.38593 Time:0.059616\n",
      "[ Epoch 157, Val ] | Loss:3.38095 Time:0.005226\n",
      "save model with val loss 3.381\n",
      "[ Epoch 158, Train ] | Loss:3.39302 Time:0.059137\n",
      "[ Epoch 158, Val ] | Loss:3.34741 Time:0.005028\n",
      "save model with val loss 3.347\n",
      "[ Epoch 159, Train ] | Loss:3.38559 Time:0.060354\n",
      "[ Epoch 159, Val ] | Loss:3.35387 Time:0.005179\n",
      "[ Epoch 160, Train ] | Loss:3.39340 Time:0.059530\n",
      "[ Epoch 160, Val ] | Loss:3.39280 Time:0.005205\n",
      "[ Epoch 161, Train ] | Loss:3.39688 Time:0.059340\n",
      "[ Epoch 161, Val ] | Loss:3.37456 Time:0.005092\n",
      "[ Epoch 162, Train ] | Loss:3.38885 Time:0.059469\n",
      "[ Epoch 162, Val ] | Loss:3.38338 Time:0.005055\n",
      "[ Epoch 163, Train ] | Loss:3.34695 Time:0.059236\n",
      "[ Epoch 163, Val ] | Loss:3.38667 Time:0.005136\n",
      "[ Epoch 164, Train ] | Loss:3.35411 Time:0.059282\n",
      "[ Epoch 164, Val ] | Loss:3.37500 Time:0.005089\n",
      "[ Epoch 165, Train ] | Loss:3.36971 Time:0.059645\n",
      "[ Epoch 165, Val ] | Loss:3.46448 Time:0.005208\n",
      "[ Epoch 166, Train ] | Loss:3.40216 Time:0.059529\n",
      "[ Epoch 166, Val ] | Loss:3.47017 Time:0.005089\n",
      "[ Epoch 167, Train ] | Loss:3.39920 Time:0.059308\n",
      "[ Epoch 167, Val ] | Loss:3.42409 Time:0.005101\n",
      "[ Epoch 168, Train ] | Loss:3.38355 Time:0.059491\n",
      "[ Epoch 168, Val ] | Loss:3.39474 Time:0.005051\n",
      "[ Epoch 169, Train ] | Loss:3.36991 Time:0.059662\n",
      "[ Epoch 169, Val ] | Loss:3.37501 Time:0.005200\n",
      "[ Epoch 170, Train ] | Loss:3.34507 Time:0.059519\n",
      "[ Epoch 170, Val ] | Loss:3.37097 Time:0.005114\n",
      "[ Epoch 171, Train ] | Loss:3.35270 Time:0.059435\n",
      "[ Epoch 171, Val ] | Loss:3.37369 Time:0.004817\n",
      "[ Epoch 172, Train ] | Loss:3.40517 Time:0.057545\n",
      "[ Epoch 172, Val ] | Loss:3.38528 Time:0.005218\n",
      "[ Epoch 173, Train ] | Loss:3.37416 Time:0.059309\n",
      "[ Epoch 173, Val ] | Loss:3.35989 Time:0.005116\n",
      "[ Epoch 174, Train ] | Loss:3.34328 Time:0.059359\n",
      "[ Epoch 174, Val ] | Loss:3.38446 Time:0.005102\n",
      "[ Epoch 175, Train ] | Loss:3.37778 Time:0.059775\n",
      "[ Epoch 175, Val ] | Loss:3.38869 Time:0.005108\n",
      "[ Epoch 176, Train ] | Loss:3.35548 Time:0.058990\n",
      "[ Epoch 176, Val ] | Loss:3.38166 Time:0.005225\n",
      "[ Epoch 177, Train ] | Loss:3.36869 Time:0.058899\n",
      "[ Epoch 177, Val ] | Loss:3.36330 Time:0.005115\n",
      "[ Epoch 178, Train ] | Loss:3.36862 Time:0.059800\n",
      "[ Epoch 178, Val ] | Loss:3.35748 Time:0.005099\n",
      "[ Epoch 179, Train ] | Loss:3.38163 Time:0.059257\n",
      "[ Epoch 179, Val ] | Loss:3.36014 Time:0.005085\n",
      "[ Epoch 180, Train ] | Loss:3.35821 Time:0.059028\n",
      "[ Epoch 180, Val ] | Loss:3.35257 Time:0.007209\n",
      "[ Epoch 181, Train ] | Loss:3.35255 Time:0.059228\n",
      "[ Epoch 181, Val ] | Loss:3.34508 Time:0.005092\n",
      "save model with val loss 3.345\n",
      "[ Epoch 182, Train ] | Loss:3.36687 Time:0.061200\n",
      "[ Epoch 182, Val ] | Loss:3.39758 Time:0.005023\n",
      "[ Epoch 183, Train ] | Loss:3.35298 Time:0.061241\n",
      "[ Epoch 183, Val ] | Loss:3.35748 Time:0.005032\n",
      "[ Epoch 184, Train ] | Loss:3.34942 Time:0.058771\n",
      "[ Epoch 184, Val ] | Loss:3.35320 Time:0.005221\n",
      "[ Epoch 185, Train ] | Loss:3.39755 Time:0.059485\n",
      "[ Epoch 185, Val ] | Loss:3.34819 Time:0.005089\n",
      "[ Epoch 186, Train ] | Loss:3.33238 Time:0.059477\n",
      "[ Epoch 186, Val ] | Loss:3.37404 Time:0.004824\n",
      "[ Epoch 187, Train ] | Loss:3.35842 Time:0.059230\n",
      "[ Epoch 187, Val ] | Loss:3.35035 Time:0.005095\n",
      "[ Epoch 188, Train ] | Loss:3.36583 Time:0.058873\n",
      "[ Epoch 188, Val ] | Loss:3.34692 Time:0.005102\n",
      "[ Epoch 189, Train ] | Loss:3.32809 Time:0.061236\n",
      "[ Epoch 189, Val ] | Loss:3.36015 Time:0.004964\n",
      "[ Epoch 190, Train ] | Loss:3.37625 Time:0.059177\n",
      "[ Epoch 190, Val ] | Loss:3.36296 Time:0.005080\n",
      "[ Epoch 191, Train ] | Loss:3.33544 Time:0.061032\n",
      "[ Epoch 191, Val ] | Loss:3.37338 Time:0.005099\n",
      "[ Epoch 192, Train ] | Loss:3.36748 Time:0.061342\n",
      "[ Epoch 192, Val ] | Loss:3.37979 Time:0.005050\n",
      "[ Epoch 193, Train ] | Loss:3.39272 Time:0.060963\n",
      "[ Epoch 193, Val ] | Loss:3.38290 Time:0.005101\n",
      "[ Epoch 194, Train ] | Loss:3.32336 Time:0.060753\n",
      "[ Epoch 194, Val ] | Loss:3.36946 Time:0.005100\n",
      "[ Epoch 195, Train ] | Loss:3.34379 Time:0.061135\n",
      "[ Epoch 195, Val ] | Loss:3.36110 Time:0.005042\n",
      "[ Epoch 196, Train ] | Loss:3.34037 Time:0.060929\n",
      "[ Epoch 196, Val ] | Loss:3.35729 Time:0.005113\n",
      "[ Epoch 197, Train ] | Loss:3.35154 Time:0.061345\n",
      "[ Epoch 197, Val ] | Loss:3.35775 Time:0.005109\n",
      "[ Epoch 198, Train ] | Loss:3.34499 Time:0.061026\n",
      "[ Epoch 198, Val ] | Loss:3.35734 Time:0.005036\n",
      "[ Epoch 199, Train ] | Loss:3.36746 Time:0.061002\n",
      "[ Epoch 199, Val ] | Loss:3.35652 Time:0.005092\n",
      "[ Epoch 200, Train ] | Loss:3.36324 Time:0.061097\n",
      "[ Epoch 200, Val ] | Loss:3.35998 Time:0.005093\n",
      "[ Epoch 201, Train ] | Loss:3.33474 Time:0.060682\n",
      "[ Epoch 201, Val ] | Loss:3.36304 Time:0.005041\n",
      "[ Epoch 202, Train ] | Loss:3.35091 Time:0.061033\n",
      "[ Epoch 202, Val ] | Loss:3.36506 Time:0.005094\n",
      "[ Epoch 203, Train ] | Loss:3.36237 Time:0.060778\n",
      "[ Epoch 203, Val ] | Loss:3.36549 Time:0.005094\n",
      "[ Epoch 204, Train ] | Loss:3.39440 Time:0.060761\n",
      "[ Epoch 204, Val ] | Loss:3.36402 Time:0.004917\n",
      "[ Epoch 205, Train ] | Loss:3.37325 Time:0.060838\n",
      "[ Epoch 205, Val ] | Loss:3.36122 Time:0.005086\n",
      "[ Epoch 206, Train ] | Loss:3.34923 Time:0.060874\n",
      "[ Epoch 206, Val ] | Loss:3.32585 Time:0.005095\n",
      "save model with val loss 3.326\n",
      "[ Epoch 207, Train ] | Loss:3.33539 Time:0.061012\n",
      "[ Epoch 207, Val ] | Loss:3.32397 Time:0.004975\n",
      "save model with val loss 3.324\n",
      "[ Epoch 208, Train ] | Loss:3.36950 Time:0.059346\n",
      "[ Epoch 208, Val ] | Loss:3.32923 Time:0.005042\n",
      "[ Epoch 209, Train ] | Loss:3.34053 Time:0.060672\n",
      "[ Epoch 209, Val ] | Loss:3.32491 Time:0.005196\n",
      "[ Epoch 210, Train ] | Loss:3.34676 Time:0.060881\n",
      "[ Epoch 210, Val ] | Loss:3.33265 Time:0.005042\n",
      "[ Epoch 211, Train ] | Loss:3.31165 Time:0.060869\n",
      "[ Epoch 211, Val ] | Loss:3.33676 Time:0.005102\n",
      "[ Epoch 212, Train ] | Loss:3.33402 Time:0.060752\n",
      "[ Epoch 212, Val ] | Loss:3.33611 Time:0.005116\n",
      "[ Epoch 213, Train ] | Loss:3.32135 Time:0.060415\n",
      "[ Epoch 213, Val ] | Loss:3.31059 Time:0.005043\n",
      "save model with val loss 3.311\n",
      "[ Epoch 214, Train ] | Loss:3.31878 Time:0.061634\n",
      "[ Epoch 214, Val ] | Loss:3.31073 Time:0.005032\n",
      "[ Epoch 215, Train ] | Loss:3.33583 Time:0.060803\n",
      "[ Epoch 215, Val ] | Loss:3.31056 Time:0.005090\n",
      "save model with val loss 3.311\n",
      "[ Epoch 216, Train ] | Loss:3.30494 Time:0.061435\n",
      "[ Epoch 216, Val ] | Loss:3.30657 Time:0.004987\n",
      "save model with val loss 3.307\n",
      "[ Epoch 217, Train ] | Loss:3.32162 Time:0.059184\n",
      "[ Epoch 217, Val ] | Loss:3.30685 Time:0.005025\n",
      "[ Epoch 218, Train ] | Loss:3.30611 Time:0.060764\n",
      "[ Epoch 218, Val ] | Loss:3.30932 Time:0.005100\n",
      "[ Epoch 219, Train ] | Loss:3.29603 Time:0.058482\n",
      "[ Epoch 219, Val ] | Loss:3.31200 Time:0.005210\n",
      "[ Epoch 220, Train ] | Loss:3.34917 Time:0.060835\n",
      "[ Epoch 220, Val ] | Loss:3.31395 Time:0.005105\n",
      "[ Epoch 221, Train ] | Loss:3.33208 Time:0.060192\n",
      "[ Epoch 221, Val ] | Loss:3.31765 Time:0.005097\n",
      "[ Epoch 222, Train ] | Loss:3.34512 Time:0.065148\n",
      "[ Epoch 222, Val ] | Loss:3.31942 Time:0.004997\n",
      "[ Epoch 223, Train ] | Loss:3.32234 Time:0.058450\n",
      "[ Epoch 223, Val ] | Loss:3.31923 Time:0.005077\n",
      "[ Epoch 224, Train ] | Loss:3.34130 Time:0.060491\n",
      "[ Epoch 224, Val ] | Loss:3.31822 Time:0.005089\n",
      "[ Epoch 225, Train ] | Loss:3.31716 Time:0.060322\n",
      "[ Epoch 225, Val ] | Loss:3.31763 Time:0.005047\n",
      "[ Epoch 226, Train ] | Loss:3.33336 Time:0.060754\n",
      "[ Epoch 226, Val ] | Loss:3.32012 Time:0.005104\n",
      "[ Epoch 227, Train ] | Loss:3.33357 Time:0.060799\n",
      "[ Epoch 227, Val ] | Loss:3.32074 Time:0.005091\n",
      "[ Epoch 228, Train ] | Loss:3.35844 Time:0.060197\n",
      "[ Epoch 228, Val ] | Loss:3.32240 Time:0.005042\n",
      "[ Epoch 229, Train ] | Loss:3.31994 Time:0.060588\n",
      "[ Epoch 229, Val ] | Loss:3.32744 Time:0.005106\n",
      "[ Epoch 230, Train ] | Loss:3.30737 Time:0.060272\n",
      "[ Epoch 230, Val ] | Loss:3.33507 Time:0.005115\n",
      "[ Epoch 231, Train ] | Loss:3.32256 Time:0.060499\n",
      "[ Epoch 231, Val ] | Loss:3.30684 Time:0.005026\n",
      "[ Epoch 232, Train ] | Loss:3.33562 Time:0.060306\n",
      "[ Epoch 232, Val ] | Loss:3.29299 Time:0.005115\n",
      "save model with val loss 3.293\n",
      "[ Epoch 233, Train ] | Loss:3.33403 Time:0.060600\n",
      "[ Epoch 233, Val ] | Loss:3.26683 Time:0.005175\n",
      "save model with val loss 3.267\n",
      "[ Epoch 234, Train ] | Loss:3.34104 Time:0.061459\n",
      "[ Epoch 234, Val ] | Loss:3.28455 Time:0.004963\n",
      "[ Epoch 235, Train ] | Loss:3.29570 Time:0.060287\n",
      "[ Epoch 235, Val ] | Loss:3.28999 Time:0.005084\n",
      "[ Epoch 236, Train ] | Loss:3.31271 Time:0.060368\n",
      "[ Epoch 236, Val ] | Loss:3.33012 Time:0.005240\n",
      "[ Epoch 237, Train ] | Loss:3.28967 Time:0.060081\n",
      "[ Epoch 237, Val ] | Loss:3.29896 Time:0.005023\n",
      "[ Epoch 238, Train ] | Loss:3.29078 Time:0.060153\n",
      "[ Epoch 238, Val ] | Loss:3.26157 Time:0.005101\n",
      "save model with val loss 3.262\n",
      "[ Epoch 239, Train ] | Loss:3.27903 Time:0.058920\n",
      "[ Epoch 239, Val ] | Loss:3.26396 Time:0.005211\n",
      "[ Epoch 240, Train ] | Loss:3.30202 Time:0.060402\n",
      "[ Epoch 240, Val ] | Loss:3.23773 Time:0.005034\n",
      "save model with val loss 3.238\n",
      "[ Epoch 241, Train ] | Loss:3.32632 Time:0.061062\n",
      "[ Epoch 241, Val ] | Loss:3.25012 Time:0.005038\n",
      "[ Epoch 242, Train ] | Loss:3.32142 Time:0.057921\n",
      "[ Epoch 242, Val ] | Loss:3.25915 Time:0.005108\n",
      "[ Epoch 243, Train ] | Loss:3.31332 Time:0.060316\n",
      "[ Epoch 243, Val ] | Loss:3.25109 Time:0.005034\n",
      "[ Epoch 244, Train ] | Loss:3.30544 Time:0.060133\n",
      "[ Epoch 244, Val ] | Loss:3.25122 Time:0.005106\n",
      "[ Epoch 245, Train ] | Loss:3.33323 Time:0.060329\n",
      "[ Epoch 245, Val ] | Loss:3.24700 Time:0.005204\n",
      "[ Epoch 246, Train ] | Loss:3.30838 Time:0.060278\n",
      "[ Epoch 246, Val ] | Loss:3.23471 Time:0.005217\n",
      "save model with val loss 3.235\n",
      "[ Epoch 247, Train ] | Loss:3.31087 Time:0.060609\n",
      "[ Epoch 247, Val ] | Loss:3.24586 Time:0.005206\n",
      "[ Epoch 248, Train ] | Loss:3.32818 Time:0.059171\n",
      "[ Epoch 248, Val ] | Loss:3.24514 Time:0.005082\n",
      "[ Epoch 249, Train ] | Loss:3.32062 Time:0.060053\n",
      "[ Epoch 249, Val ] | Loss:3.26189 Time:0.005195\n",
      "[ Epoch 250, Train ] | Loss:3.27802 Time:0.060447\n",
      "[ Epoch 250, Val ] | Loss:3.25596 Time:0.005107\n",
      "[ Epoch 251, Train ] | Loss:3.26742 Time:0.060465\n",
      "[ Epoch 251, Val ] | Loss:3.25731 Time:0.005100\n",
      "[ Epoch 252, Train ] | Loss:3.29229 Time:0.060352\n",
      "[ Epoch 252, Val ] | Loss:3.23519 Time:0.005063\n",
      "[ Epoch 253, Train ] | Loss:3.29395 Time:0.060571\n",
      "[ Epoch 253, Val ] | Loss:3.23488 Time:0.005109\n",
      "[ Epoch 254, Train ] | Loss:3.29178 Time:0.060108\n",
      "[ Epoch 254, Val ] | Loss:3.21800 Time:0.005097\n",
      "save model with val loss 3.218\n",
      "[ Epoch 255, Train ] | Loss:3.28978 Time:0.060827\n",
      "[ Epoch 255, Val ] | Loss:3.22624 Time:0.004981\n",
      "[ Epoch 256, Train ] | Loss:3.26959 Time:0.060138\n",
      "[ Epoch 256, Val ] | Loss:3.22497 Time:0.005091\n",
      "[ Epoch 257, Train ] | Loss:3.27774 Time:0.059887\n",
      "[ Epoch 257, Val ] | Loss:3.25167 Time:0.005105\n",
      "[ Epoch 258, Train ] | Loss:3.28522 Time:0.060269\n",
      "[ Epoch 258, Val ] | Loss:3.25850 Time:0.005203\n",
      "[ Epoch 259, Train ] | Loss:3.26181 Time:0.060023\n",
      "[ Epoch 259, Val ] | Loss:3.23576 Time:0.005110\n",
      "[ Epoch 260, Train ] | Loss:3.27803 Time:0.060035\n",
      "[ Epoch 260, Val ] | Loss:3.21497 Time:0.005113\n",
      "save model with val loss 3.215\n",
      "[ Epoch 261, Train ] | Loss:3.27415 Time:0.060840\n",
      "[ Epoch 261, Val ] | Loss:3.20577 Time:0.004972\n",
      "save model with val loss 3.206\n",
      "[ Epoch 262, Train ] | Loss:3.27177 Time:0.061148\n",
      "[ Epoch 262, Val ] | Loss:3.21264 Time:0.005039\n",
      "[ Epoch 263, Train ] | Loss:3.27863 Time:0.059667\n",
      "[ Epoch 263, Val ] | Loss:3.21226 Time:0.005077\n",
      "[ Epoch 264, Train ] | Loss:3.28104 Time:0.060109\n",
      "[ Epoch 264, Val ] | Loss:3.23530 Time:0.005026\n",
      "[ Epoch 265, Train ] | Loss:3.26134 Time:0.060225\n",
      "[ Epoch 265, Val ] | Loss:3.20923 Time:0.005096\n",
      "[ Epoch 266, Train ] | Loss:3.27521 Time:0.059867\n",
      "[ Epoch 266, Val ] | Loss:3.19391 Time:0.005100\n",
      "save model with val loss 3.194\n",
      "[ Epoch 267, Train ] | Loss:3.26274 Time:0.060692\n",
      "[ Epoch 267, Val ] | Loss:3.19052 Time:0.004967\n",
      "save model with val loss 3.191\n",
      "[ Epoch 268, Train ] | Loss:3.28329 Time:0.061035\n",
      "[ Epoch 268, Val ] | Loss:3.21430 Time:0.005043\n",
      "[ Epoch 269, Train ] | Loss:3.29270 Time:0.060104\n",
      "[ Epoch 269, Val ] | Loss:3.21430 Time:0.005085\n",
      "[ Epoch 270, Train ] | Loss:3.28882 Time:0.060133\n",
      "[ Epoch 270, Val ] | Loss:3.21199 Time:0.005204\n",
      "[ Epoch 271, Train ] | Loss:3.27245 Time:0.059935\n",
      "[ Epoch 271, Val ] | Loss:3.24903 Time:0.005114\n",
      "[ Epoch 272, Train ] | Loss:3.26967 Time:0.060189\n",
      "[ Epoch 272, Val ] | Loss:3.21692 Time:0.005234\n",
      "[ Epoch 273, Train ] | Loss:3.27274 Time:0.059486\n",
      "[ Epoch 273, Val ] | Loss:3.18858 Time:0.005036\n",
      "save model with val loss 3.189\n",
      "[ Epoch 274, Train ] | Loss:3.24302 Time:0.060722\n",
      "[ Epoch 274, Val ] | Loss:3.20195 Time:0.005028\n",
      "[ Epoch 275, Train ] | Loss:3.27787 Time:0.059766\n",
      "[ Epoch 275, Val ] | Loss:3.22991 Time:0.005067\n",
      "[ Epoch 276, Train ] | Loss:3.22039 Time:0.060063\n",
      "[ Epoch 276, Val ] | Loss:3.20639 Time:0.005029\n",
      "[ Epoch 277, Train ] | Loss:3.26720 Time:0.060174\n",
      "[ Epoch 277, Val ] | Loss:3.19370 Time:0.005095\n",
      "[ Epoch 278, Train ] | Loss:3.23540 Time:0.059593\n",
      "[ Epoch 278, Val ] | Loss:3.18696 Time:0.005105\n",
      "save model with val loss 3.187\n",
      "[ Epoch 279, Train ] | Loss:3.28090 Time:0.057712\n",
      "[ Epoch 279, Val ] | Loss:3.20848 Time:0.005181\n",
      "[ Epoch 280, Train ] | Loss:3.26927 Time:0.059607\n",
      "[ Epoch 280, Val ] | Loss:3.20338 Time:0.005083\n",
      "[ Epoch 281, Train ] | Loss:3.25894 Time:0.059822\n",
      "[ Epoch 281, Val ] | Loss:3.22166 Time:0.005087\n",
      "[ Epoch 282, Train ] | Loss:3.22701 Time:0.059933\n",
      "[ Epoch 282, Val ] | Loss:3.22611 Time:0.005020\n",
      "[ Epoch 283, Train ] | Loss:3.28190 Time:0.057114\n",
      "[ Epoch 283, Val ] | Loss:3.22689 Time:0.005087\n",
      "[ Epoch 284, Train ] | Loss:3.25640 Time:0.060009\n",
      "[ Epoch 284, Val ] | Loss:3.22224 Time:0.005107\n",
      "[ Epoch 285, Train ] | Loss:3.24327 Time:0.062382\n",
      "[ Epoch 285, Val ] | Loss:3.21651 Time:0.005031\n",
      "[ Epoch 286, Train ] | Loss:3.26978 Time:0.059742\n",
      "[ Epoch 286, Val ] | Loss:3.21193 Time:0.005093\n",
      "[ Epoch 287, Train ] | Loss:3.25438 Time:0.060109\n",
      "[ Epoch 287, Val ] | Loss:3.20986 Time:0.005097\n",
      "[ Epoch 288, Train ] | Loss:3.24252 Time:0.059746\n",
      "[ Epoch 288, Val ] | Loss:3.21091 Time:0.005059\n",
      "[ Epoch 289, Train ] | Loss:3.28117 Time:0.060041\n",
      "[ Epoch 289, Val ] | Loss:3.21331 Time:0.005200\n",
      "[ Epoch 290, Train ] | Loss:3.26316 Time:0.059646\n",
      "[ Epoch 290, Val ] | Loss:3.21516 Time:0.005101\n",
      "[ Epoch 291, Train ] | Loss:3.24906 Time:0.059530\n",
      "[ Epoch 291, Val ] | Loss:3.21694 Time:0.005043\n",
      "[ Epoch 292, Train ] | Loss:3.21948 Time:0.059415\n",
      "[ Epoch 292, Val ] | Loss:3.21928 Time:0.005100\n",
      "[ Epoch 293, Train ] | Loss:3.28186 Time:0.059326\n",
      "[ Epoch 293, Val ] | Loss:3.22154 Time:0.005109\n",
      "[ Epoch 294, Train ] | Loss:3.24727 Time:0.059623\n",
      "[ Epoch 294, Val ] | Loss:3.22328 Time:0.005048\n",
      "[ Epoch 295, Train ] | Loss:3.27729 Time:0.059611\n",
      "[ Epoch 295, Val ] | Loss:3.22156 Time:0.005090\n",
      "[ Epoch 296, Train ] | Loss:3.28092 Time:0.059350\n",
      "[ Epoch 296, Val ] | Loss:3.22126 Time:0.005107\n",
      "[ Epoch 297, Train ] | Loss:3.26615 Time:0.059470\n",
      "[ Epoch 297, Val ] | Loss:3.22785 Time:0.005033\n",
      "[ Epoch 298, Train ] | Loss:3.30965 Time:0.059484\n",
      "[ Epoch 298, Val ] | Loss:3.23101 Time:0.005101\n",
      "[ Epoch 299, Train ] | Loss:3.24955 Time:0.059110\n",
      "[ Epoch 299, Val ] | Loss:3.23382 Time:0.005099\n",
      "[ Epoch 300, Train ] | Loss:3.24607 Time:0.061984\n",
      "[ Epoch 300, Val ] | Loss:3.29861 Time:0.005025\n",
      "Finish training!\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "7daa751c-902e-486a-b45c-eaeaa0fe81ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin testing\n",
      "Test loss: 3.2180793285369873\n",
      "——————Test——————\n",
      "           class precision  recall f1-score support\n",
      "0              0    0.5556  1.0000   0.7143       5\n",
      "1              1    0.3333  0.6000   0.4286       5\n",
      "2              2    1.0000  1.0000   1.0000       5\n",
      "3              3    1.0000  0.8000   0.8889       5\n",
      "4              4    1.0000  0.2000   0.3333       5\n",
      "5              5    0.8333  1.0000   0.9091       5\n",
      "6              6    0.6667  0.4000   0.5000       5\n",
      "7              7    1.0000  0.8000   0.8889       5\n",
      "8              8    0.7500  0.6000   0.6667       5\n",
      "9              9    1.0000  0.8000   0.8889       5\n",
      "10            10    1.0000  1.0000   1.0000       5\n",
      "11            11    0.4286  0.6000   0.5000       5\n",
      "12            12    1.0000  0.6000   0.7500       5\n",
      "13            13    0.8000  0.8000   0.8000       5\n",
      "14            14    1.0000  0.0000   0.0000       5\n",
      "15            15    0.6667  0.8000   0.7273       5\n",
      "16            16    0.6250  1.0000   0.7692       5\n",
      "17            17    1.0000  0.4000   0.5714       5\n",
      "18            18    0.6000  0.6000   0.6000       5\n",
      "19            19    0.5714  0.8000   0.6667       5\n",
      "20            20    1.0000  0.8000   0.8889       5\n",
      "21            21    0.5714  0.8000   0.6667       5\n",
      "22            22    0.6000  0.6000   0.6000       5\n",
      "23            23    0.5000  0.6000   0.5455       5\n",
      "24            24    0.6250  1.0000   0.7692       5\n",
      "25            25    1.0000  1.0000   1.0000       5\n",
      "26            26    0.6667  0.8000   0.7273       5\n",
      "27            27    0.8333  1.0000   0.9091       5\n",
      "28            28    0.8000  0.8000   0.8000       5\n",
      "29            29    0.8000  0.8000   0.8000       5\n",
      "30            30    1.0000  0.6000   0.7500       5\n",
      "31            31    1.0000  1.0000   1.0000       5\n",
      "32            32    0.5000  0.6000   0.5455       5\n",
      "33            33    0.8333  1.0000   0.9091       5\n",
      "34            34    0.7500  0.6000   0.6667       5\n",
      "35            35    0.8333  1.0000   0.9091       5\n",
      "36            36    1.0000  0.4000   0.5714       5\n",
      "37            37    0.5714  0.8000   0.6667       5\n",
      "38            38    0.6250  1.0000   0.7692       5\n",
      "39            39    1.0000  1.0000   1.0000       5\n",
      "40            40    1.0000  0.8000   0.8889       5\n",
      "41            41    0.5000  0.8000   0.6154       5\n",
      "42            42    0.8000  0.8000   0.8000       5\n",
      "43            43    0.8333  1.0000   0.9091       5\n",
      "44            44    0.7500  0.6000   0.6667       5\n",
      "45            45    0.8333  1.0000   0.9091       5\n",
      "46            46    0.5000  0.2000   0.2857       5\n",
      "47            47    1.0000  0.6000   0.7500       5\n",
      "48            48    1.0000  0.8000   0.8889       5\n",
      "49            49    1.0000  0.4000   0.5714       5\n",
      "50      accuracy                     0.7400     250\n",
      "51     macro avg    0.7911  0.7400   0.7277     250\n",
      "52  weighted avg    0.7911  0.7400   0.7277     250\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "f094dc17-4c97-476b-a155-317cd05c3954",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_classifier_no_dropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_classifier_no_dropout, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, 1, 1), # input channel, ouput channel, filter size, stride, padding\n",
    "            nn.BatchNorm2d(64), # num_features\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0), # kernel size, stride, padding\n",
    "        \n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "            \n",
    "            nn.Conv2d(256, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "        \n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 50)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1) # resize, first axis is batch_num\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "48f944a2-0000-44f2-9954-5cebdb6435e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cnn_no_dropout = CNN_classifier_no_dropout()\n",
    "epochs = 300\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4\n",
    "optimizer = Adam(new_cnn_no_dropout.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_dir = './output'\n",
    "model_name = 'new_cnn_no_dropout'\n",
    "random_seed = 2022\n",
    "trainer = Trainer(model=new_cnn_no_dropout, \n",
    "                  epochs=epochs, \n",
    "                  train_dataloader=train_loader, \n",
    "                  val_dataloader=val_loader, \n",
    "                  test_dataloader=test_loader,\n",
    "                  criterion=criterion, \n",
    "                  optimizer=optimizer, \n",
    "                  lr=learning_rate, \n",
    "                  device=device, \n",
    "                  model_dir=model_dir, \n",
    "                  model_name=model_name, \n",
    "                  random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "93ed86dd-e00c-4606-89d7-9a44e3ff6664",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameters: 1761714, trainable parameters: 1761714\n",
      "\n",
      "        initial learning rate: 0.001, weight_decay: 0.0001,\n",
      "        total epochs: 300\n",
      "        \n",
      "Begin training, total epochs: 300\n",
      "[ Epoch 1, Train ] | Loss:3.81949 Time:0.085778\n",
      "[ Epoch 1, Val ] | Loss:3.92103 Time:0.005203\n",
      "save model with val loss 3.921\n",
      "[ Epoch 2, Train ] | Loss:3.23311 Time:0.083286\n",
      "[ Epoch 2, Val ] | Loss:4.32159 Time:0.005341\n",
      "[ Epoch 3, Train ] | Loss:2.32870 Time:0.083036\n",
      "[ Epoch 3, Val ] | Loss:5.73301 Time:0.005365\n",
      "[ Epoch 4, Train ] | Loss:1.46408 Time:0.082711\n",
      "[ Epoch 4, Val ] | Loss:6.51907 Time:0.005563\n",
      "[ Epoch 5, Train ] | Loss:0.83748 Time:0.082801\n",
      "[ Epoch 5, Val ] | Loss:6.99638 Time:0.005448\n",
      "[ Epoch 6, Train ] | Loss:0.47491 Time:0.082819\n",
      "[ Epoch 6, Val ] | Loss:4.30820 Time:0.005375\n",
      "[ Epoch 7, Train ] | Loss:0.28042 Time:0.082893\n",
      "[ Epoch 7, Val ] | Loss:2.45511 Time:0.005433\n",
      "save model with val loss 2.455\n",
      "[ Epoch 8, Train ] | Loss:0.16995 Time:0.083241\n",
      "[ Epoch 8, Val ] | Loss:1.36730 Time:0.005557\n",
      "save model with val loss 1.367\n",
      "[ Epoch 9, Train ] | Loss:0.11528 Time:0.084511\n",
      "[ Epoch 9, Val ] | Loss:0.98938 Time:0.005565\n",
      "save model with val loss 0.989\n",
      "[ Epoch 10, Train ] | Loss:0.06279 Time:0.085489\n",
      "[ Epoch 10, Val ] | Loss:0.55406 Time:0.004755\n",
      "save model with val loss 0.554\n",
      "[ Epoch 11, Train ] | Loss:0.04242 Time:0.083117\n",
      "[ Epoch 11, Val ] | Loss:1.09611 Time:0.005371\n",
      "[ Epoch 12, Train ] | Loss:0.03053 Time:0.083021\n",
      "[ Epoch 12, Val ] | Loss:0.40746 Time:0.005591\n",
      "save model with val loss 0.407\n",
      "[ Epoch 13, Train ] | Loss:0.02036 Time:0.084808\n",
      "[ Epoch 13, Val ] | Loss:0.50457 Time:0.005507\n",
      "[ Epoch 14, Train ] | Loss:0.00983 Time:0.082842\n",
      "[ Epoch 14, Val ] | Loss:0.63641 Time:0.005439\n",
      "[ Epoch 15, Train ] | Loss:0.00584 Time:0.084347\n",
      "[ Epoch 15, Val ] | Loss:0.48630 Time:0.005392\n",
      "[ Epoch 16, Train ] | Loss:0.00401 Time:0.082563\n",
      "[ Epoch 16, Val ] | Loss:0.35918 Time:0.005444\n",
      "save model with val loss 0.359\n",
      "[ Epoch 17, Train ] | Loss:0.00287 Time:0.084369\n",
      "[ Epoch 17, Val ] | Loss:0.33112 Time:0.005521\n",
      "save model with val loss 0.331\n",
      "[ Epoch 18, Train ] | Loss:0.00231 Time:0.082590\n",
      "[ Epoch 18, Val ] | Loss:0.35136 Time:0.005377\n",
      "[ Epoch 19, Train ] | Loss:0.00173 Time:0.082499\n",
      "[ Epoch 19, Val ] | Loss:0.37928 Time:0.005434\n",
      "[ Epoch 20, Train ] | Loss:0.00193 Time:0.084767\n",
      "[ Epoch 20, Val ] | Loss:0.38899 Time:0.004996\n",
      "[ Epoch 21, Train ] | Loss:0.00148 Time:0.082713\n",
      "[ Epoch 21, Val ] | Loss:0.34236 Time:0.005451\n",
      "[ Epoch 22, Train ] | Loss:0.00151 Time:0.082363\n",
      "[ Epoch 22, Val ] | Loss:0.34450 Time:0.005444\n",
      "[ Epoch 23, Train ] | Loss:0.00119 Time:0.082385\n",
      "[ Epoch 23, Val ] | Loss:0.35053 Time:0.005391\n",
      "[ Epoch 24, Train ] | Loss:0.00108 Time:0.082734\n",
      "[ Epoch 24, Val ] | Loss:0.34372 Time:0.005457\n",
      "[ Epoch 25, Train ] | Loss:0.00106 Time:0.082700\n",
      "[ Epoch 25, Val ] | Loss:0.33795 Time:0.005442\n",
      "[ Epoch 26, Train ] | Loss:0.00110 Time:0.082435\n",
      "[ Epoch 26, Val ] | Loss:0.32786 Time:0.005403\n",
      "save model with val loss 0.328\n",
      "[ Epoch 27, Train ] | Loss:0.00106 Time:0.084020\n",
      "[ Epoch 27, Val ] | Loss:0.32902 Time:0.005382\n",
      "[ Epoch 28, Train ] | Loss:0.00101 Time:0.082585\n",
      "[ Epoch 28, Val ] | Loss:0.34262 Time:0.005450\n",
      "[ Epoch 29, Train ] | Loss:0.00087 Time:0.084451\n",
      "[ Epoch 29, Val ] | Loss:0.34630 Time:0.005542\n",
      "[ Epoch 30, Train ] | Loss:0.00086 Time:0.085842\n",
      "[ Epoch 30, Val ] | Loss:0.32528 Time:0.005421\n",
      "save model with val loss 0.325\n",
      "[ Epoch 31, Train ] | Loss:0.00080 Time:0.084146\n",
      "[ Epoch 31, Val ] | Loss:0.31777 Time:0.005524\n",
      "save model with val loss 0.318\n",
      "[ Epoch 32, Train ] | Loss:0.00078 Time:0.085848\n",
      "[ Epoch 32, Val ] | Loss:0.31522 Time:0.005353\n",
      "save model with val loss 0.315\n",
      "[ Epoch 33, Train ] | Loss:0.00067 Time:0.084985\n",
      "[ Epoch 33, Val ] | Loss:0.31446 Time:0.005278\n",
      "save model with val loss 0.314\n",
      "[ Epoch 34, Train ] | Loss:0.00067 Time:0.085062\n",
      "[ Epoch 34, Val ] | Loss:0.32197 Time:0.005506\n",
      "[ Epoch 35, Train ] | Loss:0.00064 Time:0.085988\n",
      "[ Epoch 35, Val ] | Loss:0.32515 Time:0.005535\n",
      "[ Epoch 36, Train ] | Loss:0.00066 Time:0.084379\n",
      "[ Epoch 36, Val ] | Loss:0.32305 Time:0.005438\n",
      "[ Epoch 37, Train ] | Loss:0.00061 Time:0.082085\n",
      "[ Epoch 37, Val ] | Loss:0.32303 Time:0.005455\n",
      "[ Epoch 38, Train ] | Loss:0.00053 Time:0.082708\n",
      "[ Epoch 38, Val ] | Loss:0.31069 Time:0.005399\n",
      "save model with val loss 0.311\n",
      "[ Epoch 39, Train ] | Loss:0.00059 Time:0.083670\n",
      "[ Epoch 39, Val ] | Loss:0.30757 Time:0.005522\n",
      "save model with val loss 0.308\n",
      "[ Epoch 40, Train ] | Loss:0.00052 Time:0.084002\n",
      "[ Epoch 40, Val ] | Loss:0.31228 Time:0.005311\n",
      "[ Epoch 41, Train ] | Loss:0.00050 Time:0.084553\n",
      "[ Epoch 41, Val ] | Loss:0.33033 Time:0.007651\n",
      "[ Epoch 42, Train ] | Loss:0.00051 Time:0.084670\n",
      "[ Epoch 42, Val ] | Loss:0.31921 Time:0.005448\n",
      "[ Epoch 43, Train ] | Loss:0.00047 Time:0.084507\n",
      "[ Epoch 43, Val ] | Loss:0.33082 Time:0.005362\n",
      "[ Epoch 44, Train ] | Loss:0.00044 Time:0.084499\n",
      "[ Epoch 44, Val ] | Loss:0.32198 Time:0.005565\n",
      "[ Epoch 45, Train ] | Loss:0.00079 Time:0.084532\n",
      "[ Epoch 45, Val ] | Loss:0.31800 Time:0.005452\n",
      "[ Epoch 46, Train ] | Loss:0.00044 Time:0.084130\n",
      "[ Epoch 46, Val ] | Loss:0.37250 Time:0.005382\n",
      "[ Epoch 47, Train ] | Loss:0.00052 Time:0.084423\n",
      "[ Epoch 47, Val ] | Loss:0.34957 Time:0.005449\n",
      "[ Epoch 48, Train ] | Loss:0.00042 Time:0.084453\n",
      "[ Epoch 48, Val ] | Loss:0.29777 Time:0.005434\n",
      "save model with val loss 0.298\n",
      "[ Epoch 49, Train ] | Loss:0.00040 Time:0.083740\n",
      "[ Epoch 49, Val ] | Loss:0.29180 Time:0.005330\n",
      "save model with val loss 0.292\n",
      "[ Epoch 50, Train ] | Loss:0.00047 Time:0.084353\n",
      "[ Epoch 50, Val ] | Loss:0.29902 Time:0.005373\n",
      "[ Epoch 51, Train ] | Loss:0.00035 Time:0.084043\n",
      "[ Epoch 51, Val ] | Loss:0.30886 Time:0.004588\n",
      "[ Epoch 52, Train ] | Loss:0.00038 Time:0.082094\n",
      "[ Epoch 52, Val ] | Loss:0.30902 Time:0.005557\n",
      "[ Epoch 53, Train ] | Loss:0.00034 Time:0.084368\n",
      "[ Epoch 53, Val ] | Loss:0.31596 Time:0.005436\n",
      "[ Epoch 54, Train ] | Loss:0.00034 Time:0.085097\n",
      "[ Epoch 54, Val ] | Loss:0.30760 Time:0.005380\n",
      "[ Epoch 55, Train ] | Loss:0.00032 Time:0.084522\n",
      "[ Epoch 55, Val ] | Loss:0.30087 Time:0.005574\n",
      "[ Epoch 56, Train ] | Loss:0.00034 Time:0.084341\n",
      "[ Epoch 56, Val ] | Loss:0.29950 Time:0.005445\n",
      "[ Epoch 57, Train ] | Loss:0.00029 Time:0.084081\n",
      "[ Epoch 57, Val ] | Loss:0.32220 Time:0.005385\n",
      "[ Epoch 58, Train ] | Loss:0.00032 Time:0.084382\n",
      "[ Epoch 58, Val ] | Loss:0.32212 Time:0.005434\n",
      "[ Epoch 59, Train ] | Loss:0.00031 Time:0.084309\n",
      "[ Epoch 59, Val ] | Loss:0.30668 Time:0.005446\n",
      "[ Epoch 60, Train ] | Loss:0.00028 Time:0.083985\n",
      "[ Epoch 60, Val ] | Loss:0.30225 Time:0.005389\n",
      "[ Epoch 61, Train ] | Loss:0.00027 Time:0.084394\n",
      "[ Epoch 61, Val ] | Loss:0.30064 Time:0.005459\n",
      "[ Epoch 62, Train ] | Loss:0.00028 Time:0.084087\n",
      "[ Epoch 62, Val ] | Loss:0.28726 Time:0.005449\n",
      "save model with val loss 0.287\n",
      "[ Epoch 63, Train ] | Loss:0.00025 Time:0.082502\n",
      "[ Epoch 63, Val ] | Loss:0.29019 Time:0.005328\n",
      "[ Epoch 64, Train ] | Loss:0.00025 Time:0.084059\n",
      "[ Epoch 64, Val ] | Loss:0.30020 Time:0.005439\n",
      "[ Epoch 65, Train ] | Loss:0.00025 Time:0.084182\n",
      "[ Epoch 65, Val ] | Loss:0.30660 Time:0.005451\n",
      "[ Epoch 66, Train ] | Loss:0.00023 Time:0.084415\n",
      "[ Epoch 66, Val ] | Loss:0.31110 Time:0.005569\n",
      "[ Epoch 67, Train ] | Loss:0.00024 Time:0.083980\n",
      "[ Epoch 67, Val ] | Loss:0.31145 Time:0.005447\n",
      "[ Epoch 68, Train ] | Loss:0.00025 Time:0.083901\n",
      "[ Epoch 68, Val ] | Loss:0.30278 Time:0.005452\n",
      "[ Epoch 69, Train ] | Loss:0.00022 Time:0.084267\n",
      "[ Epoch 69, Val ] | Loss:0.31130 Time:0.005563\n",
      "[ Epoch 70, Train ] | Loss:0.00021 Time:0.082491\n",
      "[ Epoch 70, Val ] | Loss:0.30371 Time:0.005439\n",
      "[ Epoch 71, Train ] | Loss:0.00021 Time:0.084131\n",
      "[ Epoch 71, Val ] | Loss:0.29630 Time:0.005447\n",
      "[ Epoch 72, Train ] | Loss:0.00023 Time:0.083684\n",
      "[ Epoch 72, Val ] | Loss:0.30258 Time:0.005380\n",
      "[ Epoch 73, Train ] | Loss:0.00024 Time:0.083486\n",
      "[ Epoch 73, Val ] | Loss:0.30699 Time:0.005436\n",
      "[ Epoch 74, Train ] | Loss:0.00026 Time:0.083647\n",
      "[ Epoch 74, Val ] | Loss:0.30901 Time:0.005448\n",
      "[ Epoch 75, Train ] | Loss:0.00023 Time:0.083850\n",
      "[ Epoch 75, Val ] | Loss:0.30703 Time:0.005382\n",
      "[ Epoch 76, Train ] | Loss:0.00021 Time:0.083832\n",
      "[ Epoch 76, Val ] | Loss:0.31774 Time:0.005565\n",
      "[ Epoch 77, Train ] | Loss:0.00020 Time:0.083957\n",
      "[ Epoch 77, Val ] | Loss:0.31132 Time:0.005454\n",
      "[ Epoch 78, Train ] | Loss:0.00021 Time:0.083975\n",
      "[ Epoch 78, Val ] | Loss:0.30556 Time:0.005554\n",
      "[ Epoch 79, Train ] | Loss:0.00020 Time:0.086203\n",
      "[ Epoch 79, Val ] | Loss:0.30919 Time:0.005394\n",
      "[ Epoch 80, Train ] | Loss:0.00020 Time:0.084005\n",
      "[ Epoch 80, Val ] | Loss:0.30709 Time:0.005556\n",
      "[ Epoch 81, Train ] | Loss:0.00020 Time:0.083769\n",
      "[ Epoch 81, Val ] | Loss:0.29771 Time:0.005542\n",
      "[ Epoch 82, Train ] | Loss:0.00020 Time:0.083447\n",
      "[ Epoch 82, Val ] | Loss:0.30101 Time:0.005572\n",
      "[ Epoch 83, Train ] | Loss:0.00019 Time:0.082063\n",
      "[ Epoch 83, Val ] | Loss:0.30071 Time:0.005564\n",
      "[ Epoch 84, Train ] | Loss:0.00018 Time:0.083562\n",
      "[ Epoch 84, Val ] | Loss:0.29527 Time:0.005388\n",
      "[ Epoch 85, Train ] | Loss:0.00018 Time:0.084140\n",
      "[ Epoch 85, Val ] | Loss:0.29384 Time:0.005456\n",
      "[ Epoch 86, Train ] | Loss:0.00021 Time:0.083477\n",
      "[ Epoch 86, Val ] | Loss:0.29671 Time:0.005567\n",
      "[ Epoch 87, Train ] | Loss:0.00021 Time:0.084018\n",
      "[ Epoch 87, Val ] | Loss:0.29513 Time:0.005373\n",
      "[ Epoch 88, Train ] | Loss:0.00018 Time:0.083799\n",
      "[ Epoch 88, Val ] | Loss:0.29777 Time:0.005444\n",
      "[ Epoch 89, Train ] | Loss:0.00020 Time:0.083825\n",
      "[ Epoch 89, Val ] | Loss:0.29357 Time:0.005439\n",
      "[ Epoch 90, Train ] | Loss:0.00020 Time:0.084033\n",
      "[ Epoch 90, Val ] | Loss:0.29585 Time:0.005373\n",
      "[ Epoch 91, Train ] | Loss:0.00019 Time:0.083800\n",
      "[ Epoch 91, Val ] | Loss:0.29339 Time:0.005447\n",
      "[ Epoch 92, Train ] | Loss:0.00019 Time:0.084028\n",
      "[ Epoch 92, Val ] | Loss:0.29685 Time:0.005436\n",
      "[ Epoch 93, Train ] | Loss:0.00029 Time:0.083864\n",
      "[ Epoch 93, Val ] | Loss:0.29410 Time:0.005382\n",
      "[ Epoch 94, Train ] | Loss:0.00024 Time:0.083538\n",
      "[ Epoch 94, Val ] | Loss:0.30145 Time:0.005420\n",
      "[ Epoch 95, Train ] | Loss:0.00021 Time:0.083490\n",
      "[ Epoch 95, Val ] | Loss:0.31773 Time:0.005461\n",
      "[ Epoch 96, Train ] | Loss:0.00021 Time:0.083997\n",
      "[ Epoch 96, Val ] | Loss:0.33229 Time:0.005395\n",
      "[ Epoch 97, Train ] | Loss:0.00020 Time:0.083776\n",
      "[ Epoch 97, Val ] | Loss:0.32109 Time:0.005438\n",
      "[ Epoch 98, Train ] | Loss:0.00020 Time:0.085999\n",
      "[ Epoch 98, Val ] | Loss:0.29599 Time:0.005425\n",
      "[ Epoch 99, Train ] | Loss:0.00017 Time:0.084754\n",
      "[ Epoch 99, Val ] | Loss:0.28964 Time:0.007466\n",
      "[ Epoch 100, Train ] | Loss:0.00019 Time:0.085227\n",
      "[ Epoch 100, Val ] | Loss:0.29022 Time:0.005419\n",
      "[ Epoch 101, Train ] | Loss:0.00017 Time:0.085732\n",
      "[ Epoch 101, Val ] | Loss:0.29887 Time:0.005407\n",
      "[ Epoch 102, Train ] | Loss:0.00017 Time:0.084825\n",
      "[ Epoch 102, Val ] | Loss:0.29067 Time:0.005356\n",
      "[ Epoch 103, Train ] | Loss:0.00016 Time:0.085429\n",
      "[ Epoch 103, Val ] | Loss:0.28757 Time:0.005431\n",
      "[ Epoch 104, Train ] | Loss:0.00019 Time:0.082884\n",
      "[ Epoch 104, Val ] | Loss:0.29204 Time:0.005559\n",
      "[ Epoch 105, Train ] | Loss:0.00019 Time:0.086057\n",
      "[ Epoch 105, Val ] | Loss:0.28801 Time:0.005341\n",
      "[ Epoch 106, Train ] | Loss:0.00021 Time:0.085329\n",
      "[ Epoch 106, Val ] | Loss:0.29855 Time:0.005419\n",
      "[ Epoch 107, Train ] | Loss:0.00017 Time:0.085030\n",
      "[ Epoch 107, Val ] | Loss:0.30796 Time:0.005548\n",
      "[ Epoch 108, Train ] | Loss:0.00017 Time:0.084618\n",
      "[ Epoch 108, Val ] | Loss:0.29593 Time:0.005354\n",
      "[ Epoch 109, Train ] | Loss:0.00018 Time:0.084438\n",
      "[ Epoch 109, Val ] | Loss:0.28353 Time:0.005445\n",
      "save model with val loss 0.284\n",
      "[ Epoch 110, Train ] | Loss:0.00017 Time:0.084967\n",
      "[ Epoch 110, Val ] | Loss:0.27406 Time:0.005286\n",
      "save model with val loss 0.274\n",
      "[ Epoch 111, Train ] | Loss:0.00018 Time:0.083405\n",
      "[ Epoch 111, Val ] | Loss:0.27092 Time:0.005366\n",
      "save model with val loss 0.271\n",
      "[ Epoch 112, Train ] | Loss:0.00016 Time:0.083351\n",
      "[ Epoch 112, Val ] | Loss:0.30243 Time:0.005310\n",
      "[ Epoch 113, Train ] | Loss:0.00018 Time:0.083022\n",
      "[ Epoch 113, Val ] | Loss:0.29030 Time:0.005432\n",
      "[ Epoch 114, Train ] | Loss:0.00016 Time:0.083620\n",
      "[ Epoch 114, Val ] | Loss:0.31018 Time:0.005453\n",
      "[ Epoch 115, Train ] | Loss:0.00019 Time:0.082777\n",
      "[ Epoch 115, Val ] | Loss:0.30464 Time:0.005385\n",
      "[ Epoch 116, Train ] | Loss:0.00016 Time:0.083628\n",
      "[ Epoch 116, Val ] | Loss:0.29205 Time:0.005455\n",
      "[ Epoch 117, Train ] | Loss:0.00016 Time:0.083454\n",
      "[ Epoch 117, Val ] | Loss:0.27629 Time:0.005442\n",
      "[ Epoch 118, Train ] | Loss:0.00019 Time:0.083241\n",
      "[ Epoch 118, Val ] | Loss:0.27756 Time:0.005385\n",
      "[ Epoch 119, Train ] | Loss:0.00036 Time:0.083371\n",
      "[ Epoch 119, Val ] | Loss:0.31237 Time:0.005449\n",
      "[ Epoch 120, Train ] | Loss:0.00036 Time:0.082946\n",
      "[ Epoch 120, Val ] | Loss:0.34354 Time:0.005459\n",
      "[ Epoch 121, Train ] | Loss:0.00020 Time:0.083421\n",
      "[ Epoch 121, Val ] | Loss:0.25301 Time:0.005391\n",
      "save model with val loss 0.253\n",
      "[ Epoch 122, Train ] | Loss:0.00029 Time:0.084667\n",
      "[ Epoch 122, Val ] | Loss:0.28686 Time:0.005368\n",
      "[ Epoch 123, Train ] | Loss:0.00022 Time:0.085612\n",
      "[ Epoch 123, Val ] | Loss:0.31289 Time:0.005363\n",
      "[ Epoch 124, Train ] | Loss:0.00022 Time:0.083533\n",
      "[ Epoch 124, Val ] | Loss:0.32523 Time:0.005447\n",
      "[ Epoch 125, Train ] | Loss:0.00017 Time:0.083327\n",
      "[ Epoch 125, Val ] | Loss:0.31961 Time:0.005440\n",
      "[ Epoch 126, Train ] | Loss:0.00015 Time:0.083259\n",
      "[ Epoch 126, Val ] | Loss:0.29784 Time:0.005377\n",
      "[ Epoch 127, Train ] | Loss:0.00018 Time:0.083491\n",
      "[ Epoch 127, Val ] | Loss:0.29764 Time:0.005451\n",
      "[ Epoch 128, Train ] | Loss:0.00016 Time:0.083074\n",
      "[ Epoch 128, Val ] | Loss:0.30571 Time:0.005442\n",
      "[ Epoch 129, Train ] | Loss:0.00018 Time:0.083292\n",
      "[ Epoch 129, Val ] | Loss:0.30478 Time:0.005390\n",
      "[ Epoch 130, Train ] | Loss:0.00015 Time:0.083055\n",
      "[ Epoch 130, Val ] | Loss:0.28135 Time:0.005455\n",
      "[ Epoch 131, Train ] | Loss:0.00014 Time:0.083033\n",
      "[ Epoch 131, Val ] | Loss:0.27471 Time:0.005432\n",
      "[ Epoch 132, Train ] | Loss:0.00014 Time:0.083479\n",
      "[ Epoch 132, Val ] | Loss:0.27359 Time:0.005378\n",
      "[ Epoch 133, Train ] | Loss:0.00015 Time:0.083257\n",
      "[ Epoch 133, Val ] | Loss:0.28170 Time:0.005445\n",
      "[ Epoch 134, Train ] | Loss:0.00013 Time:0.082979\n",
      "[ Epoch 134, Val ] | Loss:0.27741 Time:0.005437\n",
      "[ Epoch 135, Train ] | Loss:0.00014 Time:0.083189\n",
      "[ Epoch 135, Val ] | Loss:0.28113 Time:0.005400\n",
      "[ Epoch 136, Train ] | Loss:0.00015 Time:0.083171\n",
      "[ Epoch 136, Val ] | Loss:0.28164 Time:0.005574\n",
      "[ Epoch 137, Train ] | Loss:0.00026 Time:0.083361\n",
      "[ Epoch 137, Val ] | Loss:0.28792 Time:0.005565\n",
      "[ Epoch 138, Train ] | Loss:0.00020 Time:0.083266\n",
      "[ Epoch 138, Val ] | Loss:0.35524 Time:0.005384\n",
      "[ Epoch 139, Train ] | Loss:0.00019 Time:0.082925\n",
      "[ Epoch 139, Val ] | Loss:0.26832 Time:0.007547\n",
      "[ Epoch 140, Train ] | Loss:0.00017 Time:0.083216\n",
      "[ Epoch 140, Val ] | Loss:0.27900 Time:0.005452\n",
      "[ Epoch 141, Train ] | Loss:0.00015 Time:0.082687\n",
      "[ Epoch 141, Val ] | Loss:0.28405 Time:0.005588\n",
      "[ Epoch 142, Train ] | Loss:0.00016 Time:0.083839\n",
      "[ Epoch 142, Val ] | Loss:0.29511 Time:0.005555\n",
      "[ Epoch 143, Train ] | Loss:0.00014 Time:0.082795\n",
      "[ Epoch 143, Val ] | Loss:0.28124 Time:0.005454\n",
      "[ Epoch 144, Train ] | Loss:0.00016 Time:0.083046\n",
      "[ Epoch 144, Val ] | Loss:0.27456 Time:0.005559\n",
      "[ Epoch 145, Train ] | Loss:0.00015 Time:0.083659\n",
      "[ Epoch 145, Val ] | Loss:0.27124 Time:0.005559\n",
      "[ Epoch 146, Train ] | Loss:0.00016 Time:0.082634\n",
      "[ Epoch 146, Val ] | Loss:0.28392 Time:0.005446\n",
      "[ Epoch 147, Train ] | Loss:0.00014 Time:0.083272\n",
      "[ Epoch 147, Val ] | Loss:0.28285 Time:0.005380\n",
      "[ Epoch 148, Train ] | Loss:0.00017 Time:0.082970\n",
      "[ Epoch 148, Val ] | Loss:0.28368 Time:0.005454\n",
      "[ Epoch 149, Train ] | Loss:0.00014 Time:0.082576\n",
      "[ Epoch 149, Val ] | Loss:0.29345 Time:0.005458\n",
      "[ Epoch 150, Train ] | Loss:0.00017 Time:0.082548\n",
      "[ Epoch 150, Val ] | Loss:0.28164 Time:0.005569\n",
      "[ Epoch 151, Train ] | Loss:0.00015 Time:0.082300\n",
      "[ Epoch 151, Val ] | Loss:0.25228 Time:0.005426\n",
      "save model with val loss 0.252\n",
      "[ Epoch 152, Train ] | Loss:0.00014 Time:0.083573\n",
      "[ Epoch 152, Val ] | Loss:0.27187 Time:0.005497\n",
      "[ Epoch 153, Train ] | Loss:0.00014 Time:0.083052\n",
      "[ Epoch 153, Val ] | Loss:0.27665 Time:0.005421\n",
      "[ Epoch 154, Train ] | Loss:0.00014 Time:0.082652\n",
      "[ Epoch 154, Val ] | Loss:0.29032 Time:0.005447\n",
      "[ Epoch 155, Train ] | Loss:0.00014 Time:0.082558\n",
      "[ Epoch 155, Val ] | Loss:0.27760 Time:0.005395\n",
      "[ Epoch 156, Train ] | Loss:0.00015 Time:0.082487\n",
      "[ Epoch 156, Val ] | Loss:0.25469 Time:0.005447\n",
      "[ Epoch 157, Train ] | Loss:0.00016 Time:0.083287\n",
      "[ Epoch 157, Val ] | Loss:0.26610 Time:0.005569\n",
      "[ Epoch 158, Train ] | Loss:0.00016 Time:0.082802\n",
      "[ Epoch 158, Val ] | Loss:0.28489 Time:0.005550\n",
      "[ Epoch 159, Train ] | Loss:0.00014 Time:0.082514\n",
      "[ Epoch 159, Val ] | Loss:0.28290 Time:0.005564\n",
      "[ Epoch 160, Train ] | Loss:0.00016 Time:0.082573\n",
      "[ Epoch 160, Val ] | Loss:0.26635 Time:0.005461\n",
      "[ Epoch 161, Train ] | Loss:0.00015 Time:0.082913\n",
      "[ Epoch 161, Val ] | Loss:0.27702 Time:0.005393\n",
      "[ Epoch 162, Train ] | Loss:0.00014 Time:0.082837\n",
      "[ Epoch 162, Val ] | Loss:0.27404 Time:0.005442\n",
      "[ Epoch 163, Train ] | Loss:0.00014 Time:0.083075\n",
      "[ Epoch 163, Val ] | Loss:0.26703 Time:0.005458\n",
      "[ Epoch 164, Train ] | Loss:0.00014 Time:0.082663\n",
      "[ Epoch 164, Val ] | Loss:0.25771 Time:0.005449\n",
      "[ Epoch 165, Train ] | Loss:0.00014 Time:0.085426\n",
      "[ Epoch 165, Val ] | Loss:0.25792 Time:0.005416\n",
      "[ Epoch 166, Train ] | Loss:0.00013 Time:0.086136\n",
      "[ Epoch 166, Val ] | Loss:0.26830 Time:0.005335\n",
      "[ Epoch 167, Train ] | Loss:0.00014 Time:0.086587\n",
      "[ Epoch 167, Val ] | Loss:0.27662 Time:0.005427\n",
      "[ Epoch 168, Train ] | Loss:0.00015 Time:0.086562\n",
      "[ Epoch 168, Val ] | Loss:0.26772 Time:0.005406\n",
      "[ Epoch 169, Train ] | Loss:0.00016 Time:0.086613\n",
      "[ Epoch 169, Val ] | Loss:0.25443 Time:0.005351\n",
      "[ Epoch 170, Train ] | Loss:0.00014 Time:0.085977\n",
      "[ Epoch 170, Val ] | Loss:0.25207 Time:0.005423\n",
      "save model with val loss 0.252\n",
      "[ Epoch 171, Train ] | Loss:0.00015 Time:0.085601\n",
      "[ Epoch 171, Val ] | Loss:0.26454 Time:0.005311\n",
      "[ Epoch 172, Train ] | Loss:0.00019 Time:0.082728\n",
      "[ Epoch 172, Val ] | Loss:0.27927 Time:0.005443\n",
      "[ Epoch 173, Train ] | Loss:0.00017 Time:0.082578\n",
      "[ Epoch 173, Val ] | Loss:0.26831 Time:0.005460\n",
      "[ Epoch 174, Train ] | Loss:0.00016 Time:0.082372\n",
      "[ Epoch 174, Val ] | Loss:0.25731 Time:0.005381\n",
      "[ Epoch 175, Train ] | Loss:0.00015 Time:0.082719\n",
      "[ Epoch 175, Val ] | Loss:0.27027 Time:0.005456\n",
      "[ Epoch 176, Train ] | Loss:0.00015 Time:0.082131\n",
      "[ Epoch 176, Val ] | Loss:0.28064 Time:0.005438\n",
      "[ Epoch 177, Train ] | Loss:0.00015 Time:0.082625\n",
      "[ Epoch 177, Val ] | Loss:0.27982 Time:0.005392\n",
      "[ Epoch 178, Train ] | Loss:0.00016 Time:0.082613\n",
      "[ Epoch 178, Val ] | Loss:0.26581 Time:0.005487\n",
      "[ Epoch 179, Train ] | Loss:0.00015 Time:0.082227\n",
      "[ Epoch 179, Val ] | Loss:0.27723 Time:0.005454\n",
      "[ Epoch 180, Train ] | Loss:0.00014 Time:0.082539\n",
      "[ Epoch 180, Val ] | Loss:0.27394 Time:0.005376\n",
      "[ Epoch 181, Train ] | Loss:0.00014 Time:0.082365\n",
      "[ Epoch 181, Val ] | Loss:0.26280 Time:0.005456\n",
      "[ Epoch 182, Train ] | Loss:0.00014 Time:0.084636\n",
      "[ Epoch 182, Val ] | Loss:0.25574 Time:0.005450\n",
      "[ Epoch 183, Train ] | Loss:0.00018 Time:0.082290\n",
      "[ Epoch 183, Val ] | Loss:0.25383 Time:0.005373\n",
      "[ Epoch 184, Train ] | Loss:0.00014 Time:0.084679\n",
      "[ Epoch 184, Val ] | Loss:0.26877 Time:0.005450\n",
      "[ Epoch 185, Train ] | Loss:0.00016 Time:0.084748\n",
      "[ Epoch 185, Val ] | Loss:0.26554 Time:0.005456\n",
      "[ Epoch 186, Train ] | Loss:0.00014 Time:0.082448\n",
      "[ Epoch 186, Val ] | Loss:0.27701 Time:0.005381\n",
      "[ Epoch 187, Train ] | Loss:0.00014 Time:0.082323\n",
      "[ Epoch 187, Val ] | Loss:0.25977 Time:0.005466\n",
      "[ Epoch 188, Train ] | Loss:0.00015 Time:0.084550\n",
      "[ Epoch 188, Val ] | Loss:0.25407 Time:0.005457\n",
      "[ Epoch 189, Train ] | Loss:0.00014 Time:0.084522\n",
      "[ Epoch 189, Val ] | Loss:0.26410 Time:0.005364\n",
      "[ Epoch 190, Train ] | Loss:0.00014 Time:0.084801\n",
      "[ Epoch 190, Val ] | Loss:0.25580 Time:0.005447\n",
      "[ Epoch 191, Train ] | Loss:0.00015 Time:0.084110\n",
      "[ Epoch 191, Val ] | Loss:0.27680 Time:0.005453\n",
      "[ Epoch 192, Train ] | Loss:0.00013 Time:0.084196\n",
      "[ Epoch 192, Val ] | Loss:0.24867 Time:0.005393\n",
      "save model with val loss 0.249\n",
      "[ Epoch 193, Train ] | Loss:0.00018 Time:0.083262\n",
      "[ Epoch 193, Val ] | Loss:0.25283 Time:0.005361\n",
      "[ Epoch 194, Train ] | Loss:0.00014 Time:0.084418\n",
      "[ Epoch 194, Val ] | Loss:0.33188 Time:0.005373\n",
      "[ Epoch 195, Train ] | Loss:0.00014 Time:0.084067\n",
      "[ Epoch 195, Val ] | Loss:0.25732 Time:0.005440\n",
      "[ Epoch 196, Train ] | Loss:0.00013 Time:0.084328\n",
      "[ Epoch 196, Val ] | Loss:0.22199 Time:0.005449\n",
      "save model with val loss 0.222\n",
      "[ Epoch 197, Train ] | Loss:0.00015 Time:0.083234\n",
      "[ Epoch 197, Val ] | Loss:0.24230 Time:0.005336\n",
      "[ Epoch 198, Train ] | Loss:0.00015 Time:0.083643\n",
      "[ Epoch 198, Val ] | Loss:0.25641 Time:0.005440\n",
      "[ Epoch 199, Train ] | Loss:0.00014 Time:0.086840\n",
      "[ Epoch 199, Val ] | Loss:0.23794 Time:0.005435\n",
      "[ Epoch 200, Train ] | Loss:0.00014 Time:0.086617\n",
      "[ Epoch 200, Val ] | Loss:0.25183 Time:0.005387\n",
      "[ Epoch 201, Train ] | Loss:0.00014 Time:0.084305\n",
      "[ Epoch 201, Val ] | Loss:0.24058 Time:0.005462\n",
      "[ Epoch 202, Train ] | Loss:0.00016 Time:0.084598\n",
      "[ Epoch 202, Val ] | Loss:0.23739 Time:0.005446\n",
      "[ Epoch 203, Train ] | Loss:0.00015 Time:0.083605\n",
      "[ Epoch 203, Val ] | Loss:0.23302 Time:0.005390\n",
      "[ Epoch 204, Train ] | Loss:0.00014 Time:0.084315\n",
      "[ Epoch 204, Val ] | Loss:0.25438 Time:0.005444\n",
      "[ Epoch 205, Train ] | Loss:0.00013 Time:0.084298\n",
      "[ Epoch 205, Val ] | Loss:0.25868 Time:0.005457\n",
      "[ Epoch 206, Train ] | Loss:0.00013 Time:0.084313\n",
      "[ Epoch 206, Val ] | Loss:0.25011 Time:0.005558\n",
      "[ Epoch 207, Train ] | Loss:0.00015 Time:0.084269\n",
      "[ Epoch 207, Val ] | Loss:0.25712 Time:0.005449\n",
      "[ Epoch 208, Train ] | Loss:0.00014 Time:0.084214\n",
      "[ Epoch 208, Val ] | Loss:0.23360 Time:0.005443\n",
      "[ Epoch 209, Train ] | Loss:0.00013 Time:0.083799\n",
      "[ Epoch 209, Val ] | Loss:0.24941 Time:0.005401\n",
      "[ Epoch 210, Train ] | Loss:0.00014 Time:0.084263\n",
      "[ Epoch 210, Val ] | Loss:0.25043 Time:0.005453\n",
      "[ Epoch 211, Train ] | Loss:0.00015 Time:0.083902\n",
      "[ Epoch 211, Val ] | Loss:0.25736 Time:0.005447\n",
      "[ Epoch 212, Train ] | Loss:0.00014 Time:0.083761\n",
      "[ Epoch 212, Val ] | Loss:0.25624 Time:0.005390\n",
      "[ Epoch 213, Train ] | Loss:0.00014 Time:0.083998\n",
      "[ Epoch 213, Val ] | Loss:0.24614 Time:0.005459\n",
      "[ Epoch 214, Train ] | Loss:0.00013 Time:0.083671\n",
      "[ Epoch 214, Val ] | Loss:0.23820 Time:0.005451\n",
      "[ Epoch 215, Train ] | Loss:0.00013 Time:0.083915\n",
      "[ Epoch 215, Val ] | Loss:0.25281 Time:0.005375\n",
      "[ Epoch 216, Train ] | Loss:0.00017 Time:0.083145\n",
      "[ Epoch 216, Val ] | Loss:0.26983 Time:0.005576\n",
      "[ Epoch 217, Train ] | Loss:0.00025 Time:0.084374\n",
      "[ Epoch 217, Val ] | Loss:0.27397 Time:0.005452\n",
      "[ Epoch 218, Train ] | Loss:0.00022 Time:0.084343\n",
      "[ Epoch 218, Val ] | Loss:0.26797 Time:0.005387\n",
      "[ Epoch 219, Train ] | Loss:0.00025 Time:0.088435\n",
      "[ Epoch 219, Val ] | Loss:0.41949 Time:0.005440\n",
      "[ Epoch 220, Train ] | Loss:0.00017 Time:0.083926\n",
      "[ Epoch 220, Val ] | Loss:0.32974 Time:0.005453\n",
      "[ Epoch 221, Train ] | Loss:0.00016 Time:0.084088\n",
      "[ Epoch 221, Val ] | Loss:0.24448 Time:0.005581\n",
      "[ Epoch 222, Train ] | Loss:0.00014 Time:0.082674\n",
      "[ Epoch 222, Val ] | Loss:0.28562 Time:0.005558\n",
      "[ Epoch 223, Train ] | Loss:0.00015 Time:0.084083\n",
      "[ Epoch 223, Val ] | Loss:0.24025 Time:0.005560\n",
      "[ Epoch 224, Train ] | Loss:0.00015 Time:0.083462\n",
      "[ Epoch 224, Val ] | Loss:0.20385 Time:0.005389\n",
      "save model with val loss 0.204\n",
      "[ Epoch 225, Train ] | Loss:0.00013 Time:0.084700\n",
      "[ Epoch 225, Val ] | Loss:0.23951 Time:0.005318\n",
      "[ Epoch 226, Train ] | Loss:0.00013 Time:0.083908\n",
      "[ Epoch 226, Val ] | Loss:0.26816 Time:0.005369\n",
      "[ Epoch 227, Train ] | Loss:0.00013 Time:0.083620\n",
      "[ Epoch 227, Val ] | Loss:0.25325 Time:0.005450\n",
      "[ Epoch 228, Train ] | Loss:0.00013 Time:0.083544\n",
      "[ Epoch 228, Val ] | Loss:0.22812 Time:0.005442\n",
      "[ Epoch 229, Train ] | Loss:0.00012 Time:0.083929\n",
      "[ Epoch 229, Val ] | Loss:0.22546 Time:0.005395\n",
      "[ Epoch 230, Train ] | Loss:0.00012 Time:0.083364\n",
      "[ Epoch 230, Val ] | Loss:0.20818 Time:0.005463\n",
      "[ Epoch 231, Train ] | Loss:0.00012 Time:0.083516\n",
      "[ Epoch 231, Val ] | Loss:0.22236 Time:0.005446\n",
      "[ Epoch 232, Train ] | Loss:0.00013 Time:0.084158\n",
      "[ Epoch 232, Val ] | Loss:0.25605 Time:0.005405\n",
      "[ Epoch 233, Train ] | Loss:0.00012 Time:0.083540\n",
      "[ Epoch 233, Val ] | Loss:0.23747 Time:0.005441\n",
      "[ Epoch 234, Train ] | Loss:0.00014 Time:0.083942\n",
      "[ Epoch 234, Val ] | Loss:0.22159 Time:0.005439\n",
      "[ Epoch 235, Train ] | Loss:0.00013 Time:0.083367\n",
      "[ Epoch 235, Val ] | Loss:0.24018 Time:0.005394\n",
      "[ Epoch 236, Train ] | Loss:0.00013 Time:0.083776\n",
      "[ Epoch 236, Val ] | Loss:0.23247 Time:0.005575\n",
      "[ Epoch 237, Train ] | Loss:0.00017 Time:0.082542\n",
      "[ Epoch 237, Val ] | Loss:0.23566 Time:0.005463\n",
      "[ Epoch 238, Train ] | Loss:0.00014 Time:0.083836\n",
      "[ Epoch 238, Val ] | Loss:0.34173 Time:0.005375\n",
      "[ Epoch 239, Train ] | Loss:0.00015 Time:0.086097\n",
      "[ Epoch 239, Val ] | Loss:0.21963 Time:0.005450\n",
      "[ Epoch 240, Train ] | Loss:0.00014 Time:0.083924\n",
      "[ Epoch 240, Val ] | Loss:0.19988 Time:0.005593\n",
      "save model with val loss 0.200\n",
      "[ Epoch 241, Train ] | Loss:0.00016 Time:0.084565\n",
      "[ Epoch 241, Val ] | Loss:0.23379 Time:0.005327\n",
      "[ Epoch 242, Train ] | Loss:0.00015 Time:0.083472\n",
      "[ Epoch 242, Val ] | Loss:0.22691 Time:0.005555\n",
      "[ Epoch 243, Train ] | Loss:0.00013 Time:0.083858\n",
      "[ Epoch 243, Val ] | Loss:0.33096 Time:0.005555\n",
      "[ Epoch 244, Train ] | Loss:0.00013 Time:0.083437\n",
      "[ Epoch 244, Val ] | Loss:0.22095 Time:0.005381\n",
      "[ Epoch 245, Train ] | Loss:0.00014 Time:0.083348\n",
      "[ Epoch 245, Val ] | Loss:0.19673 Time:0.005445\n",
      "save model with val loss 0.197\n",
      "[ Epoch 246, Train ] | Loss:0.00013 Time:0.085292\n",
      "[ Epoch 246, Val ] | Loss:0.28442 Time:0.005320\n",
      "[ Epoch 247, Train ] | Loss:0.00012 Time:0.083779\n",
      "[ Epoch 247, Val ] | Loss:0.23349 Time:0.005441\n",
      "[ Epoch 248, Train ] | Loss:0.00012 Time:0.082256\n",
      "[ Epoch 248, Val ] | Loss:0.22356 Time:0.005555\n",
      "[ Epoch 249, Train ] | Loss:0.00013 Time:0.083742\n",
      "[ Epoch 249, Val ] | Loss:0.22357 Time:0.005394\n",
      "[ Epoch 250, Train ] | Loss:0.00015 Time:0.083836\n",
      "[ Epoch 250, Val ] | Loss:0.23873 Time:0.005437\n",
      "[ Epoch 251, Train ] | Loss:0.00015 Time:0.083285\n",
      "[ Epoch 251, Val ] | Loss:0.20618 Time:0.005456\n",
      "[ Epoch 252, Train ] | Loss:0.00013 Time:0.083248\n",
      "[ Epoch 252, Val ] | Loss:0.27817 Time:0.005383\n",
      "[ Epoch 253, Train ] | Loss:0.00013 Time:0.083618\n",
      "[ Epoch 253, Val ] | Loss:0.19824 Time:0.005456\n",
      "[ Epoch 254, Train ] | Loss:0.00013 Time:0.083639\n",
      "[ Epoch 254, Val ] | Loss:0.19819 Time:0.005450\n",
      "[ Epoch 255, Train ] | Loss:0.00014 Time:0.082711\n",
      "[ Epoch 255, Val ] | Loss:0.24505 Time:0.005380\n",
      "[ Epoch 256, Train ] | Loss:0.00014 Time:0.083190\n",
      "[ Epoch 256, Val ] | Loss:0.20896 Time:0.005439\n",
      "[ Epoch 257, Train ] | Loss:0.00014 Time:0.085315\n",
      "[ Epoch 257, Val ] | Loss:0.25175 Time:0.005540\n",
      "[ Epoch 258, Train ] | Loss:0.00015 Time:0.084633\n",
      "[ Epoch 258, Val ] | Loss:0.20178 Time:0.007730\n",
      "[ Epoch 259, Train ] | Loss:0.00013 Time:0.085500\n",
      "[ Epoch 259, Val ] | Loss:0.24093 Time:0.005541\n",
      "[ Epoch 260, Train ] | Loss:0.00014 Time:0.083935\n",
      "[ Epoch 260, Val ] | Loss:0.22411 Time:0.005428\n",
      "[ Epoch 261, Train ] | Loss:0.00013 Time:0.086437\n",
      "[ Epoch 261, Val ] | Loss:0.19119 Time:0.005349\n",
      "save model with val loss 0.191\n",
      "[ Epoch 262, Train ] | Loss:0.00014 Time:0.085784\n",
      "[ Epoch 262, Val ] | Loss:0.23936 Time:0.005333\n",
      "[ Epoch 263, Train ] | Loss:0.00013 Time:0.085168\n",
      "[ Epoch 263, Val ] | Loss:0.22915 Time:0.005325\n",
      "[ Epoch 264, Train ] | Loss:0.00013 Time:0.084734\n",
      "[ Epoch 264, Val ] | Loss:0.20263 Time:0.005412\n",
      "[ Epoch 265, Train ] | Loss:0.00013 Time:0.084655\n",
      "[ Epoch 265, Val ] | Loss:0.21952 Time:0.005415\n",
      "[ Epoch 266, Train ] | Loss:0.00016 Time:0.086064\n",
      "[ Epoch 266, Val ] | Loss:0.25124 Time:0.005354\n",
      "[ Epoch 267, Train ] | Loss:0.00015 Time:0.085042\n",
      "[ Epoch 267, Val ] | Loss:0.24609 Time:0.005569\n",
      "[ Epoch 268, Train ] | Loss:0.00014 Time:0.083432\n",
      "[ Epoch 268, Val ] | Loss:0.19512 Time:0.005453\n",
      "[ Epoch 269, Train ] | Loss:0.00014 Time:0.082860\n",
      "[ Epoch 269, Val ] | Loss:0.24965 Time:0.005382\n",
      "[ Epoch 270, Train ] | Loss:0.00019 Time:0.083334\n",
      "[ Epoch 270, Val ] | Loss:0.22729 Time:0.005449\n",
      "[ Epoch 271, Train ] | Loss:0.00020 Time:0.083537\n",
      "[ Epoch 271, Val ] | Loss:0.29025 Time:0.005449\n",
      "[ Epoch 272, Train ] | Loss:0.00019 Time:0.082940\n",
      "[ Epoch 272, Val ] | Loss:0.33391 Time:0.005375\n",
      "[ Epoch 273, Train ] | Loss:0.00015 Time:0.083098\n",
      "[ Epoch 273, Val ] | Loss:0.20949 Time:0.005466\n",
      "[ Epoch 274, Train ] | Loss:0.00014 Time:0.083163\n",
      "[ Epoch 274, Val ] | Loss:0.24271 Time:0.005463\n",
      "[ Epoch 275, Train ] | Loss:0.00012 Time:0.082325\n",
      "[ Epoch 275, Val ] | Loss:0.22924 Time:0.005385\n",
      "[ Epoch 276, Train ] | Loss:0.00012 Time:0.083323\n",
      "[ Epoch 276, Val ] | Loss:0.25401 Time:0.005438\n",
      "[ Epoch 277, Train ] | Loss:0.00013 Time:0.083469\n",
      "[ Epoch 277, Val ] | Loss:0.20670 Time:0.005467\n",
      "[ Epoch 278, Train ] | Loss:0.00012 Time:0.082643\n",
      "[ Epoch 278, Val ] | Loss:0.22176 Time:0.005093\n",
      "[ Epoch 279, Train ] | Loss:0.00013 Time:0.082647\n",
      "[ Epoch 279, Val ] | Loss:0.26514 Time:0.005439\n",
      "[ Epoch 280, Train ] | Loss:0.00014 Time:0.083293\n",
      "[ Epoch 280, Val ] | Loss:0.19839 Time:0.005568\n",
      "[ Epoch 281, Train ] | Loss:0.00013 Time:0.083025\n",
      "[ Epoch 281, Val ] | Loss:0.21489 Time:0.005382\n",
      "[ Epoch 282, Train ] | Loss:0.00014 Time:0.083430\n",
      "[ Epoch 282, Val ] | Loss:0.22956 Time:0.005430\n",
      "[ Epoch 283, Train ] | Loss:0.00015 Time:0.083117\n",
      "[ Epoch 283, Val ] | Loss:0.24478 Time:0.005454\n",
      "[ Epoch 284, Train ] | Loss:0.00013 Time:0.082709\n",
      "[ Epoch 284, Val ] | Loss:0.19258 Time:0.005563\n",
      "[ Epoch 285, Train ] | Loss:0.00013 Time:0.081746\n",
      "[ Epoch 285, Val ] | Loss:0.26548 Time:0.005463\n",
      "[ Epoch 286, Train ] | Loss:0.00013 Time:0.083117\n",
      "[ Epoch 286, Val ] | Loss:0.23201 Time:0.005458\n",
      "[ Epoch 287, Train ] | Loss:0.00013 Time:0.083144\n",
      "[ Epoch 287, Val ] | Loss:0.19543 Time:0.005383\n",
      "[ Epoch 288, Train ] | Loss:0.00012 Time:0.082864\n",
      "[ Epoch 288, Val ] | Loss:0.22593 Time:0.005451\n",
      "[ Epoch 289, Train ] | Loss:0.00013 Time:0.083363\n",
      "[ Epoch 289, Val ] | Loss:0.22936 Time:0.005448\n",
      "[ Epoch 290, Train ] | Loss:0.00014 Time:0.082464\n",
      "[ Epoch 290, Val ] | Loss:0.30693 Time:0.005394\n",
      "[ Epoch 291, Train ] | Loss:0.00016 Time:0.083294\n",
      "[ Epoch 291, Val ] | Loss:0.21225 Time:0.005554\n",
      "[ Epoch 292, Train ] | Loss:0.00013 Time:0.082964\n",
      "[ Epoch 292, Val ] | Loss:0.20469 Time:0.005461\n",
      "[ Epoch 293, Train ] | Loss:0.00013 Time:0.082939\n",
      "[ Epoch 293, Val ] | Loss:0.24074 Time:0.005375\n",
      "[ Epoch 294, Train ] | Loss:0.00013 Time:0.082775\n",
      "[ Epoch 294, Val ] | Loss:0.20454 Time:0.005442\n",
      "[ Epoch 295, Train ] | Loss:0.00013 Time:0.082783\n",
      "[ Epoch 295, Val ] | Loss:0.20490 Time:0.005451\n",
      "[ Epoch 296, Train ] | Loss:0.00013 Time:0.082974\n",
      "[ Epoch 296, Val ] | Loss:0.21213 Time:0.005385\n",
      "[ Epoch 297, Train ] | Loss:0.00014 Time:0.082965\n",
      "[ Epoch 297, Val ] | Loss:0.23054 Time:0.005440\n",
      "[ Epoch 298, Train ] | Loss:0.00015 Time:0.082880\n",
      "[ Epoch 298, Val ] | Loss:0.21783 Time:0.005455\n",
      "[ Epoch 299, Train ] | Loss:0.00016 Time:0.082636\n",
      "[ Epoch 299, Val ] | Loss:0.22433 Time:0.005374\n",
      "[ Epoch 300, Train ] | Loss:0.00016 Time:0.082924\n",
      "[ Epoch 300, Val ] | Loss:0.20072 Time:0.005462\n",
      "Finish training!\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "06171aa5-0e20-4812-ac99-c6b4fbe2804b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin testing\n",
      "Test loss: 0.31151776015758514\n",
      "——————Test——————\n",
      "           class precision  recall f1-score support\n",
      "0              0    0.6667  0.8000   0.7273       5\n",
      "1              1    0.7500  0.6000   0.6667       5\n",
      "2              2    1.0000  1.0000   1.0000       5\n",
      "3              3    1.0000  0.8000   0.8889       5\n",
      "4              4    1.0000  1.0000   1.0000       5\n",
      "5              5    0.8000  0.8000   0.8000       5\n",
      "6              6    0.6667  0.8000   0.7273       5\n",
      "7              7    1.0000  0.8000   0.8889       5\n",
      "8              8    1.0000  1.0000   1.0000       5\n",
      "9              9    1.0000  0.8000   0.8889       5\n",
      "10            10    1.0000  1.0000   1.0000       5\n",
      "11            11    1.0000  0.8000   0.8889       5\n",
      "12            12    1.0000  1.0000   1.0000       5\n",
      "13            13    1.0000  0.8000   0.8889       5\n",
      "14            14    1.0000  1.0000   1.0000       5\n",
      "15            15    1.0000  1.0000   1.0000       5\n",
      "16            16    1.0000  1.0000   1.0000       5\n",
      "17            17    1.0000  1.0000   1.0000       5\n",
      "18            18    1.0000  0.8000   0.8889       5\n",
      "19            19    0.8333  1.0000   0.9091       5\n",
      "20            20    1.0000  0.8000   0.8889       5\n",
      "21            21    0.8333  1.0000   0.9091       5\n",
      "22            22    1.0000  0.8000   0.8889       5\n",
      "23            23    0.6667  0.8000   0.7273       5\n",
      "24            24    1.0000  1.0000   1.0000       5\n",
      "25            25    1.0000  1.0000   1.0000       5\n",
      "26            26    1.0000  1.0000   1.0000       5\n",
      "27            27    1.0000  1.0000   1.0000       5\n",
      "28            28    0.7143  1.0000   0.8333       5\n",
      "29            29    1.0000  1.0000   1.0000       5\n",
      "30            30    1.0000  1.0000   1.0000       5\n",
      "31            31    0.8333  1.0000   0.9091       5\n",
      "32            32    1.0000  1.0000   1.0000       5\n",
      "33            33    1.0000  1.0000   1.0000       5\n",
      "34            34    1.0000  0.8000   0.8889       5\n",
      "35            35    1.0000  1.0000   1.0000       5\n",
      "36            36    1.0000  1.0000   1.0000       5\n",
      "37            37    0.8333  1.0000   0.9091       5\n",
      "38            38    1.0000  1.0000   1.0000       5\n",
      "39            39    1.0000  1.0000   1.0000       5\n",
      "40            40    1.0000  1.0000   1.0000       5\n",
      "41            41    1.0000  1.0000   1.0000       5\n",
      "42            42    1.0000  1.0000   1.0000       5\n",
      "43            43    0.8333  1.0000   0.9091       5\n",
      "44            44    0.8000  0.8000   0.8000       5\n",
      "45            45    0.8000  0.8000   0.8000       5\n",
      "46            46    1.0000  0.6000   0.7500       5\n",
      "47            47    1.0000  1.0000   1.0000       5\n",
      "48            48    1.0000  1.0000   1.0000       5\n",
      "49            49    0.7143  1.0000   0.8333       5\n",
      "50      accuracy                     0.9240     250\n",
      "51     macro avg    0.9349  0.9240   0.9242     250\n",
      "52  weighted avg    0.9349  0.9240   0.9242     250\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "b2bd84fc-e722-41ee-a40a-ffae46f99fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_classifier_no_BN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_classifier_no_BN, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, 1, 1), # input channel, ouput channel, filter size, stride, padding\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0), # kernel size, stride, padding\n",
    "        \n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "            \n",
    "            nn.Conv2d(256, 512, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "        \n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 50)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1) # resize, first axis is batch_num\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "4db6d58d-0dbc-4f40-9c68-c7a9d7410c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cnn_no_BN = CNN_classifier_no_BN()\n",
    "epochs = 300\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "weight_decay = 5e-4\n",
    "optimizer = Adam(new_cnn_no_BN.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_dir = './output'\n",
    "model_name = 'new_cnn_no_BN'\n",
    "random_seed = 2022\n",
    "trainer = Trainer(model=new_cnn_no_BN, \n",
    "                  epochs=epochs, \n",
    "                  train_dataloader=train_loader, \n",
    "                  val_dataloader=val_loader, \n",
    "                  test_dataloader=test_loader,\n",
    "                  criterion=criterion, \n",
    "                  optimizer=optimizer, \n",
    "                  lr=learning_rate, \n",
    "                  device=device, \n",
    "                  model_dir=model_dir, \n",
    "                  model_name=model_name, \n",
    "                  random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "eaa9c04e-dc90-48df-8e81-153d385391a0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameters: 1759794, trainable parameters: 1759794\n",
      "\n",
      "        initial learning rate: 0.001, weight_decay: 0.0005,\n",
      "        total epochs: 300\n",
      "        \n",
      "Begin training, total epochs: 300\n",
      "[ Epoch 1, Train ] | Loss:3.92068 Time:0.079442\n",
      "[ Epoch 1, Val ] | Loss:3.90751 Time:0.005268\n",
      "save model with val loss 3.908\n",
      "[ Epoch 2, Train ] | Loss:3.91158 Time:0.066542\n",
      "[ Epoch 2, Val ] | Loss:3.90937 Time:0.005226\n",
      "[ Epoch 3, Train ] | Loss:3.90071 Time:0.065380\n",
      "[ Epoch 3, Val ] | Loss:3.91222 Time:0.007598\n",
      "[ Epoch 4, Train ] | Loss:3.87743 Time:0.065677\n",
      "[ Epoch 4, Val ] | Loss:3.89582 Time:0.005286\n",
      "save model with val loss 3.896\n",
      "[ Epoch 5, Train ] | Loss:3.85824 Time:0.067112\n",
      "[ Epoch 5, Val ] | Loss:3.83637 Time:0.005235\n",
      "save model with val loss 3.836\n",
      "[ Epoch 6, Train ] | Loss:3.76957 Time:0.067780\n",
      "[ Epoch 6, Val ] | Loss:3.75418 Time:0.006221\n",
      "save model with val loss 3.754\n",
      "[ Epoch 7, Train ] | Loss:3.62566 Time:0.067064\n",
      "[ Epoch 7, Val ] | Loss:3.55845 Time:0.005221\n",
      "save model with val loss 3.558\n",
      "[ Epoch 8, Train ] | Loss:3.34264 Time:0.067784\n",
      "[ Epoch 8, Val ] | Loss:3.26939 Time:0.005231\n",
      "save model with val loss 3.269\n",
      "[ Epoch 9, Train ] | Loss:3.10419 Time:0.065678\n",
      "[ Epoch 9, Val ] | Loss:2.96810 Time:0.005153\n",
      "save model with val loss 2.968\n",
      "[ Epoch 10, Train ] | Loss:3.02228 Time:0.064469\n",
      "[ Epoch 10, Val ] | Loss:2.77243 Time:0.005385\n",
      "save model with val loss 2.772\n",
      "[ Epoch 11, Train ] | Loss:2.71325 Time:0.066077\n",
      "[ Epoch 11, Val ] | Loss:2.73497 Time:0.005200\n",
      "save model with val loss 2.735\n",
      "[ Epoch 12, Train ] | Loss:2.47330 Time:0.065560\n",
      "[ Epoch 12, Val ] | Loss:2.44351 Time:0.004785\n",
      "save model with val loss 2.444\n",
      "[ Epoch 13, Train ] | Loss:2.37270 Time:0.065084\n",
      "[ Epoch 13, Val ] | Loss:2.30074 Time:0.005233\n",
      "save model with val loss 2.301\n",
      "[ Epoch 14, Train ] | Loss:2.33757 Time:0.068167\n",
      "[ Epoch 14, Val ] | Loss:2.33931 Time:0.005377\n",
      "[ Epoch 15, Train ] | Loss:2.19209 Time:0.064951\n",
      "[ Epoch 15, Val ] | Loss:2.29976 Time:0.005216\n",
      "save model with val loss 2.300\n",
      "[ Epoch 16, Train ] | Loss:2.20509 Time:0.065868\n",
      "[ Epoch 16, Val ] | Loss:2.10632 Time:0.004789\n",
      "save model with val loss 2.106\n",
      "[ Epoch 17, Train ] | Loss:2.03930 Time:0.065602\n",
      "[ Epoch 17, Val ] | Loss:2.12965 Time:0.005217\n",
      "[ Epoch 18, Train ] | Loss:1.86107 Time:0.065233\n",
      "[ Epoch 18, Val ] | Loss:2.00930 Time:0.005203\n",
      "save model with val loss 2.009\n",
      "[ Epoch 19, Train ] | Loss:1.76322 Time:0.065049\n",
      "[ Epoch 19, Val ] | Loss:2.13993 Time:0.005223\n",
      "[ Epoch 20, Train ] | Loss:1.76361 Time:0.064893\n",
      "[ Epoch 20, Val ] | Loss:1.92149 Time:0.005292\n",
      "save model with val loss 1.921\n",
      "[ Epoch 21, Train ] | Loss:1.63148 Time:0.066279\n",
      "[ Epoch 21, Val ] | Loss:2.26163 Time:0.005193\n",
      "[ Epoch 22, Train ] | Loss:1.73716 Time:0.065177\n",
      "[ Epoch 22, Val ] | Loss:1.79957 Time:0.005296\n",
      "save model with val loss 1.800\n",
      "[ Epoch 23, Train ] | Loss:1.60187 Time:0.065100\n",
      "[ Epoch 23, Val ] | Loss:1.75092 Time:0.005229\n",
      "save model with val loss 1.751\n",
      "[ Epoch 24, Train ] | Loss:1.41895 Time:0.065096\n",
      "[ Epoch 24, Val ] | Loss:1.72601 Time:0.005182\n",
      "save model with val loss 1.726\n",
      "[ Epoch 25, Train ] | Loss:1.37249 Time:0.065495\n",
      "[ Epoch 25, Val ] | Loss:1.54174 Time:0.005225\n",
      "save model with val loss 1.542\n",
      "[ Epoch 26, Train ] | Loss:1.24701 Time:0.067149\n",
      "[ Epoch 26, Val ] | Loss:1.82253 Time:0.005219\n",
      "[ Epoch 27, Train ] | Loss:1.25753 Time:0.065353\n",
      "[ Epoch 27, Val ] | Loss:1.79118 Time:0.005224\n",
      "[ Epoch 28, Train ] | Loss:1.28600 Time:0.064709\n",
      "[ Epoch 28, Val ] | Loss:1.53138 Time:0.005308\n",
      "save model with val loss 1.531\n",
      "[ Epoch 29, Train ] | Loss:1.11091 Time:0.065252\n",
      "[ Epoch 29, Val ] | Loss:1.60621 Time:0.005212\n",
      "[ Epoch 30, Train ] | Loss:1.10535 Time:0.067291\n",
      "[ Epoch 30, Val ] | Loss:1.67622 Time:0.005222\n",
      "[ Epoch 31, Train ] | Loss:1.14255 Time:0.065398\n",
      "[ Epoch 31, Val ] | Loss:1.67210 Time:0.005298\n",
      "[ Epoch 32, Train ] | Loss:0.96692 Time:0.064648\n",
      "[ Epoch 32, Val ] | Loss:1.39077 Time:0.005305\n",
      "save model with val loss 1.391\n",
      "[ Epoch 33, Train ] | Loss:0.94884 Time:0.067114\n",
      "[ Epoch 33, Val ] | Loss:1.46817 Time:0.005169\n",
      "[ Epoch 34, Train ] | Loss:0.92518 Time:0.065160\n",
      "[ Epoch 34, Val ] | Loss:1.46550 Time:0.005299\n",
      "[ Epoch 35, Train ] | Loss:0.95399 Time:0.064673\n",
      "[ Epoch 35, Val ] | Loss:1.74211 Time:0.005297\n",
      "[ Epoch 36, Train ] | Loss:0.97172 Time:0.065007\n",
      "[ Epoch 36, Val ] | Loss:1.82720 Time:0.005237\n",
      "[ Epoch 37, Train ] | Loss:0.91493 Time:0.065007\n",
      "[ Epoch 37, Val ] | Loss:1.37256 Time:0.005280\n",
      "save model with val loss 1.373\n",
      "[ Epoch 38, Train ] | Loss:0.76428 Time:0.067313\n",
      "[ Epoch 38, Val ] | Loss:1.29867 Time:0.005215\n",
      "save model with val loss 1.299\n",
      "[ Epoch 39, Train ] | Loss:0.70121 Time:0.065264\n",
      "[ Epoch 39, Val ] | Loss:1.31550 Time:0.005177\n",
      "[ Epoch 40, Train ] | Loss:0.65400 Time:0.067368\n",
      "[ Epoch 40, Val ] | Loss:1.61149 Time:0.005289\n",
      "[ Epoch 41, Train ] | Loss:0.83779 Time:0.067087\n",
      "[ Epoch 41, Val ] | Loss:1.69247 Time:0.005299\n",
      "[ Epoch 42, Train ] | Loss:0.79285 Time:0.065185\n",
      "[ Epoch 42, Val ] | Loss:1.25451 Time:0.005227\n",
      "save model with val loss 1.255\n",
      "[ Epoch 43, Train ] | Loss:0.74036 Time:0.066972\n",
      "[ Epoch 43, Val ] | Loss:1.23899 Time:0.005057\n",
      "save model with val loss 1.239\n",
      "[ Epoch 44, Train ] | Loss:0.62293 Time:0.064977\n",
      "[ Epoch 44, Val ] | Loss:1.19800 Time:0.005245\n",
      "save model with val loss 1.198\n",
      "[ Epoch 45, Train ] | Loss:0.61295 Time:0.069747\n",
      "[ Epoch 45, Val ] | Loss:1.16297 Time:0.005183\n",
      "save model with val loss 1.163\n",
      "[ Epoch 46, Train ] | Loss:0.56357 Time:0.065269\n",
      "[ Epoch 46, Val ] | Loss:1.21196 Time:0.005217\n",
      "[ Epoch 47, Train ] | Loss:0.52240 Time:0.066559\n",
      "[ Epoch 47, Val ] | Loss:1.08270 Time:0.005285\n",
      "save model with val loss 1.083\n",
      "[ Epoch 48, Train ] | Loss:0.52768 Time:0.067168\n",
      "[ Epoch 48, Val ] | Loss:1.12657 Time:0.005178\n",
      "[ Epoch 49, Train ] | Loss:0.45494 Time:0.066886\n",
      "[ Epoch 49, Val ] | Loss:1.31301 Time:0.005403\n",
      "[ Epoch 50, Train ] | Loss:0.42767 Time:0.066267\n",
      "[ Epoch 50, Val ] | Loss:1.29079 Time:0.005408\n",
      "[ Epoch 51, Train ] | Loss:0.35224 Time:0.066653\n",
      "[ Epoch 51, Val ] | Loss:1.09036 Time:0.005236\n",
      "[ Epoch 52, Train ] | Loss:0.37665 Time:0.067197\n",
      "[ Epoch 52, Val ] | Loss:1.13158 Time:0.005303\n",
      "[ Epoch 53, Train ] | Loss:0.42768 Time:0.066292\n",
      "[ Epoch 53, Val ] | Loss:1.22422 Time:0.005293\n",
      "[ Epoch 54, Train ] | Loss:0.34674 Time:0.066909\n",
      "[ Epoch 54, Val ] | Loss:1.15708 Time:0.005222\n",
      "[ Epoch 55, Train ] | Loss:0.32812 Time:0.066403\n",
      "[ Epoch 55, Val ] | Loss:1.01036 Time:0.005304\n",
      "save model with val loss 1.010\n",
      "[ Epoch 56, Train ] | Loss:0.35818 Time:0.066970\n",
      "[ Epoch 56, Val ] | Loss:1.36295 Time:0.005233\n",
      "[ Epoch 57, Train ] | Loss:0.38510 Time:0.066456\n",
      "[ Epoch 57, Val ] | Loss:1.09637 Time:0.004752\n",
      "[ Epoch 58, Train ] | Loss:0.47837 Time:0.067129\n",
      "[ Epoch 58, Val ] | Loss:1.16430 Time:0.005303\n",
      "[ Epoch 59, Train ] | Loss:0.40542 Time:0.066191\n",
      "[ Epoch 59, Val ] | Loss:1.09417 Time:0.005304\n",
      "[ Epoch 60, Train ] | Loss:0.34598 Time:0.066520\n",
      "[ Epoch 60, Val ] | Loss:1.75417 Time:0.005218\n",
      "[ Epoch 61, Train ] | Loss:0.38262 Time:0.066427\n",
      "[ Epoch 61, Val ] | Loss:1.29641 Time:0.005306\n",
      "[ Epoch 62, Train ] | Loss:0.26090 Time:0.066743\n",
      "[ Epoch 62, Val ] | Loss:1.17916 Time:0.005297\n",
      "[ Epoch 63, Train ] | Loss:0.23086 Time:0.068738\n",
      "[ Epoch 63, Val ] | Loss:1.19367 Time:0.005227\n",
      "[ Epoch 64, Train ] | Loss:0.19983 Time:0.066336\n",
      "[ Epoch 64, Val ] | Loss:1.21491 Time:0.005291\n",
      "[ Epoch 65, Train ] | Loss:0.19082 Time:0.066974\n",
      "[ Epoch 65, Val ] | Loss:1.33300 Time:0.005409\n",
      "[ Epoch 66, Train ] | Loss:0.18582 Time:0.066409\n",
      "[ Epoch 66, Val ] | Loss:1.48580 Time:0.005404\n",
      "[ Epoch 67, Train ] | Loss:0.17571 Time:0.066931\n",
      "[ Epoch 67, Val ] | Loss:1.49216 Time:0.005295\n",
      "[ Epoch 68, Train ] | Loss:0.13726 Time:0.066876\n",
      "[ Epoch 68, Val ] | Loss:1.37748 Time:0.005288\n",
      "[ Epoch 69, Train ] | Loss:0.14754 Time:0.066333\n",
      "[ Epoch 69, Val ] | Loss:1.28042 Time:0.005235\n",
      "[ Epoch 70, Train ] | Loss:0.12334 Time:0.069286\n",
      "[ Epoch 70, Val ] | Loss:1.30612 Time:0.005289\n",
      "[ Epoch 71, Train ] | Loss:0.13682 Time:0.066049\n",
      "[ Epoch 71, Val ] | Loss:1.63675 Time:0.005422\n",
      "[ Epoch 72, Train ] | Loss:0.18021 Time:0.068616\n",
      "[ Epoch 72, Val ] | Loss:1.69317 Time:0.005204\n",
      "[ Epoch 73, Train ] | Loss:0.20446 Time:0.066512\n",
      "[ Epoch 73, Val ] | Loss:1.47309 Time:0.005303\n",
      "[ Epoch 74, Train ] | Loss:0.24843 Time:0.068901\n",
      "[ Epoch 74, Val ] | Loss:1.22388 Time:0.005411\n",
      "[ Epoch 75, Train ] | Loss:0.19644 Time:0.068502\n",
      "[ Epoch 75, Val ] | Loss:1.24064 Time:0.005232\n",
      "[ Epoch 76, Train ] | Loss:0.18556 Time:0.066153\n",
      "[ Epoch 76, Val ] | Loss:1.28009 Time:0.005295\n",
      "[ Epoch 77, Train ] | Loss:0.17950 Time:0.066906\n",
      "[ Epoch 77, Val ] | Loss:1.13557 Time:0.005296\n",
      "[ Epoch 78, Train ] | Loss:0.21842 Time:0.068392\n",
      "[ Epoch 78, Val ] | Loss:1.47446 Time:0.005233\n",
      "[ Epoch 79, Train ] | Loss:0.15789 Time:0.066851\n",
      "[ Epoch 79, Val ] | Loss:1.45771 Time:0.005411\n",
      "[ Epoch 80, Train ] | Loss:0.22322 Time:0.066803\n",
      "[ Epoch 80, Val ] | Loss:1.70812 Time:0.005304\n",
      "[ Epoch 81, Train ] | Loss:0.17223 Time:0.068799\n",
      "[ Epoch 81, Val ] | Loss:1.51983 Time:0.005246\n",
      "[ Epoch 82, Train ] | Loss:0.11943 Time:0.066259\n",
      "[ Epoch 82, Val ] | Loss:1.18317 Time:0.005301\n",
      "[ Epoch 83, Train ] | Loss:0.09849 Time:0.066131\n",
      "[ Epoch 83, Val ] | Loss:1.13172 Time:0.005400\n",
      "[ Epoch 84, Train ] | Loss:0.09732 Time:0.066688\n",
      "[ Epoch 84, Val ] | Loss:1.18485 Time:0.005239\n",
      "[ Epoch 85, Train ] | Loss:0.11952 Time:0.068242\n",
      "[ Epoch 85, Val ] | Loss:1.34047 Time:0.005420\n",
      "[ Epoch 86, Train ] | Loss:0.08402 Time:0.065897\n",
      "[ Epoch 86, Val ] | Loss:1.55458 Time:0.005291\n",
      "[ Epoch 87, Train ] | Loss:0.09731 Time:0.068187\n",
      "[ Epoch 87, Val ] | Loss:1.63196 Time:0.005223\n",
      "[ Epoch 88, Train ] | Loss:0.09681 Time:0.068595\n",
      "[ Epoch 88, Val ] | Loss:1.26937 Time:0.005302\n",
      "[ Epoch 89, Train ] | Loss:0.10667 Time:0.065981\n",
      "[ Epoch 89, Val ] | Loss:1.41420 Time:0.005307\n",
      "[ Epoch 90, Train ] | Loss:0.10026 Time:0.066342\n",
      "[ Epoch 90, Val ] | Loss:1.43650 Time:0.005234\n",
      "[ Epoch 91, Train ] | Loss:0.05391 Time:0.066056\n",
      "[ Epoch 91, Val ] | Loss:1.28224 Time:0.007414\n",
      "[ Epoch 92, Train ] | Loss:0.07230 Time:0.066082\n",
      "[ Epoch 92, Val ] | Loss:1.35228 Time:0.004818\n",
      "[ Epoch 93, Train ] | Loss:0.04823 Time:0.065799\n",
      "[ Epoch 93, Val ] | Loss:1.26118 Time:0.005223\n",
      "[ Epoch 94, Train ] | Loss:0.09212 Time:0.066009\n",
      "[ Epoch 94, Val ] | Loss:1.34249 Time:0.005300\n",
      "[ Epoch 95, Train ] | Loss:0.09402 Time:0.065625\n",
      "[ Epoch 95, Val ] | Loss:1.30853 Time:0.005418\n",
      "[ Epoch 96, Train ] | Loss:0.07348 Time:0.068348\n",
      "[ Epoch 96, Val ] | Loss:1.69008 Time:0.005244\n",
      "[ Epoch 97, Train ] | Loss:0.10340 Time:0.066123\n",
      "[ Epoch 97, Val ] | Loss:1.59959 Time:0.005292\n",
      "[ Epoch 98, Train ] | Loss:0.09114 Time:0.066210\n",
      "[ Epoch 98, Val ] | Loss:1.49164 Time:0.005410\n",
      "[ Epoch 99, Train ] | Loss:0.10852 Time:0.066546\n",
      "[ Epoch 99, Val ] | Loss:1.13480 Time:0.005244\n",
      "[ Epoch 100, Train ] | Loss:0.09745 Time:0.065934\n",
      "[ Epoch 100, Val ] | Loss:1.58647 Time:0.005419\n",
      "[ Epoch 101, Train ] | Loss:0.10603 Time:0.068651\n",
      "[ Epoch 101, Val ] | Loss:1.14407 Time:0.006306\n",
      "[ Epoch 102, Train ] | Loss:0.09975 Time:0.066437\n",
      "[ Epoch 102, Val ] | Loss:1.59982 Time:0.005222\n",
      "[ Epoch 103, Train ] | Loss:0.11741 Time:0.065737\n",
      "[ Epoch 103, Val ] | Loss:1.56125 Time:0.005295\n",
      "[ Epoch 104, Train ] | Loss:0.07873 Time:0.066003\n",
      "[ Epoch 104, Val ] | Loss:1.46151 Time:0.005303\n",
      "[ Epoch 105, Train ] | Loss:0.06688 Time:0.066485\n",
      "[ Epoch 105, Val ] | Loss:1.25726 Time:0.005116\n",
      "[ Epoch 106, Train ] | Loss:0.05225 Time:0.065798\n",
      "[ Epoch 106, Val ] | Loss:1.13486 Time:0.005305\n",
      "[ Epoch 107, Train ] | Loss:0.06336 Time:0.066089\n",
      "[ Epoch 107, Val ] | Loss:1.35535 Time:0.005299\n",
      "[ Epoch 108, Train ] | Loss:0.04929 Time:0.066372\n",
      "[ Epoch 108, Val ] | Loss:1.48927 Time:0.005250\n",
      "[ Epoch 109, Train ] | Loss:0.03017 Time:0.069152\n",
      "[ Epoch 109, Val ] | Loss:1.22533 Time:0.005432\n",
      "[ Epoch 110, Train ] | Loss:0.02146 Time:0.065218\n",
      "[ Epoch 110, Val ] | Loss:1.23933 Time:0.005423\n",
      "[ Epoch 111, Train ] | Loss:0.01532 Time:0.065966\n",
      "[ Epoch 111, Val ] | Loss:1.10492 Time:0.005238\n",
      "[ Epoch 112, Train ] | Loss:0.01360 Time:0.066084\n",
      "[ Epoch 112, Val ] | Loss:1.17024 Time:0.005297\n",
      "[ Epoch 113, Train ] | Loss:0.01345 Time:0.065754\n",
      "[ Epoch 113, Val ] | Loss:1.12539 Time:0.005300\n",
      "[ Epoch 114, Train ] | Loss:0.01522 Time:0.068297\n",
      "[ Epoch 114, Val ] | Loss:1.16135 Time:0.005246\n",
      "[ Epoch 115, Train ] | Loss:0.00792 Time:0.065789\n",
      "[ Epoch 115, Val ] | Loss:1.31714 Time:0.005294\n",
      "[ Epoch 116, Train ] | Loss:0.01254 Time:0.066009\n",
      "[ Epoch 116, Val ] | Loss:1.20331 Time:0.005305\n",
      "[ Epoch 117, Train ] | Loss:0.01458 Time:0.068086\n",
      "[ Epoch 117, Val ] | Loss:1.19262 Time:0.005409\n",
      "[ Epoch 118, Train ] | Loss:0.03784 Time:0.066314\n",
      "[ Epoch 118, Val ] | Loss:1.20871 Time:0.005305\n",
      "[ Epoch 119, Train ] | Loss:0.06184 Time:0.065995\n",
      "[ Epoch 119, Val ] | Loss:1.79332 Time:0.005296\n",
      "[ Epoch 120, Train ] | Loss:0.05848 Time:0.066487\n",
      "[ Epoch 120, Val ] | Loss:1.28581 Time:0.005241\n",
      "[ Epoch 121, Train ] | Loss:0.02839 Time:0.065524\n",
      "[ Epoch 121, Val ] | Loss:1.51677 Time:0.005416\n",
      "[ Epoch 122, Train ] | Loss:0.03148 Time:0.065688\n",
      "[ Epoch 122, Val ] | Loss:1.43221 Time:0.005312\n",
      "[ Epoch 123, Train ] | Loss:0.02051 Time:0.067932\n",
      "[ Epoch 123, Val ] | Loss:1.36299 Time:0.005244\n",
      "[ Epoch 124, Train ] | Loss:0.02400 Time:0.065854\n",
      "[ Epoch 124, Val ] | Loss:1.35760 Time:0.005294\n",
      "[ Epoch 125, Train ] | Loss:0.01774 Time:0.065490\n",
      "[ Epoch 125, Val ] | Loss:1.24626 Time:0.005297\n",
      "[ Epoch 126, Train ] | Loss:0.01515 Time:0.067915\n",
      "[ Epoch 126, Val ] | Loss:1.36726 Time:0.005241\n",
      "[ Epoch 127, Train ] | Loss:0.00799 Time:0.064922\n",
      "[ Epoch 127, Val ] | Loss:1.29746 Time:0.005289\n",
      "[ Epoch 128, Train ] | Loss:0.03179 Time:0.066310\n",
      "[ Epoch 128, Val ] | Loss:1.60184 Time:0.005302\n",
      "[ Epoch 129, Train ] | Loss:0.03550 Time:0.065579\n",
      "[ Epoch 129, Val ] | Loss:1.23121 Time:0.005245\n",
      "[ Epoch 130, Train ] | Loss:0.03863 Time:0.065523\n",
      "[ Epoch 130, Val ] | Loss:1.24013 Time:0.005298\n",
      "[ Epoch 131, Train ] | Loss:0.02495 Time:0.065149\n",
      "[ Epoch 131, Val ] | Loss:1.82343 Time:0.005302\n",
      "[ Epoch 132, Train ] | Loss:0.03192 Time:0.065593\n",
      "[ Epoch 132, Val ] | Loss:1.28964 Time:0.005242\n",
      "[ Epoch 133, Train ] | Loss:0.03123 Time:0.065482\n",
      "[ Epoch 133, Val ] | Loss:1.31532 Time:0.005304\n",
      "[ Epoch 134, Train ] | Loss:0.02076 Time:0.065451\n",
      "[ Epoch 134, Val ] | Loss:1.50367 Time:0.005302\n",
      "[ Epoch 135, Train ] | Loss:0.01989 Time:0.065082\n",
      "[ Epoch 135, Val ] | Loss:1.33472 Time:0.004833\n",
      "[ Epoch 136, Train ] | Loss:0.01128 Time:0.065725\n",
      "[ Epoch 136, Val ] | Loss:1.23104 Time:0.005300\n",
      "[ Epoch 137, Train ] | Loss:0.01344 Time:0.065199\n",
      "[ Epoch 137, Val ] | Loss:1.51155 Time:0.005295\n",
      "[ Epoch 138, Train ] | Loss:0.01200 Time:0.067791\n",
      "[ Epoch 138, Val ] | Loss:1.13333 Time:0.005419\n",
      "[ Epoch 139, Train ] | Loss:0.01301 Time:0.065310\n",
      "[ Epoch 139, Val ] | Loss:1.32161 Time:0.005291\n",
      "[ Epoch 140, Train ] | Loss:0.01201 Time:0.065789\n",
      "[ Epoch 140, Val ] | Loss:1.22139 Time:0.005218\n",
      "[ Epoch 141, Train ] | Loss:0.00814 Time:0.065437\n",
      "[ Epoch 141, Val ] | Loss:1.40556 Time:0.005411\n",
      "[ Epoch 142, Train ] | Loss:0.01550 Time:0.065032\n",
      "[ Epoch 142, Val ] | Loss:1.39072 Time:0.005307\n",
      "[ Epoch 143, Train ] | Loss:0.01439 Time:0.065641\n",
      "[ Epoch 143, Val ] | Loss:1.62314 Time:0.005313\n",
      "[ Epoch 144, Train ] | Loss:0.01316 Time:0.068115\n",
      "[ Epoch 144, Val ] | Loss:1.22383 Time:0.005255\n",
      "[ Epoch 145, Train ] | Loss:0.01302 Time:0.068289\n",
      "[ Epoch 145, Val ] | Loss:1.56181 Time:0.005304\n",
      "[ Epoch 146, Train ] | Loss:0.00575 Time:0.065165\n",
      "[ Epoch 146, Val ] | Loss:1.32150 Time:0.005296\n",
      "[ Epoch 147, Train ] | Loss:0.00885 Time:0.064971\n",
      "[ Epoch 147, Val ] | Loss:1.49725 Time:0.005414\n",
      "[ Epoch 148, Train ] | Loss:0.00778 Time:0.066055\n",
      "[ Epoch 148, Val ] | Loss:1.19307 Time:0.005301\n",
      "[ Epoch 149, Train ] | Loss:0.00723 Time:0.065829\n",
      "[ Epoch 149, Val ] | Loss:1.28314 Time:0.005426\n",
      "[ Epoch 150, Train ] | Loss:0.00993 Time:0.067327\n",
      "[ Epoch 150, Val ] | Loss:1.51446 Time:0.005234\n",
      "[ Epoch 151, Train ] | Loss:0.01300 Time:0.065531\n",
      "[ Epoch 151, Val ] | Loss:1.36828 Time:0.005299\n",
      "[ Epoch 152, Train ] | Loss:0.01787 Time:0.065537\n",
      "[ Epoch 152, Val ] | Loss:1.56869 Time:0.005293\n",
      "[ Epoch 153, Train ] | Loss:0.02281 Time:0.065531\n",
      "[ Epoch 153, Val ] | Loss:1.47950 Time:0.005240\n",
      "[ Epoch 154, Train ] | Loss:0.01657 Time:0.245956\n",
      "[ Epoch 154, Val ] | Loss:1.26017 Time:0.005103\n",
      "[ Epoch 155, Train ] | Loss:0.01703 Time:0.064638\n",
      "[ Epoch 155, Val ] | Loss:1.57666 Time:0.005274\n",
      "[ Epoch 156, Train ] | Loss:0.01738 Time:0.064858\n",
      "[ Epoch 156, Val ] | Loss:1.57929 Time:0.005291\n",
      "[ Epoch 157, Train ] | Loss:0.01631 Time:0.067811\n",
      "[ Epoch 157, Val ] | Loss:1.43472 Time:0.005221\n",
      "[ Epoch 158, Train ] | Loss:0.01533 Time:0.065765\n",
      "[ Epoch 158, Val ] | Loss:1.49393 Time:0.005305\n",
      "[ Epoch 159, Train ] | Loss:0.04776 Time:0.064791\n",
      "[ Epoch 159, Val ] | Loss:1.27069 Time:0.005292\n",
      "[ Epoch 160, Train ] | Loss:0.07418 Time:0.065291\n",
      "[ Epoch 160, Val ] | Loss:1.91120 Time:0.005238\n",
      "[ Epoch 161, Train ] | Loss:0.14389 Time:0.065234\n",
      "[ Epoch 161, Val ] | Loss:1.58837 Time:0.005296\n",
      "[ Epoch 162, Train ] | Loss:0.11504 Time:0.064986\n",
      "[ Epoch 162, Val ] | Loss:1.64139 Time:0.005314\n",
      "[ Epoch 163, Train ] | Loss:0.13829 Time:0.065121\n",
      "[ Epoch 163, Val ] | Loss:1.96143 Time:0.005232\n",
      "[ Epoch 164, Train ] | Loss:0.21245 Time:0.065192\n",
      "[ Epoch 164, Val ] | Loss:1.16567 Time:0.005298\n",
      "[ Epoch 165, Train ] | Loss:0.07820 Time:0.064695\n",
      "[ Epoch 165, Val ] | Loss:1.59511 Time:0.005304\n",
      "[ Epoch 166, Train ] | Loss:0.11385 Time:0.069643\n",
      "[ Epoch 166, Val ] | Loss:1.76162 Time:0.005207\n",
      "[ Epoch 167, Train ] | Loss:0.07530 Time:0.065516\n",
      "[ Epoch 167, Val ] | Loss:1.19416 Time:0.005285\n",
      "[ Epoch 168, Train ] | Loss:0.03186 Time:0.064723\n",
      "[ Epoch 168, Val ] | Loss:1.10664 Time:0.005301\n",
      "[ Epoch 169, Train ] | Loss:0.02658 Time:0.064379\n",
      "[ Epoch 169, Val ] | Loss:1.06032 Time:0.005240\n",
      "[ Epoch 170, Train ] | Loss:0.01534 Time:0.065450\n",
      "[ Epoch 170, Val ] | Loss:1.20378 Time:0.005295\n",
      "[ Epoch 171, Train ] | Loss:0.01635 Time:0.065091\n",
      "[ Epoch 171, Val ] | Loss:1.18009 Time:0.005296\n",
      "[ Epoch 172, Train ] | Loss:0.01242 Time:0.069220\n",
      "[ Epoch 172, Val ] | Loss:1.16478 Time:0.005413\n",
      "[ Epoch 173, Train ] | Loss:0.00921 Time:0.065162\n",
      "[ Epoch 173, Val ] | Loss:1.16435 Time:0.005308\n",
      "[ Epoch 174, Train ] | Loss:0.00590 Time:0.065214\n",
      "[ Epoch 174, Val ] | Loss:1.24188 Time:0.005299\n",
      "[ Epoch 175, Train ] | Loss:0.00783 Time:0.064791\n",
      "[ Epoch 175, Val ] | Loss:1.32610 Time:0.005235\n",
      "[ Epoch 176, Train ] | Loss:0.00639 Time:0.065310\n",
      "[ Epoch 176, Val ] | Loss:1.31873 Time:0.005299\n",
      "[ Epoch 177, Train ] | Loss:0.00593 Time:0.064545\n",
      "[ Epoch 177, Val ] | Loss:1.21198 Time:0.005291\n",
      "[ Epoch 178, Train ] | Loss:0.00906 Time:0.064865\n",
      "[ Epoch 178, Val ] | Loss:1.21535 Time:0.005243\n",
      "[ Epoch 179, Train ] | Loss:0.00645 Time:0.067333\n",
      "[ Epoch 179, Val ] | Loss:1.19057 Time:0.005295\n",
      "[ Epoch 180, Train ] | Loss:0.00641 Time:0.065057\n",
      "[ Epoch 180, Val ] | Loss:1.21915 Time:0.005288\n",
      "[ Epoch 181, Train ] | Loss:0.00737 Time:0.065378\n",
      "[ Epoch 181, Val ] | Loss:1.16416 Time:0.005230\n",
      "[ Epoch 182, Train ] | Loss:0.00752 Time:0.064997\n",
      "[ Epoch 182, Val ] | Loss:1.22097 Time:0.005280\n",
      "[ Epoch 183, Train ] | Loss:0.00490 Time:0.065176\n",
      "[ Epoch 183, Val ] | Loss:1.20736 Time:0.005296\n",
      "[ Epoch 184, Train ] | Loss:0.00592 Time:0.067137\n",
      "[ Epoch 184, Val ] | Loss:1.30320 Time:0.005235\n",
      "[ Epoch 185, Train ] | Loss:0.00365 Time:0.067927\n",
      "[ Epoch 185, Val ] | Loss:1.28196 Time:0.005298\n",
      "[ Epoch 186, Train ] | Loss:0.00338 Time:0.065193\n",
      "[ Epoch 186, Val ] | Loss:1.35114 Time:0.005278\n",
      "[ Epoch 187, Train ] | Loss:0.00269 Time:0.066350\n",
      "[ Epoch 187, Val ] | Loss:1.35584 Time:0.005403\n",
      "[ Epoch 188, Train ] | Loss:0.00372 Time:0.064531\n",
      "[ Epoch 188, Val ] | Loss:1.32538 Time:0.005296\n",
      "[ Epoch 189, Train ] | Loss:0.00443 Time:0.070019\n",
      "[ Epoch 189, Val ] | Loss:1.20962 Time:0.005295\n",
      "[ Epoch 190, Train ] | Loss:0.00453 Time:0.065198\n",
      "[ Epoch 190, Val ] | Loss:1.47292 Time:0.005234\n",
      "[ Epoch 191, Train ] | Loss:0.00711 Time:0.066622\n",
      "[ Epoch 191, Val ] | Loss:1.58019 Time:0.005293\n",
      "[ Epoch 192, Train ] | Loss:0.01171 Time:0.066539\n",
      "[ Epoch 192, Val ] | Loss:1.34479 Time:0.005292\n",
      "[ Epoch 193, Train ] | Loss:0.00705 Time:0.066779\n",
      "[ Epoch 193, Val ] | Loss:1.41668 Time:0.005243\n",
      "[ Epoch 194, Train ] | Loss:0.01084 Time:0.067515\n",
      "[ Epoch 194, Val ] | Loss:1.35288 Time:0.005296\n",
      "[ Epoch 195, Train ] | Loss:0.00932 Time:0.067327\n",
      "[ Epoch 195, Val ] | Loss:1.50905 Time:0.005300\n",
      "[ Epoch 196, Train ] | Loss:0.00629 Time:0.067588\n",
      "[ Epoch 196, Val ] | Loss:1.48614 Time:0.005237\n",
      "[ Epoch 197, Train ] | Loss:0.00302 Time:0.066511\n",
      "[ Epoch 197, Val ] | Loss:1.36867 Time:0.005294\n",
      "[ Epoch 198, Train ] | Loss:0.00464 Time:0.069500\n",
      "[ Epoch 198, Val ] | Loss:1.23189 Time:0.005302\n",
      "[ Epoch 199, Train ] | Loss:0.00395 Time:0.069579\n",
      "[ Epoch 199, Val ] | Loss:1.27234 Time:0.005243\n",
      "[ Epoch 200, Train ] | Loss:0.00407 Time:0.066841\n",
      "[ Epoch 200, Val ] | Loss:1.12513 Time:0.005288\n",
      "[ Epoch 201, Train ] | Loss:0.00417 Time:0.066540\n",
      "[ Epoch 201, Val ] | Loss:1.36356 Time:0.005414\n",
      "[ Epoch 202, Train ] | Loss:0.00377 Time:0.066851\n",
      "[ Epoch 202, Val ] | Loss:1.28400 Time:0.005227\n",
      "[ Epoch 203, Train ] | Loss:0.00896 Time:0.066332\n",
      "[ Epoch 203, Val ] | Loss:1.30916 Time:0.005302\n",
      "[ Epoch 204, Train ] | Loss:0.00654 Time:0.066450\n",
      "[ Epoch 204, Val ] | Loss:1.30879 Time:0.005415\n",
      "[ Epoch 205, Train ] | Loss:0.00477 Time:0.066925\n",
      "[ Epoch 205, Val ] | Loss:1.33736 Time:0.005238\n",
      "[ Epoch 206, Train ] | Loss:0.00514 Time:0.066556\n",
      "[ Epoch 206, Val ] | Loss:1.40090 Time:0.005301\n",
      "[ Epoch 207, Train ] | Loss:0.00331 Time:0.067127\n",
      "[ Epoch 207, Val ] | Loss:1.26152 Time:0.005310\n",
      "[ Epoch 208, Train ] | Loss:0.00715 Time:0.066626\n",
      "[ Epoch 208, Val ] | Loss:1.34579 Time:0.005267\n",
      "[ Epoch 209, Train ] | Loss:0.00307 Time:0.066226\n",
      "[ Epoch 209, Val ] | Loss:1.27013 Time:0.005293\n",
      "[ Epoch 210, Train ] | Loss:0.00823 Time:0.066532\n",
      "[ Epoch 210, Val ] | Loss:1.18999 Time:0.005311\n",
      "[ Epoch 211, Train ] | Loss:0.00353 Time:0.066488\n",
      "[ Epoch 211, Val ] | Loss:1.27745 Time:0.005223\n",
      "[ Epoch 212, Train ] | Loss:0.00300 Time:0.068917\n",
      "[ Epoch 212, Val ] | Loss:1.45587 Time:0.005300\n",
      "[ Epoch 213, Train ] | Loss:0.00386 Time:0.068993\n",
      "[ Epoch 213, Val ] | Loss:1.19197 Time:0.005284\n",
      "[ Epoch 214, Train ] | Loss:0.00491 Time:0.069442\n",
      "[ Epoch 214, Val ] | Loss:1.18525 Time:0.005230\n",
      "[ Epoch 215, Train ] | Loss:0.00221 Time:0.066711\n",
      "[ Epoch 215, Val ] | Loss:1.29686 Time:0.005300\n",
      "[ Epoch 216, Train ] | Loss:0.00556 Time:0.066381\n",
      "[ Epoch 216, Val ] | Loss:1.15286 Time:0.005411\n",
      "[ Epoch 217, Train ] | Loss:0.00374 Time:0.066257\n",
      "[ Epoch 217, Val ] | Loss:1.21159 Time:0.005231\n",
      "[ Epoch 218, Train ] | Loss:0.00207 Time:0.066393\n",
      "[ Epoch 218, Val ] | Loss:1.27887 Time:0.005308\n",
      "[ Epoch 219, Train ] | Loss:0.00422 Time:0.066077\n",
      "[ Epoch 219, Val ] | Loss:1.37477 Time:0.005303\n",
      "[ Epoch 220, Train ] | Loss:0.00277 Time:0.066625\n",
      "[ Epoch 220, Val ] | Loss:1.35678 Time:0.005238\n",
      "[ Epoch 221, Train ] | Loss:0.00253 Time:0.066106\n",
      "[ Epoch 221, Val ] | Loss:1.27619 Time:0.005300\n",
      "[ Epoch 222, Train ] | Loss:0.00405 Time:0.066160\n",
      "[ Epoch 222, Val ] | Loss:1.35528 Time:0.005303\n",
      "[ Epoch 223, Train ] | Loss:0.00787 Time:0.073294\n",
      "[ Epoch 223, Val ] | Loss:1.25548 Time:0.005161\n",
      "[ Epoch 224, Train ] | Loss:0.00591 Time:0.068894\n",
      "[ Epoch 224, Val ] | Loss:1.41829 Time:0.005296\n",
      "[ Epoch 225, Train ] | Loss:0.01042 Time:0.068988\n",
      "[ Epoch 225, Val ] | Loss:1.25241 Time:0.005301\n",
      "[ Epoch 226, Train ] | Loss:0.00973 Time:0.068993\n",
      "[ Epoch 226, Val ] | Loss:1.22650 Time:0.005240\n",
      "[ Epoch 227, Train ] | Loss:0.00567 Time:0.066874\n",
      "[ Epoch 227, Val ] | Loss:1.23728 Time:0.005298\n",
      "[ Epoch 228, Train ] | Loss:0.00751 Time:0.066030\n",
      "[ Epoch 228, Val ] | Loss:1.36153 Time:0.005294\n",
      "[ Epoch 229, Train ] | Loss:0.01952 Time:0.068308\n",
      "[ Epoch 229, Val ] | Loss:1.40673 Time:0.005230\n",
      "[ Epoch 230, Train ] | Loss:0.03894 Time:0.066189\n",
      "[ Epoch 230, Val ] | Loss:1.53827 Time:0.005299\n",
      "[ Epoch 231, Train ] | Loss:0.04551 Time:0.066363\n",
      "[ Epoch 231, Val ] | Loss:1.66723 Time:0.005307\n",
      "[ Epoch 232, Train ] | Loss:0.03258 Time:0.071694\n",
      "[ Epoch 232, Val ] | Loss:1.50988 Time:0.005224\n",
      "[ Epoch 233, Train ] | Loss:0.02290 Time:0.071197\n",
      "[ Epoch 233, Val ] | Loss:1.49751 Time:0.005301\n",
      "[ Epoch 234, Train ] | Loss:0.02303 Time:0.066370\n",
      "[ Epoch 234, Val ] | Loss:1.17431 Time:0.005297\n",
      "[ Epoch 235, Train ] | Loss:0.01506 Time:0.066627\n",
      "[ Epoch 235, Val ] | Loss:1.37022 Time:0.005232\n",
      "[ Epoch 236, Train ] | Loss:0.01225 Time:0.066335\n",
      "[ Epoch 236, Val ] | Loss:1.28649 Time:0.005295\n",
      "[ Epoch 237, Train ] | Loss:0.00783 Time:0.066681\n",
      "[ Epoch 237, Val ] | Loss:1.20613 Time:0.005289\n",
      "[ Epoch 238, Train ] | Loss:0.00540 Time:0.065822\n",
      "[ Epoch 238, Val ] | Loss:1.22043 Time:0.005216\n",
      "[ Epoch 239, Train ] | Loss:0.01091 Time:0.067376\n",
      "[ Epoch 239, Val ] | Loss:1.23229 Time:0.005311\n",
      "[ Epoch 240, Train ] | Loss:0.01060 Time:0.068530\n",
      "[ Epoch 240, Val ] | Loss:1.07196 Time:0.005294\n",
      "[ Epoch 241, Train ] | Loss:0.02184 Time:0.071609\n",
      "[ Epoch 241, Val ] | Loss:1.32537 Time:0.005240\n",
      "[ Epoch 242, Train ] | Loss:0.01381 Time:0.066321\n",
      "[ Epoch 242, Val ] | Loss:1.71365 Time:0.005281\n",
      "[ Epoch 243, Train ] | Loss:1.63735 Time:0.066916\n",
      "[ Epoch 243, Val ] | Loss:1.92366 Time:0.005407\n",
      "[ Epoch 244, Train ] | Loss:1.71640 Time:0.065787\n",
      "[ Epoch 244, Val ] | Loss:1.82027 Time:0.005232\n",
      "[ Epoch 245, Train ] | Loss:1.28798 Time:0.065995\n",
      "[ Epoch 245, Val ] | Loss:1.65450 Time:0.005406\n",
      "[ Epoch 246, Train ] | Loss:0.84025 Time:0.065795\n",
      "[ Epoch 246, Val ] | Loss:1.05699 Time:0.005298\n",
      "[ Epoch 247, Train ] | Loss:0.35627 Time:0.071015\n",
      "[ Epoch 247, Val ] | Loss:0.97482 Time:0.005404\n",
      "save model with val loss 0.975\n",
      "[ Epoch 248, Train ] | Loss:0.19841 Time:0.065645\n",
      "[ Epoch 248, Val ] | Loss:1.00996 Time:0.005195\n",
      "[ Epoch 249, Train ] | Loss:0.15351 Time:0.065773\n",
      "[ Epoch 249, Val ] | Loss:0.95525 Time:0.005296\n",
      "save model with val loss 0.955\n",
      "[ Epoch 250, Train ] | Loss:0.12235 Time:0.067748\n",
      "[ Epoch 250, Val ] | Loss:1.11503 Time:0.005169\n",
      "[ Epoch 251, Train ] | Loss:0.10360 Time:0.066184\n",
      "[ Epoch 251, Val ] | Loss:0.93187 Time:0.005284\n",
      "save model with val loss 0.932\n",
      "[ Epoch 252, Train ] | Loss:0.08056 Time:0.066447\n",
      "[ Epoch 252, Val ] | Loss:1.10019 Time:0.005219\n",
      "[ Epoch 253, Train ] | Loss:0.05187 Time:0.066381\n",
      "[ Epoch 253, Val ] | Loss:0.86782 Time:0.005232\n",
      "save model with val loss 0.868\n",
      "[ Epoch 254, Train ] | Loss:0.04289 Time:0.071172\n",
      "[ Epoch 254, Val ] | Loss:1.04689 Time:0.005219\n",
      "[ Epoch 255, Train ] | Loss:0.02822 Time:0.068457\n",
      "[ Epoch 255, Val ] | Loss:0.99136 Time:0.005300\n",
      "[ Epoch 256, Train ] | Loss:0.02121 Time:0.068444\n",
      "[ Epoch 256, Val ] | Loss:0.97346 Time:0.005226\n",
      "[ Epoch 257, Train ] | Loss:0.01727 Time:0.066040\n",
      "[ Epoch 257, Val ] | Loss:1.01329 Time:0.005313\n",
      "[ Epoch 258, Train ] | Loss:0.01620 Time:0.073330\n",
      "[ Epoch 258, Val ] | Loss:1.00956 Time:0.005284\n",
      "[ Epoch 259, Train ] | Loss:0.01491 Time:0.067358\n",
      "[ Epoch 259, Val ] | Loss:1.04379 Time:0.005240\n",
      "[ Epoch 260, Train ] | Loss:0.01576 Time:0.066091\n",
      "[ Epoch 260, Val ] | Loss:1.02952 Time:0.005306\n",
      "[ Epoch 261, Train ] | Loss:0.01650 Time:0.066068\n",
      "[ Epoch 261, Val ] | Loss:1.00626 Time:0.005294\n",
      "[ Epoch 262, Train ] | Loss:0.02060 Time:0.065466\n",
      "[ Epoch 262, Val ] | Loss:1.01765 Time:0.005228\n",
      "[ Epoch 263, Train ] | Loss:0.00895 Time:0.065874\n",
      "[ Epoch 263, Val ] | Loss:1.13937 Time:0.005290\n",
      "[ Epoch 264, Train ] | Loss:0.00616 Time:0.065939\n",
      "[ Epoch 264, Val ] | Loss:1.13586 Time:0.005295\n",
      "[ Epoch 265, Train ] | Loss:0.00733 Time:0.065814\n",
      "[ Epoch 265, Val ] | Loss:1.16334 Time:0.005228\n",
      "[ Epoch 266, Train ] | Loss:0.00461 Time:0.071153\n",
      "[ Epoch 266, Val ] | Loss:1.14162 Time:0.005297\n",
      "[ Epoch 267, Train ] | Loss:0.00499 Time:0.065542\n",
      "[ Epoch 267, Val ] | Loss:1.11101 Time:0.005298\n",
      "[ Epoch 268, Train ] | Loss:0.00682 Time:0.065585\n",
      "[ Epoch 268, Val ] | Loss:1.12604 Time:0.005239\n",
      "[ Epoch 269, Train ] | Loss:0.00763 Time:0.065678\n",
      "[ Epoch 269, Val ] | Loss:1.10506 Time:0.005304\n",
      "[ Epoch 270, Train ] | Loss:0.00744 Time:0.068542\n",
      "[ Epoch 270, Val ] | Loss:1.11166 Time:0.005405\n",
      "[ Epoch 271, Train ] | Loss:0.00902 Time:0.065956\n",
      "[ Epoch 271, Val ] | Loss:1.08942 Time:0.005227\n",
      "[ Epoch 272, Train ] | Loss:0.00544 Time:0.068427\n",
      "[ Epoch 272, Val ] | Loss:1.15708 Time:0.005291\n",
      "[ Epoch 273, Train ] | Loss:0.00544 Time:0.065992\n",
      "[ Epoch 273, Val ] | Loss:1.15179 Time:0.005295\n",
      "[ Epoch 274, Train ] | Loss:0.01620 Time:0.067619\n",
      "[ Epoch 274, Val ] | Loss:1.20527 Time:0.005237\n",
      "[ Epoch 275, Train ] | Loss:0.03710 Time:0.070497\n",
      "[ Epoch 275, Val ] | Loss:1.20011 Time:0.005301\n",
      "[ Epoch 276, Train ] | Loss:0.02355 Time:0.065435\n",
      "[ Epoch 276, Val ] | Loss:1.22161 Time:0.005343\n",
      "[ Epoch 277, Train ] | Loss:0.01693 Time:0.065943\n",
      "[ Epoch 277, Val ] | Loss:1.04337 Time:0.005239\n",
      "[ Epoch 278, Train ] | Loss:0.01475 Time:0.065640\n",
      "[ Epoch 278, Val ] | Loss:1.18062 Time:0.005417\n",
      "[ Epoch 279, Train ] | Loss:0.01116 Time:0.067945\n",
      "[ Epoch 279, Val ] | Loss:1.13779 Time:0.005287\n",
      "[ Epoch 280, Train ] | Loss:0.01092 Time:0.070002\n",
      "[ Epoch 280, Val ] | Loss:1.05080 Time:0.005232\n",
      "[ Epoch 281, Train ] | Loss:0.00548 Time:0.065752\n",
      "[ Epoch 281, Val ] | Loss:0.90488 Time:0.005287\n",
      "[ Epoch 282, Train ] | Loss:0.00592 Time:0.065199\n",
      "[ Epoch 282, Val ] | Loss:0.97798 Time:0.005293\n",
      "[ Epoch 283, Train ] | Loss:0.00443 Time:0.065427\n",
      "[ Epoch 283, Val ] | Loss:1.11056 Time:0.005232\n",
      "[ Epoch 284, Train ] | Loss:0.00558 Time:0.065743\n",
      "[ Epoch 284, Val ] | Loss:1.00546 Time:0.005285\n",
      "[ Epoch 285, Train ] | Loss:0.00616 Time:0.068044\n",
      "[ Epoch 285, Val ] | Loss:0.99396 Time:0.005407\n",
      "[ Epoch 286, Train ] | Loss:0.00353 Time:0.068055\n",
      "[ Epoch 286, Val ] | Loss:0.92972 Time:0.005410\n",
      "[ Epoch 287, Train ] | Loss:0.00448 Time:0.065073\n",
      "[ Epoch 287, Val ] | Loss:0.97718 Time:0.005294\n",
      "[ Epoch 288, Train ] | Loss:0.00352 Time:0.070238\n",
      "[ Epoch 288, Val ] | Loss:1.03948 Time:0.005288\n",
      "[ Epoch 289, Train ] | Loss:0.00638 Time:0.067838\n",
      "[ Epoch 289, Val ] | Loss:1.15443 Time:0.005235\n",
      "[ Epoch 290, Train ] | Loss:0.00492 Time:0.068120\n",
      "[ Epoch 290, Val ] | Loss:1.25413 Time:0.005289\n",
      "[ Epoch 291, Train ] | Loss:0.00495 Time:0.068042\n",
      "[ Epoch 291, Val ] | Loss:1.19093 Time:0.005281\n",
      "[ Epoch 292, Train ] | Loss:0.00511 Time:0.068148\n",
      "[ Epoch 292, Val ] | Loss:1.12364 Time:0.005416\n",
      "[ Epoch 293, Train ] | Loss:0.00651 Time:0.067834\n",
      "[ Epoch 293, Val ] | Loss:1.10132 Time:0.005417\n",
      "[ Epoch 294, Train ] | Loss:0.00627 Time:0.065410\n",
      "[ Epoch 294, Val ] | Loss:1.12719 Time:0.005406\n",
      "[ Epoch 295, Train ] | Loss:0.00485 Time:0.070424\n",
      "[ Epoch 295, Val ] | Loss:1.11412 Time:0.005222\n",
      "[ Epoch 296, Train ] | Loss:0.00692 Time:0.064655\n",
      "[ Epoch 296, Val ] | Loss:1.22057 Time:0.005288\n",
      "[ Epoch 297, Train ] | Loss:0.01042 Time:0.065103\n",
      "[ Epoch 297, Val ] | Loss:1.27442 Time:0.005290\n",
      "[ Epoch 298, Train ] | Loss:0.00727 Time:0.065364\n",
      "[ Epoch 298, Val ] | Loss:1.18062 Time:0.005213\n",
      "[ Epoch 299, Train ] | Loss:0.00519 Time:0.070386\n",
      "[ Epoch 299, Val ] | Loss:1.05989 Time:0.005307\n",
      "[ Epoch 300, Train ] | Loss:0.00457 Time:0.070404\n",
      "[ Epoch 300, Val ] | Loss:1.10962 Time:0.005300\n",
      "Finish training!\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "b147d7d5-1355-469a-9a9a-5c8895bbe60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin testing\n",
      "Test loss: 1.0726249516010284\n",
      "——————Test——————\n",
      "           class precision  recall f1-score support\n",
      "0              0    1.0000  0.8000   0.8889       5\n",
      "1              1    0.5000  0.4000   0.4444       5\n",
      "2              2    0.8000  0.8000   0.8000       5\n",
      "3              3    0.5000  0.6000   0.5455       5\n",
      "4              4    0.6000  0.6000   0.6000       5\n",
      "5              5    1.0000  1.0000   1.0000       5\n",
      "6              6    0.7500  0.6000   0.6667       5\n",
      "7              7    0.8000  0.8000   0.8000       5\n",
      "8              8    0.8333  1.0000   0.9091       5\n",
      "9              9    0.8333  1.0000   0.9091       5\n",
      "10            10    1.0000  1.0000   1.0000       5\n",
      "11            11    1.0000  0.6000   0.7500       5\n",
      "12            12    0.6667  0.8000   0.7273       5\n",
      "13            13    1.0000  0.8000   0.8889       5\n",
      "14            14    0.6667  0.4000   0.5000       5\n",
      "15            15    0.8333  1.0000   0.9091       5\n",
      "16            16    0.8000  0.8000   0.8000       5\n",
      "17            17    1.0000  0.6000   0.7500       5\n",
      "18            18    0.5000  0.6000   0.5455       5\n",
      "19            19    0.7143  1.0000   0.8333       5\n",
      "20            20    0.8000  0.8000   0.8000       5\n",
      "21            21    1.0000  0.8000   0.8889       5\n",
      "22            22    1.0000  0.6000   0.7500       5\n",
      "23            23    0.5714  0.8000   0.6667       5\n",
      "24            24    0.6250  1.0000   0.7692       5\n",
      "25            25    1.0000  1.0000   1.0000       5\n",
      "26            26    1.0000  0.8000   0.8889       5\n",
      "27            27    1.0000  1.0000   1.0000       5\n",
      "28            28    0.7500  0.6000   0.6667       5\n",
      "29            29    0.6667  0.8000   0.7273       5\n",
      "30            30    0.7143  1.0000   0.8333       5\n",
      "31            31    0.7500  0.6000   0.6667       5\n",
      "32            32    0.8000  0.8000   0.8000       5\n",
      "33            33    0.7143  1.0000   0.8333       5\n",
      "34            34    0.8000  0.8000   0.8000       5\n",
      "35            35    1.0000  1.0000   1.0000       5\n",
      "36            36    0.5714  0.8000   0.6667       5\n",
      "37            37    0.4286  0.6000   0.5000       5\n",
      "38            38    1.0000  0.8000   0.8889       5\n",
      "39            39    1.0000  1.0000   1.0000       5\n",
      "40            40    0.6667  0.4000   0.5000       5\n",
      "41            41    0.7500  0.6000   0.6667       5\n",
      "42            42    0.8000  0.8000   0.8000       5\n",
      "43            43    0.8333  1.0000   0.9091       5\n",
      "44            44    0.4000  0.4000   0.4000       5\n",
      "45            45    0.4000  0.4000   0.4000       5\n",
      "46            46    0.3750  0.6000   0.4615       5\n",
      "47            47    1.0000  0.6000   0.7500       5\n",
      "48            48    1.0000  1.0000   1.0000       5\n",
      "49            49    0.0000  0.0000   0.0000       5\n",
      "50      accuracy                     0.7520     250\n",
      "51     macro avg    0.7643  0.7520   0.7460     250\n",
      "52  weighted avg    0.7643  0.7520   0.7460     250\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570757d9-43bd-4cf0-8ae8-d74d973704c0",
   "metadata": {},
   "source": [
    "## 7.3 Different Number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b701bae-a079-4024-a0f6-bf83490aa3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "num_classes = 60\n",
    "num_samples_train = 15\n",
    "num_samples_test = 5\n",
    "seed = 2022\n",
    "data_folder = './omniglot_resized'\n",
    "train_image, train_label, test_image, test_label = LoadData(num_classes, num_samples_train, \n",
    "                                                            num_samples_test, seed, data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d11a356a-f2e5-420d-8da9-a57f153e8384",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(train_image, train_label, \n",
    "                                                  test_size=0.1, random_state=2022)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_data = ImgDataset(x_train.reshape(-1,1,28,28), y_train)\n",
    "val_data = ImgDataset(x_val.reshape(-1,1,28,28), y_val)\n",
    "test_data = ImgDataset(test_image.reshape(-1,1,28,28), test_label)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c231cd2e-1434-454f-8503-425696bd3d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_classifier_60(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_classifier_60, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, 1, 1), # input channel, ouput channel, filter size, stride, padding\n",
    "            nn.BatchNorm2d(64), # num_features\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0), # kernel size, stride, padding\n",
    "        \n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "            \n",
    "            nn.Conv2d(256, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "        \n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 60)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1) # resize, first axis is batch_num\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2fc312e-110f-4721-8792-31988e23e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cnn_60 = CNN_classifier_60()\n",
    "epochs = 300\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4\n",
    "optimizer = Adam(new_cnn_60.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_dir = './output'\n",
    "model_name = 'new_cnn_60'\n",
    "random_seed = 2022\n",
    "trainer = Trainer(model=new_cnn_60, \n",
    "                  epochs=epochs, \n",
    "                  train_dataloader=train_loader, \n",
    "                  val_dataloader=val_loader, \n",
    "                  test_dataloader=test_loader,\n",
    "                  criterion=criterion, \n",
    "                  optimizer=optimizer, \n",
    "                  lr=learning_rate, \n",
    "                  device=device, \n",
    "                  model_dir=model_dir, \n",
    "                  model_name=model_name, \n",
    "                  random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2d46629-39b8-45d5-8d78-a535ef255755",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameters: 1764284, trainable parameters: 1764284\n",
      "\n",
      "        initial learning rate: 0.001, weight_decay: 0.0001,\n",
      "        total epochs: 300\n",
      "        \n",
      "Begin training, total epochs: 300\n",
      "[ Epoch 1, Train ] | Loss:4.04100 Time:1.533246\n",
      "[ Epoch 1, Val ] | Loss:4.10976 Time:0.020238\n",
      "save model with val loss 4.110\n",
      "[ Epoch 2, Train ] | Loss:3.64665 Time:0.101382\n",
      "[ Epoch 2, Val ] | Loss:4.38843 Time:0.005455\n",
      "[ Epoch 3, Train ] | Loss:3.03283 Time:0.097478\n",
      "[ Epoch 3, Val ] | Loss:5.19533 Time:0.005459\n",
      "[ Epoch 4, Train ] | Loss:2.35483 Time:0.097763\n",
      "[ Epoch 4, Val ] | Loss:5.15242 Time:0.005556\n",
      "[ Epoch 5, Train ] | Loss:1.72903 Time:0.100007\n",
      "[ Epoch 5, Val ] | Loss:4.87990 Time:0.005477\n",
      "[ Epoch 6, Train ] | Loss:1.19634 Time:0.096860\n",
      "[ Epoch 6, Val ] | Loss:3.28981 Time:0.005565\n",
      "save model with val loss 3.290\n",
      "[ Epoch 7, Train ] | Loss:0.79984 Time:0.098181\n",
      "[ Epoch 7, Val ] | Loss:1.87954 Time:0.005645\n",
      "save model with val loss 1.880\n",
      "[ Epoch 8, Train ] | Loss:0.56159 Time:0.098029\n",
      "[ Epoch 8, Val ] | Loss:1.98634 Time:0.005629\n",
      "[ Epoch 9, Train ] | Loss:0.37380 Time:0.096867\n",
      "[ Epoch 9, Val ] | Loss:1.10335 Time:0.005476\n",
      "save model with val loss 1.103\n",
      "[ Epoch 10, Train ] | Loss:0.24952 Time:0.098084\n",
      "[ Epoch 10, Val ] | Loss:0.68849 Time:0.005471\n",
      "save model with val loss 0.688\n",
      "[ Epoch 11, Train ] | Loss:0.17445 Time:0.097592\n",
      "[ Epoch 11, Val ] | Loss:0.86650 Time:0.005441\n",
      "[ Epoch 12, Train ] | Loss:0.14312 Time:0.097384\n",
      "[ Epoch 12, Val ] | Loss:0.73816 Time:0.005553\n",
      "[ Epoch 13, Train ] | Loss:0.12983 Time:0.099532\n",
      "[ Epoch 13, Val ] | Loss:0.65159 Time:0.005495\n",
      "save model with val loss 0.652\n",
      "[ Epoch 14, Train ] | Loss:0.09878 Time:0.097459\n",
      "[ Epoch 14, Val ] | Loss:0.53566 Time:0.005470\n",
      "save model with val loss 0.536\n",
      "[ Epoch 15, Train ] | Loss:0.06875 Time:0.102162\n",
      "[ Epoch 15, Val ] | Loss:0.62500 Time:0.005415\n",
      "[ Epoch 16, Train ] | Loss:0.06052 Time:0.097031\n",
      "[ Epoch 16, Val ] | Loss:0.56565 Time:0.005547\n",
      "[ Epoch 17, Train ] | Loss:0.05863 Time:0.099898\n",
      "[ Epoch 17, Val ] | Loss:0.51916 Time:0.005495\n",
      "save model with val loss 0.519\n",
      "[ Epoch 18, Train ] | Loss:0.03965 Time:0.096748\n",
      "[ Epoch 18, Val ] | Loss:0.48349 Time:0.005460\n",
      "save model with val loss 0.483\n",
      "[ Epoch 19, Train ] | Loss:0.03769 Time:0.097345\n",
      "[ Epoch 19, Val ] | Loss:0.35086 Time:0.005429\n",
      "save model with val loss 0.351\n",
      "[ Epoch 20, Train ] | Loss:0.02635 Time:0.097539\n",
      "[ Epoch 20, Val ] | Loss:0.44863 Time:0.005469\n",
      "[ Epoch 21, Train ] | Loss:0.01558 Time:0.098721\n",
      "[ Epoch 21, Val ] | Loss:0.38427 Time:0.005472\n",
      "[ Epoch 22, Train ] | Loss:0.01329 Time:0.096872\n",
      "[ Epoch 22, Val ] | Loss:0.30614 Time:0.005578\n",
      "save model with val loss 0.306\n",
      "[ Epoch 23, Train ] | Loss:0.00963 Time:0.099009\n",
      "[ Epoch 23, Val ] | Loss:0.31299 Time:0.005439\n",
      "[ Epoch 24, Train ] | Loss:0.01131 Time:0.096932\n",
      "[ Epoch 24, Val ] | Loss:0.35263 Time:0.005559\n",
      "[ Epoch 25, Train ] | Loss:0.00740 Time:0.098430\n",
      "[ Epoch 25, Val ] | Loss:0.71930 Time:0.005498\n",
      "[ Epoch 26, Train ] | Loss:0.01291 Time:0.096266\n",
      "[ Epoch 26, Val ] | Loss:0.45833 Time:0.005557\n",
      "[ Epoch 27, Train ] | Loss:0.01445 Time:0.096806\n",
      "[ Epoch 27, Val ] | Loss:0.32986 Time:0.013457\n",
      "[ Epoch 28, Train ] | Loss:0.00878 Time:0.097498\n",
      "[ Epoch 28, Val ] | Loss:0.34332 Time:0.005558\n",
      "[ Epoch 29, Train ] | Loss:0.01084 Time:0.096522\n",
      "[ Epoch 29, Val ] | Loss:0.43091 Time:0.005509\n",
      "[ Epoch 30, Train ] | Loss:0.00945 Time:0.097461\n",
      "[ Epoch 30, Val ] | Loss:0.38143 Time:0.005559\n",
      "[ Epoch 31, Train ] | Loss:0.00858 Time:0.099363\n",
      "[ Epoch 31, Val ] | Loss:0.38160 Time:0.005506\n",
      "[ Epoch 32, Train ] | Loss:0.00648 Time:0.098696\n",
      "[ Epoch 32, Val ] | Loss:0.26653 Time:0.005566\n",
      "save model with val loss 0.267\n",
      "[ Epoch 33, Train ] | Loss:0.00465 Time:0.098945\n",
      "[ Epoch 33, Val ] | Loss:0.32601 Time:0.005636\n",
      "[ Epoch 34, Train ] | Loss:0.00424 Time:0.098938\n",
      "[ Epoch 34, Val ] | Loss:0.25393 Time:0.005547\n",
      "save model with val loss 0.254\n",
      "[ Epoch 35, Train ] | Loss:0.00346 Time:0.098739\n",
      "[ Epoch 35, Val ] | Loss:0.33264 Time:0.005429\n",
      "[ Epoch 36, Train ] | Loss:0.00294 Time:0.098967\n",
      "[ Epoch 36, Val ] | Loss:0.33385 Time:0.005547\n",
      "[ Epoch 37, Train ] | Loss:0.00394 Time:0.099574\n",
      "[ Epoch 37, Val ] | Loss:0.29528 Time:0.005501\n",
      "[ Epoch 38, Train ] | Loss:0.00244 Time:0.099316\n",
      "[ Epoch 38, Val ] | Loss:0.32902 Time:0.005566\n",
      "[ Epoch 39, Train ] | Loss:0.00286 Time:0.099287\n",
      "[ Epoch 39, Val ] | Loss:0.27968 Time:0.005495\n",
      "[ Epoch 40, Train ] | Loss:0.00322 Time:0.098493\n",
      "[ Epoch 40, Val ] | Loss:0.30154 Time:0.005566\n",
      "[ Epoch 41, Train ] | Loss:0.00396 Time:0.099330\n",
      "[ Epoch 41, Val ] | Loss:0.32651 Time:0.005499\n",
      "[ Epoch 42, Train ] | Loss:0.00540 Time:0.098935\n",
      "[ Epoch 42, Val ] | Loss:0.47586 Time:0.005571\n",
      "[ Epoch 43, Train ] | Loss:0.01543 Time:0.099408\n",
      "[ Epoch 43, Val ] | Loss:0.70938 Time:0.005706\n",
      "[ Epoch 44, Train ] | Loss:0.02248 Time:0.098855\n",
      "[ Epoch 44, Val ] | Loss:1.08732 Time:0.005527\n",
      "[ Epoch 45, Train ] | Loss:0.02167 Time:0.098709\n",
      "[ Epoch 45, Val ] | Loss:0.46232 Time:0.005491\n",
      "[ Epoch 46, Train ] | Loss:0.02042 Time:0.098913\n",
      "[ Epoch 46, Val ] | Loss:0.51633 Time:0.005561\n",
      "[ Epoch 47, Train ] | Loss:0.01928 Time:0.099055\n",
      "[ Epoch 47, Val ] | Loss:0.51520 Time:0.005489\n",
      "[ Epoch 48, Train ] | Loss:0.01815 Time:0.098555\n",
      "[ Epoch 48, Val ] | Loss:0.46332 Time:0.005577\n",
      "[ Epoch 49, Train ] | Loss:0.02495 Time:0.098368\n",
      "[ Epoch 49, Val ] | Loss:0.59109 Time:0.005509\n",
      "[ Epoch 50, Train ] | Loss:0.02775 Time:0.098839\n",
      "[ Epoch 50, Val ] | Loss:0.80750 Time:0.005563\n",
      "[ Epoch 51, Train ] | Loss:0.04726 Time:0.098886\n",
      "[ Epoch 51, Val ] | Loss:0.51553 Time:0.005531\n",
      "[ Epoch 52, Train ] | Loss:0.06299 Time:0.098406\n",
      "[ Epoch 52, Val ] | Loss:1.87945 Time:0.005566\n",
      "[ Epoch 53, Train ] | Loss:0.09450 Time:0.098746\n",
      "[ Epoch 53, Val ] | Loss:1.22911 Time:0.005508\n",
      "[ Epoch 54, Train ] | Loss:0.08096 Time:0.099245\n",
      "[ Epoch 54, Val ] | Loss:0.75633 Time:0.005567\n",
      "[ Epoch 55, Train ] | Loss:0.07008 Time:0.098916\n",
      "[ Epoch 55, Val ] | Loss:1.15829 Time:0.005508\n",
      "[ Epoch 56, Train ] | Loss:0.06976 Time:0.099176\n",
      "[ Epoch 56, Val ] | Loss:1.06821 Time:0.005559\n",
      "[ Epoch 57, Train ] | Loss:0.04979 Time:0.098046\n",
      "[ Epoch 57, Val ] | Loss:0.87853 Time:0.005512\n",
      "[ Epoch 58, Train ] | Loss:0.03580 Time:0.098793\n",
      "[ Epoch 58, Val ] | Loss:1.12046 Time:0.005567\n",
      "[ Epoch 59, Train ] | Loss:0.01899 Time:0.098153\n",
      "[ Epoch 59, Val ] | Loss:0.70338 Time:0.005507\n",
      "[ Epoch 60, Train ] | Loss:0.02115 Time:0.098991\n",
      "[ Epoch 60, Val ] | Loss:0.52248 Time:0.005583\n",
      "[ Epoch 61, Train ] | Loss:0.02230 Time:0.098823\n",
      "[ Epoch 61, Val ] | Loss:0.50397 Time:0.005501\n",
      "[ Epoch 62, Train ] | Loss:0.01885 Time:0.098951\n",
      "[ Epoch 62, Val ] | Loss:0.81164 Time:0.005569\n",
      "[ Epoch 63, Train ] | Loss:0.01514 Time:0.098751\n",
      "[ Epoch 63, Val ] | Loss:0.56174 Time:0.005522\n",
      "[ Epoch 64, Train ] | Loss:0.01029 Time:0.098585\n",
      "[ Epoch 64, Val ] | Loss:0.39496 Time:0.005568\n",
      "[ Epoch 65, Train ] | Loss:0.00814 Time:0.098422\n",
      "[ Epoch 65, Val ] | Loss:0.34743 Time:0.005515\n",
      "[ Epoch 66, Train ] | Loss:0.00436 Time:0.098614\n",
      "[ Epoch 66, Val ] | Loss:0.35968 Time:0.005569\n",
      "[ Epoch 67, Train ] | Loss:0.00538 Time:0.098155\n",
      "[ Epoch 67, Val ] | Loss:0.39878 Time:0.005517\n",
      "[ Epoch 68, Train ] | Loss:0.00624 Time:0.099001\n",
      "[ Epoch 68, Val ] | Loss:0.40015 Time:0.005569\n",
      "[ Epoch 69, Train ] | Loss:0.00309 Time:0.098258\n",
      "[ Epoch 69, Val ] | Loss:0.44601 Time:0.005510\n",
      "[ Epoch 70, Train ] | Loss:0.00729 Time:0.099266\n",
      "[ Epoch 70, Val ] | Loss:0.49035 Time:0.005554\n",
      "[ Epoch 71, Train ] | Loss:0.00434 Time:0.100899\n",
      "[ Epoch 71, Val ] | Loss:0.44946 Time:0.005494\n",
      "[ Epoch 72, Train ] | Loss:0.00329 Time:0.098385\n",
      "[ Epoch 72, Val ] | Loss:0.40165 Time:0.005571\n",
      "[ Epoch 73, Train ] | Loss:0.00398 Time:0.098386\n",
      "[ Epoch 73, Val ] | Loss:0.37532 Time:0.005504\n",
      "[ Epoch 74, Train ] | Loss:0.00383 Time:0.098495\n",
      "[ Epoch 74, Val ] | Loss:0.29922 Time:0.006761\n",
      "[ Epoch 75, Train ] | Loss:0.00219 Time:0.097310\n",
      "[ Epoch 75, Val ] | Loss:0.35837 Time:0.005497\n",
      "[ Epoch 76, Train ] | Loss:0.00471 Time:0.097603\n",
      "[ Epoch 76, Val ] | Loss:0.33616 Time:0.005573\n",
      "[ Epoch 77, Train ] | Loss:0.00320 Time:0.100723\n",
      "[ Epoch 77, Val ] | Loss:0.45901 Time:0.005508\n",
      "[ Epoch 78, Train ] | Loss:0.00232 Time:0.098204\n",
      "[ Epoch 78, Val ] | Loss:0.39363 Time:0.005567\n",
      "[ Epoch 79, Train ] | Loss:0.00185 Time:0.098577\n",
      "[ Epoch 79, Val ] | Loss:0.38477 Time:0.005503\n",
      "[ Epoch 80, Train ] | Loss:0.00213 Time:0.098975\n",
      "[ Epoch 80, Val ] | Loss:0.38099 Time:0.005577\n",
      "[ Epoch 81, Train ] | Loss:0.00149 Time:0.098171\n",
      "[ Epoch 81, Val ] | Loss:0.35978 Time:0.005508\n",
      "[ Epoch 82, Train ] | Loss:0.00119 Time:0.099006\n",
      "[ Epoch 82, Val ] | Loss:0.33675 Time:0.005566\n",
      "[ Epoch 83, Train ] | Loss:0.00129 Time:0.098384\n",
      "[ Epoch 83, Val ] | Loss:0.32013 Time:0.005512\n",
      "[ Epoch 84, Train ] | Loss:0.00069 Time:0.098467\n",
      "[ Epoch 84, Val ] | Loss:0.33952 Time:0.005569\n",
      "[ Epoch 85, Train ] | Loss:0.00083 Time:0.098665\n",
      "[ Epoch 85, Val ] | Loss:0.34329 Time:0.005494\n",
      "[ Epoch 86, Train ] | Loss:0.00164 Time:0.098607\n",
      "[ Epoch 86, Val ] | Loss:0.37033 Time:0.005570\n",
      "[ Epoch 87, Train ] | Loss:0.00081 Time:0.098150\n",
      "[ Epoch 87, Val ] | Loss:0.40730 Time:0.005500\n",
      "[ Epoch 88, Train ] | Loss:0.00093 Time:0.098220\n",
      "[ Epoch 88, Val ] | Loss:0.38766 Time:0.005687\n",
      "[ Epoch 89, Train ] | Loss:0.00038 Time:0.098197\n",
      "[ Epoch 89, Val ] | Loss:0.36640 Time:0.005505\n",
      "[ Epoch 90, Train ] | Loss:0.00057 Time:0.098099\n",
      "[ Epoch 90, Val ] | Loss:0.36092 Time:0.005580\n",
      "[ Epoch 91, Train ] | Loss:0.00038 Time:0.098342\n",
      "[ Epoch 91, Val ] | Loss:0.35666 Time:0.005498\n",
      "[ Epoch 92, Train ] | Loss:0.00078 Time:0.098118\n",
      "[ Epoch 92, Val ] | Loss:0.40279 Time:0.005578\n",
      "[ Epoch 93, Train ] | Loss:0.00035 Time:0.098179\n",
      "[ Epoch 93, Val ] | Loss:0.42426 Time:0.005512\n",
      "[ Epoch 94, Train ] | Loss:0.00045 Time:0.097965\n",
      "[ Epoch 94, Val ] | Loss:0.40364 Time:0.005570\n",
      "[ Epoch 95, Train ] | Loss:0.00062 Time:0.098140\n",
      "[ Epoch 95, Val ] | Loss:0.36577 Time:0.005513\n",
      "[ Epoch 96, Train ] | Loss:0.00035 Time:0.098276\n",
      "[ Epoch 96, Val ] | Loss:0.32356 Time:0.005576\n",
      "[ Epoch 97, Train ] | Loss:0.00029 Time:0.097859\n",
      "[ Epoch 97, Val ] | Loss:0.31992 Time:0.005516\n",
      "[ Epoch 98, Train ] | Loss:0.00032 Time:0.097924\n",
      "[ Epoch 98, Val ] | Loss:0.31870 Time:0.005577\n",
      "[ Epoch 99, Train ] | Loss:0.00040 Time:0.098221\n",
      "[ Epoch 99, Val ] | Loss:0.32529 Time:0.005508\n",
      "[ Epoch 100, Train ] | Loss:0.00054 Time:0.098072\n",
      "[ Epoch 100, Val ] | Loss:0.37249 Time:0.005570\n",
      "[ Epoch 101, Train ] | Loss:0.00023 Time:0.098316\n",
      "[ Epoch 101, Val ] | Loss:0.40196 Time:0.005509\n",
      "[ Epoch 102, Train ] | Loss:0.00024 Time:0.098433\n",
      "[ Epoch 102, Val ] | Loss:0.40308 Time:0.005571\n",
      "[ Epoch 103, Train ] | Loss:0.00056 Time:0.097829\n",
      "[ Epoch 103, Val ] | Loss:0.35826 Time:0.005506\n",
      "[ Epoch 104, Train ] | Loss:0.00051 Time:0.098078\n",
      "[ Epoch 104, Val ] | Loss:0.31857 Time:0.005599\n",
      "[ Epoch 105, Train ] | Loss:0.00027 Time:0.098215\n",
      "[ Epoch 105, Val ] | Loss:0.31647 Time:0.005506\n",
      "[ Epoch 106, Train ] | Loss:0.00044 Time:0.098118\n",
      "[ Epoch 106, Val ] | Loss:0.31475 Time:0.005575\n",
      "[ Epoch 107, Train ] | Loss:0.00039 Time:0.098433\n",
      "[ Epoch 107, Val ] | Loss:0.30739 Time:0.005509\n",
      "[ Epoch 108, Train ] | Loss:0.00030 Time:0.097451\n",
      "[ Epoch 108, Val ] | Loss:0.30600 Time:0.005571\n",
      "[ Epoch 109, Train ] | Loss:0.00025 Time:0.100964\n",
      "[ Epoch 109, Val ] | Loss:0.30671 Time:0.005496\n",
      "[ Epoch 110, Train ] | Loss:0.00047 Time:0.098100\n",
      "[ Epoch 110, Val ] | Loss:0.29657 Time:0.005574\n",
      "[ Epoch 111, Train ] | Loss:0.00038 Time:0.100055\n",
      "[ Epoch 111, Val ] | Loss:0.30235 Time:0.005506\n",
      "[ Epoch 112, Train ] | Loss:0.00023 Time:0.098042\n",
      "[ Epoch 112, Val ] | Loss:0.30451 Time:0.005577\n",
      "[ Epoch 113, Train ] | Loss:0.00024 Time:0.098311\n",
      "[ Epoch 113, Val ] | Loss:0.30470 Time:0.005523\n",
      "[ Epoch 114, Train ] | Loss:0.00021 Time:0.101441\n",
      "[ Epoch 114, Val ] | Loss:0.30608 Time:0.005592\n",
      "[ Epoch 115, Train ] | Loss:0.00029 Time:0.097653\n",
      "[ Epoch 115, Val ] | Loss:0.30896 Time:0.005514\n",
      "[ Epoch 116, Train ] | Loss:0.00024 Time:0.097491\n",
      "[ Epoch 116, Val ] | Loss:0.30662 Time:0.005579\n",
      "[ Epoch 117, Train ] | Loss:0.00029 Time:0.097883\n",
      "[ Epoch 117, Val ] | Loss:0.29480 Time:0.005508\n",
      "[ Epoch 118, Train ] | Loss:0.00069 Time:0.097542\n",
      "[ Epoch 118, Val ] | Loss:0.30931 Time:0.005583\n",
      "[ Epoch 119, Train ] | Loss:0.00028 Time:0.098472\n",
      "[ Epoch 119, Val ] | Loss:0.32215 Time:0.005502\n",
      "[ Epoch 120, Train ] | Loss:0.00037 Time:0.098038\n",
      "[ Epoch 120, Val ] | Loss:0.32266 Time:0.005568\n",
      "[ Epoch 121, Train ] | Loss:0.00022 Time:0.097526\n",
      "[ Epoch 121, Val ] | Loss:0.33311 Time:0.005508\n",
      "[ Epoch 122, Train ] | Loss:0.00030 Time:0.097921\n",
      "[ Epoch 122, Val ] | Loss:0.34288 Time:0.005572\n",
      "[ Epoch 123, Train ] | Loss:0.00027 Time:0.097889\n",
      "[ Epoch 123, Val ] | Loss:0.33877 Time:0.005501\n",
      "[ Epoch 124, Train ] | Loss:0.00019 Time:0.097727\n",
      "[ Epoch 124, Val ] | Loss:0.33505 Time:0.005562\n",
      "[ Epoch 125, Train ] | Loss:0.00020 Time:0.097728\n",
      "[ Epoch 125, Val ] | Loss:0.33355 Time:0.005513\n",
      "[ Epoch 126, Train ] | Loss:0.00023 Time:0.098085\n",
      "[ Epoch 126, Val ] | Loss:0.33367 Time:0.005584\n",
      "[ Epoch 127, Train ] | Loss:0.00020 Time:0.097618\n",
      "[ Epoch 127, Val ] | Loss:0.33459 Time:0.005522\n",
      "[ Epoch 128, Train ] | Loss:0.00022 Time:0.097344\n",
      "[ Epoch 128, Val ] | Loss:0.32882 Time:0.005573\n",
      "[ Epoch 129, Train ] | Loss:0.00018 Time:0.097435\n",
      "[ Epoch 129, Val ] | Loss:0.31384 Time:0.005515\n",
      "[ Epoch 130, Train ] | Loss:0.00034 Time:0.097596\n",
      "[ Epoch 130, Val ] | Loss:0.30875 Time:0.005580\n",
      "[ Epoch 131, Train ] | Loss:0.00020 Time:0.098002\n",
      "[ Epoch 131, Val ] | Loss:0.31220 Time:0.005511\n",
      "[ Epoch 132, Train ] | Loss:0.00023 Time:0.098270\n",
      "[ Epoch 132, Val ] | Loss:0.31496 Time:0.005548\n",
      "[ Epoch 133, Train ] | Loss:0.00028 Time:0.097582\n",
      "[ Epoch 133, Val ] | Loss:0.30566 Time:0.005498\n",
      "[ Epoch 134, Train ] | Loss:0.00033 Time:0.097858\n",
      "[ Epoch 134, Val ] | Loss:0.29596 Time:0.005690\n",
      "[ Epoch 135, Train ] | Loss:0.00024 Time:0.096902\n",
      "[ Epoch 135, Val ] | Loss:0.29686 Time:0.005510\n",
      "[ Epoch 136, Train ] | Loss:0.00020 Time:0.097357\n",
      "[ Epoch 136, Val ] | Loss:0.29122 Time:0.005580\n",
      "[ Epoch 137, Train ] | Loss:0.00027 Time:0.097492\n",
      "[ Epoch 137, Val ] | Loss:0.29344 Time:0.005509\n",
      "[ Epoch 138, Train ] | Loss:0.00018 Time:0.097594\n",
      "[ Epoch 138, Val ] | Loss:0.28330 Time:0.005572\n",
      "[ Epoch 139, Train ] | Loss:0.00016 Time:0.097397\n",
      "[ Epoch 139, Val ] | Loss:0.28114 Time:0.005503\n",
      "[ Epoch 140, Train ] | Loss:0.00017 Time:0.096887\n",
      "[ Epoch 140, Val ] | Loss:0.28154 Time:0.005578\n",
      "[ Epoch 141, Train ] | Loss:0.00022 Time:0.099257\n",
      "[ Epoch 141, Val ] | Loss:0.27878 Time:0.005527\n",
      "[ Epoch 142, Train ] | Loss:0.00022 Time:0.097717\n",
      "[ Epoch 142, Val ] | Loss:0.28509 Time:0.005679\n",
      "[ Epoch 143, Train ] | Loss:0.00021 Time:0.097861\n",
      "[ Epoch 143, Val ] | Loss:0.28722 Time:0.005501\n",
      "[ Epoch 144, Train ] | Loss:0.00020 Time:0.098037\n",
      "[ Epoch 144, Val ] | Loss:0.28972 Time:0.005674\n",
      "[ Epoch 145, Train ] | Loss:0.00015 Time:0.097017\n",
      "[ Epoch 145, Val ] | Loss:0.29237 Time:0.005511\n",
      "[ Epoch 146, Train ] | Loss:0.00028 Time:0.097601\n",
      "[ Epoch 146, Val ] | Loss:0.32027 Time:0.005524\n",
      "[ Epoch 147, Train ] | Loss:0.00030 Time:0.097751\n",
      "[ Epoch 147, Val ] | Loss:0.32099 Time:0.005493\n",
      "[ Epoch 148, Train ] | Loss:0.00020 Time:0.097272\n",
      "[ Epoch 148, Val ] | Loss:0.31815 Time:0.005563\n",
      "[ Epoch 149, Train ] | Loss:0.00029 Time:0.097436\n",
      "[ Epoch 149, Val ] | Loss:0.31868 Time:0.005498\n",
      "[ Epoch 150, Train ] | Loss:0.00019 Time:0.097490\n",
      "[ Epoch 150, Val ] | Loss:0.32016 Time:0.005570\n",
      "[ Epoch 151, Train ] | Loss:0.00013 Time:0.096942\n",
      "[ Epoch 151, Val ] | Loss:0.32011 Time:0.005518\n",
      "[ Epoch 152, Train ] | Loss:0.00027 Time:0.096992\n",
      "[ Epoch 152, Val ] | Loss:0.31709 Time:0.005573\n",
      "[ Epoch 153, Train ] | Loss:0.00024 Time:0.097538\n",
      "[ Epoch 153, Val ] | Loss:0.31633 Time:0.005503\n",
      "[ Epoch 154, Train ] | Loss:0.00016 Time:0.097213\n",
      "[ Epoch 154, Val ] | Loss:0.31457 Time:0.005579\n",
      "[ Epoch 155, Train ] | Loss:0.00013 Time:0.097609\n",
      "[ Epoch 155, Val ] | Loss:0.31391 Time:0.005512\n",
      "[ Epoch 156, Train ] | Loss:0.00020 Time:0.097251\n",
      "[ Epoch 156, Val ] | Loss:0.31814 Time:0.005566\n",
      "[ Epoch 157, Train ] | Loss:0.00023 Time:0.096790\n",
      "[ Epoch 157, Val ] | Loss:0.32283 Time:0.005524\n",
      "[ Epoch 158, Train ] | Loss:0.00030 Time:0.099306\n",
      "[ Epoch 158, Val ] | Loss:0.32608 Time:0.005581\n",
      "[ Epoch 159, Train ] | Loss:0.00017 Time:0.096979\n",
      "[ Epoch 159, Val ] | Loss:0.33075 Time:0.005557\n",
      "[ Epoch 160, Train ] | Loss:0.00019 Time:0.097205\n",
      "[ Epoch 160, Val ] | Loss:0.32784 Time:0.005568\n",
      "[ Epoch 161, Train ] | Loss:0.00021 Time:0.097204\n",
      "[ Epoch 161, Val ] | Loss:0.31989 Time:0.005517\n",
      "[ Epoch 162, Train ] | Loss:0.00021 Time:0.097415\n",
      "[ Epoch 162, Val ] | Loss:0.30916 Time:0.005562\n",
      "[ Epoch 163, Train ] | Loss:0.00019 Time:0.097068\n",
      "[ Epoch 163, Val ] | Loss:0.30575 Time:0.005518\n",
      "[ Epoch 164, Train ] | Loss:0.00023 Time:0.096802\n",
      "[ Epoch 164, Val ] | Loss:0.30565 Time:0.005574\n",
      "[ Epoch 165, Train ] | Loss:0.00017 Time:0.096558\n",
      "[ Epoch 165, Val ] | Loss:0.31421 Time:0.005510\n",
      "[ Epoch 166, Train ] | Loss:0.00032 Time:0.097949\n",
      "[ Epoch 166, Val ] | Loss:0.31760 Time:0.005557\n",
      "[ Epoch 167, Train ] | Loss:0.00041 Time:0.097246\n",
      "[ Epoch 167, Val ] | Loss:0.32031 Time:0.005547\n",
      "[ Epoch 168, Train ] | Loss:0.00035 Time:0.096826\n",
      "[ Epoch 168, Val ] | Loss:0.35452 Time:0.005585\n",
      "[ Epoch 169, Train ] | Loss:0.00021 Time:0.097145\n",
      "[ Epoch 169, Val ] | Loss:0.36433 Time:0.005508\n",
      "[ Epoch 170, Train ] | Loss:0.00018 Time:0.097164\n",
      "[ Epoch 170, Val ] | Loss:0.36076 Time:0.005573\n",
      "[ Epoch 171, Train ] | Loss:0.00035 Time:0.097258\n",
      "[ Epoch 171, Val ] | Loss:0.34570 Time:0.005509\n",
      "[ Epoch 172, Train ] | Loss:0.00044 Time:0.096973\n",
      "[ Epoch 172, Val ] | Loss:0.32434 Time:0.005573\n",
      "[ Epoch 173, Train ] | Loss:0.00017 Time:0.096806\n",
      "[ Epoch 173, Val ] | Loss:0.31861 Time:0.005505\n",
      "[ Epoch 174, Train ] | Loss:0.00023 Time:0.096806\n",
      "[ Epoch 174, Val ] | Loss:0.31336 Time:0.005565\n",
      "[ Epoch 175, Train ] | Loss:0.00024 Time:0.097012\n",
      "[ Epoch 175, Val ] | Loss:0.30511 Time:0.005509\n",
      "[ Epoch 176, Train ] | Loss:0.00023 Time:0.097249\n",
      "[ Epoch 176, Val ] | Loss:0.29788 Time:0.005564\n",
      "[ Epoch 177, Train ] | Loss:0.00021 Time:0.097433\n",
      "[ Epoch 177, Val ] | Loss:0.29377 Time:0.005678\n",
      "[ Epoch 178, Train ] | Loss:0.00029 Time:0.096377\n",
      "[ Epoch 178, Val ] | Loss:0.29263 Time:0.005583\n",
      "[ Epoch 179, Train ] | Loss:0.00018 Time:0.099245\n",
      "[ Epoch 179, Val ] | Loss:0.29525 Time:0.005508\n",
      "[ Epoch 180, Train ] | Loss:0.00041 Time:0.096979\n",
      "[ Epoch 180, Val ] | Loss:0.28050 Time:0.005578\n",
      "[ Epoch 181, Train ] | Loss:0.00031 Time:0.096766\n",
      "[ Epoch 181, Val ] | Loss:0.28231 Time:0.005503\n",
      "[ Epoch 182, Train ] | Loss:0.00020 Time:0.096660\n",
      "[ Epoch 182, Val ] | Loss:0.28103 Time:0.005579\n",
      "[ Epoch 183, Train ] | Loss:0.00024 Time:0.099266\n",
      "[ Epoch 183, Val ] | Loss:0.27485 Time:0.005508\n",
      "[ Epoch 184, Train ] | Loss:0.00034 Time:0.099168\n",
      "[ Epoch 184, Val ] | Loss:0.26964 Time:0.005572\n",
      "[ Epoch 185, Train ] | Loss:0.00024 Time:0.099063\n",
      "[ Epoch 185, Val ] | Loss:0.26425 Time:0.005496\n",
      "[ Epoch 186, Train ] | Loss:0.00018 Time:0.099376\n",
      "[ Epoch 186, Val ] | Loss:0.26474 Time:0.005570\n",
      "[ Epoch 187, Train ] | Loss:0.00026 Time:0.099536\n",
      "[ Epoch 187, Val ] | Loss:0.26338 Time:0.005507\n",
      "[ Epoch 188, Train ] | Loss:0.00026 Time:0.099005\n",
      "[ Epoch 188, Val ] | Loss:0.26860 Time:0.005576\n",
      "[ Epoch 189, Train ] | Loss:0.00018 Time:0.099189\n",
      "[ Epoch 189, Val ] | Loss:0.27635 Time:0.005513\n",
      "[ Epoch 190, Train ] | Loss:0.00020 Time:0.099099\n",
      "[ Epoch 190, Val ] | Loss:0.26716 Time:0.005570\n",
      "[ Epoch 191, Train ] | Loss:0.00020 Time:0.099127\n",
      "[ Epoch 191, Val ] | Loss:0.26620 Time:0.005522\n",
      "[ Epoch 192, Train ] | Loss:0.00016 Time:0.097002\n",
      "[ Epoch 192, Val ] | Loss:0.26918 Time:0.005585\n",
      "[ Epoch 193, Train ] | Loss:0.00018 Time:0.098950\n",
      "[ Epoch 193, Val ] | Loss:0.26634 Time:0.005506\n",
      "[ Epoch 194, Train ] | Loss:0.00028 Time:0.098449\n",
      "[ Epoch 194, Val ] | Loss:0.26542 Time:0.005579\n",
      "[ Epoch 195, Train ] | Loss:0.00023 Time:0.098801\n",
      "[ Epoch 195, Val ] | Loss:0.27064 Time:0.005676\n",
      "[ Epoch 196, Train ] | Loss:0.00018 Time:0.098965\n",
      "[ Epoch 196, Val ] | Loss:0.27126 Time:0.005583\n",
      "[ Epoch 197, Train ] | Loss:0.00017 Time:0.098701\n",
      "[ Epoch 197, Val ] | Loss:0.28214 Time:0.005511\n",
      "[ Epoch 198, Train ] | Loss:0.00023 Time:0.098208\n",
      "[ Epoch 198, Val ] | Loss:0.29368 Time:0.005576\n",
      "[ Epoch 199, Train ] | Loss:0.00020 Time:0.100482\n",
      "[ Epoch 199, Val ] | Loss:0.31264 Time:0.005507\n",
      "[ Epoch 200, Train ] | Loss:0.00015 Time:0.099013\n",
      "[ Epoch 200, Val ] | Loss:0.32640 Time:0.005590\n",
      "[ Epoch 201, Train ] | Loss:0.00023 Time:0.103172\n",
      "[ Epoch 201, Val ] | Loss:0.32280 Time:0.005517\n",
      "[ Epoch 202, Train ] | Loss:0.00027 Time:0.099276\n",
      "[ Epoch 202, Val ] | Loss:0.32290 Time:0.005580\n",
      "[ Epoch 203, Train ] | Loss:0.00020 Time:0.098538\n",
      "[ Epoch 203, Val ] | Loss:0.31185 Time:0.005486\n",
      "[ Epoch 204, Train ] | Loss:0.00013 Time:0.099086\n",
      "[ Epoch 204, Val ] | Loss:0.29924 Time:0.005584\n",
      "[ Epoch 205, Train ] | Loss:0.00020 Time:0.102493\n",
      "[ Epoch 205, Val ] | Loss:0.30092 Time:0.005509\n",
      "[ Epoch 206, Train ] | Loss:0.00019 Time:0.098255\n",
      "[ Epoch 206, Val ] | Loss:0.28848 Time:0.005567\n",
      "[ Epoch 207, Train ] | Loss:0.00022 Time:0.098897\n",
      "[ Epoch 207, Val ] | Loss:0.28226 Time:0.005504\n",
      "[ Epoch 208, Train ] | Loss:0.00018 Time:0.098629\n",
      "[ Epoch 208, Val ] | Loss:0.29032 Time:0.005569\n",
      "[ Epoch 209, Train ] | Loss:0.00019 Time:0.098650\n",
      "[ Epoch 209, Val ] | Loss:0.29800 Time:0.005508\n",
      "[ Epoch 210, Train ] | Loss:0.00019 Time:0.098440\n",
      "[ Epoch 210, Val ] | Loss:0.29762 Time:0.005582\n",
      "[ Epoch 211, Train ] | Loss:0.00028 Time:0.098182\n",
      "[ Epoch 211, Val ] | Loss:0.29020 Time:0.005506\n",
      "[ Epoch 212, Train ] | Loss:0.00019 Time:0.098805\n",
      "[ Epoch 212, Val ] | Loss:0.30501 Time:0.005584\n",
      "[ Epoch 213, Train ] | Loss:0.00018 Time:0.098384\n",
      "[ Epoch 213, Val ] | Loss:0.31120 Time:0.005511\n",
      "[ Epoch 214, Train ] | Loss:0.00021 Time:0.098661\n",
      "[ Epoch 214, Val ] | Loss:0.30487 Time:0.005578\n",
      "[ Epoch 215, Train ] | Loss:0.00029 Time:0.098833\n",
      "[ Epoch 215, Val ] | Loss:0.30986 Time:0.005504\n",
      "[ Epoch 216, Train ] | Loss:0.00032 Time:0.098336\n",
      "[ Epoch 216, Val ] | Loss:0.27647 Time:0.005584\n",
      "[ Epoch 217, Train ] | Loss:0.00024 Time:0.101766\n",
      "[ Epoch 217, Val ] | Loss:0.25508 Time:0.005513\n",
      "[ Epoch 218, Train ] | Loss:0.00028 Time:0.099170\n",
      "[ Epoch 218, Val ] | Loss:0.27169 Time:0.005575\n",
      "[ Epoch 219, Train ] | Loss:0.00020 Time:0.098740\n",
      "[ Epoch 219, Val ] | Loss:0.28630 Time:0.005508\n",
      "[ Epoch 220, Train ] | Loss:0.00035 Time:0.098574\n",
      "[ Epoch 220, Val ] | Loss:0.29892 Time:0.005572\n",
      "[ Epoch 221, Train ] | Loss:0.00024 Time:0.098588\n",
      "[ Epoch 221, Val ] | Loss:0.31606 Time:0.005496\n",
      "[ Epoch 222, Train ] | Loss:0.00027 Time:0.098904\n",
      "[ Epoch 222, Val ] | Loss:0.32677 Time:0.005571\n",
      "[ Epoch 223, Train ] | Loss:0.00027 Time:0.098007\n",
      "[ Epoch 223, Val ] | Loss:0.32692 Time:0.005486\n",
      "[ Epoch 224, Train ] | Loss:0.00032 Time:0.098345\n",
      "[ Epoch 224, Val ] | Loss:0.33237 Time:0.005574\n",
      "[ Epoch 225, Train ] | Loss:0.00022 Time:0.098685\n",
      "[ Epoch 225, Val ] | Loss:0.33506 Time:0.005508\n",
      "[ Epoch 226, Train ] | Loss:0.00027 Time:0.098652\n",
      "[ Epoch 226, Val ] | Loss:0.29251 Time:0.005562\n",
      "[ Epoch 227, Train ] | Loss:0.00047 Time:0.098006\n",
      "[ Epoch 227, Val ] | Loss:0.28340 Time:0.005521\n",
      "[ Epoch 228, Train ] | Loss:0.00024 Time:0.097774\n",
      "[ Epoch 228, Val ] | Loss:0.31106 Time:0.005574\n",
      "[ Epoch 229, Train ] | Loss:0.00028 Time:0.098360\n",
      "[ Epoch 229, Val ] | Loss:0.32374 Time:0.005511\n",
      "[ Epoch 230, Train ] | Loss:0.00033 Time:0.097544\n",
      "[ Epoch 230, Val ] | Loss:0.32952 Time:0.005569\n",
      "[ Epoch 231, Train ] | Loss:0.00038 Time:0.098160\n",
      "[ Epoch 231, Val ] | Loss:0.32727 Time:0.005506\n",
      "[ Epoch 232, Train ] | Loss:0.00022 Time:0.098563\n",
      "[ Epoch 232, Val ] | Loss:0.29318 Time:0.005685\n",
      "[ Epoch 233, Train ] | Loss:0.00023 Time:0.098508\n",
      "[ Epoch 233, Val ] | Loss:0.28631 Time:0.005508\n",
      "[ Epoch 234, Train ] | Loss:0.00020 Time:0.098144\n",
      "[ Epoch 234, Val ] | Loss:0.29045 Time:0.005570\n",
      "[ Epoch 235, Train ] | Loss:0.00051 Time:0.098517\n",
      "[ Epoch 235, Val ] | Loss:0.32946 Time:0.005520\n",
      "[ Epoch 236, Train ] | Loss:0.00087 Time:0.098732\n",
      "[ Epoch 236, Val ] | Loss:0.44729 Time:0.005566\n",
      "[ Epoch 237, Train ] | Loss:0.00084 Time:0.098140\n",
      "[ Epoch 237, Val ] | Loss:0.39729 Time:0.005516\n",
      "[ Epoch 238, Train ] | Loss:0.00103 Time:0.098403\n",
      "[ Epoch 238, Val ] | Loss:0.33763 Time:0.005582\n",
      "[ Epoch 239, Train ] | Loss:0.00169 Time:0.098124\n",
      "[ Epoch 239, Val ] | Loss:0.26633 Time:0.005499\n",
      "[ Epoch 240, Train ] | Loss:0.00237 Time:0.102495\n",
      "[ Epoch 240, Val ] | Loss:0.38467 Time:0.005570\n",
      "[ Epoch 241, Train ] | Loss:0.01590 Time:0.097935\n",
      "[ Epoch 241, Val ] | Loss:1.72631 Time:0.005517\n",
      "[ Epoch 242, Train ] | Loss:0.68164 Time:0.097762\n",
      "[ Epoch 242, Val ] | Loss:5.59287 Time:0.005576\n",
      "[ Epoch 243, Train ] | Loss:0.59848 Time:0.102562\n",
      "[ Epoch 243, Val ] | Loss:2.20520 Time:0.005518\n",
      "[ Epoch 244, Train ] | Loss:0.29713 Time:0.097964\n",
      "[ Epoch 244, Val ] | Loss:1.25306 Time:0.005687\n",
      "[ Epoch 245, Train ] | Loss:0.15304 Time:0.098488\n",
      "[ Epoch 245, Val ] | Loss:0.79862 Time:0.005521\n",
      "[ Epoch 246, Train ] | Loss:0.11790 Time:0.097905\n",
      "[ Epoch 246, Val ] | Loss:1.40638 Time:0.005570\n",
      "[ Epoch 247, Train ] | Loss:0.10921 Time:0.098739\n",
      "[ Epoch 247, Val ] | Loss:2.19944 Time:0.005506\n",
      "[ Epoch 248, Train ] | Loss:0.07475 Time:0.097662\n",
      "[ Epoch 248, Val ] | Loss:0.98600 Time:0.005573\n",
      "[ Epoch 249, Train ] | Loss:0.08254 Time:0.098129\n",
      "[ Epoch 249, Val ] | Loss:0.75312 Time:0.005526\n",
      "[ Epoch 250, Train ] | Loss:0.05558 Time:0.097815\n",
      "[ Epoch 250, Val ] | Loss:0.53903 Time:0.005572\n",
      "[ Epoch 251, Train ] | Loss:0.03162 Time:0.098161\n",
      "[ Epoch 251, Val ] | Loss:0.40469 Time:0.005490\n",
      "[ Epoch 252, Train ] | Loss:0.01579 Time:0.098690\n",
      "[ Epoch 252, Val ] | Loss:0.52132 Time:0.005573\n",
      "[ Epoch 253, Train ] | Loss:0.01420 Time:0.097612\n",
      "[ Epoch 253, Val ] | Loss:0.38423 Time:0.005507\n",
      "[ Epoch 254, Train ] | Loss:0.00582 Time:0.097796\n",
      "[ Epoch 254, Val ] | Loss:0.24750 Time:0.005561\n",
      "save model with val loss 0.247\n",
      "[ Epoch 255, Train ] | Loss:0.00809 Time:0.097504\n",
      "[ Epoch 255, Val ] | Loss:0.25976 Time:0.005428\n",
      "[ Epoch 256, Train ] | Loss:0.00331 Time:0.097290\n",
      "[ Epoch 256, Val ] | Loss:0.29948 Time:0.005558\n",
      "[ Epoch 257, Train ] | Loss:0.00220 Time:0.097725\n",
      "[ Epoch 257, Val ] | Loss:0.24960 Time:0.005506\n",
      "[ Epoch 258, Train ] | Loss:0.00195 Time:0.098303\n",
      "[ Epoch 258, Val ] | Loss:0.23153 Time:0.005573\n",
      "save model with val loss 0.232\n",
      "[ Epoch 259, Train ] | Loss:0.00155 Time:0.097769\n",
      "[ Epoch 259, Val ] | Loss:0.23171 Time:0.005420\n",
      "[ Epoch 260, Train ] | Loss:0.00090 Time:0.098286\n",
      "[ Epoch 260, Val ] | Loss:0.23662 Time:0.005559\n",
      "[ Epoch 261, Train ] | Loss:0.00110 Time:0.098290\n",
      "[ Epoch 261, Val ] | Loss:0.23617 Time:0.005501\n",
      "[ Epoch 262, Train ] | Loss:0.00076 Time:0.097569\n",
      "[ Epoch 262, Val ] | Loss:0.23629 Time:0.005566\n",
      "[ Epoch 263, Train ] | Loss:0.00122 Time:0.097693\n",
      "[ Epoch 263, Val ] | Loss:0.23173 Time:0.005496\n",
      "[ Epoch 264, Train ] | Loss:0.00045 Time:0.097304\n",
      "[ Epoch 264, Val ] | Loss:0.22910 Time:0.005580\n",
      "save model with val loss 0.229\n",
      "[ Epoch 265, Train ] | Loss:0.00058 Time:0.097895\n",
      "[ Epoch 265, Val ] | Loss:0.23126 Time:0.005420\n",
      "[ Epoch 266, Train ] | Loss:0.00056 Time:0.097322\n",
      "[ Epoch 266, Val ] | Loss:0.23081 Time:0.004833\n",
      "[ Epoch 267, Train ] | Loss:0.00049 Time:0.097780\n",
      "[ Epoch 267, Val ] | Loss:0.22979 Time:0.005503\n",
      "[ Epoch 268, Train ] | Loss:0.00063 Time:0.097645\n",
      "[ Epoch 268, Val ] | Loss:0.22578 Time:0.005576\n",
      "save model with val loss 0.226\n",
      "[ Epoch 269, Train ] | Loss:0.00048 Time:0.099046\n",
      "[ Epoch 269, Val ] | Loss:0.22459 Time:0.005417\n",
      "save model with val loss 0.225\n",
      "[ Epoch 270, Train ] | Loss:0.00087 Time:0.098142\n",
      "[ Epoch 270, Val ] | Loss:0.20918 Time:0.005468\n",
      "save model with val loss 0.209\n",
      "[ Epoch 271, Train ] | Loss:0.00069 Time:0.097906\n",
      "[ Epoch 271, Val ] | Loss:0.19017 Time:0.005427\n",
      "save model with val loss 0.190\n",
      "[ Epoch 272, Train ] | Loss:0.00041 Time:0.098514\n",
      "[ Epoch 272, Val ] | Loss:0.18586 Time:0.005487\n",
      "save model with val loss 0.186\n",
      "[ Epoch 273, Train ] | Loss:0.00053 Time:0.100784\n",
      "[ Epoch 273, Val ] | Loss:0.18026 Time:0.005431\n",
      "save model with val loss 0.180\n",
      "[ Epoch 274, Train ] | Loss:0.00047 Time:0.098496\n",
      "[ Epoch 274, Val ] | Loss:0.18049 Time:0.005477\n",
      "[ Epoch 275, Train ] | Loss:0.00043 Time:0.097790\n",
      "[ Epoch 275, Val ] | Loss:0.17992 Time:0.005492\n",
      "save model with val loss 0.180\n",
      "[ Epoch 276, Train ] | Loss:0.00038 Time:0.097116\n",
      "[ Epoch 276, Val ] | Loss:0.18689 Time:0.005482\n",
      "[ Epoch 277, Train ] | Loss:0.00027 Time:0.097148\n",
      "[ Epoch 277, Val ] | Loss:0.19589 Time:0.005474\n",
      "[ Epoch 278, Train ] | Loss:0.00035 Time:0.098162\n",
      "[ Epoch 278, Val ] | Loss:0.19438 Time:0.005570\n",
      "[ Epoch 279, Train ] | Loss:0.00037 Time:0.097231\n",
      "[ Epoch 279, Val ] | Loss:0.18557 Time:0.005481\n",
      "[ Epoch 280, Train ] | Loss:0.00120 Time:0.097348\n",
      "[ Epoch 280, Val ] | Loss:0.21544 Time:0.005570\n",
      "[ Epoch 281, Train ] | Loss:0.00040 Time:0.097888\n",
      "[ Epoch 281, Val ] | Loss:0.21328 Time:0.005679\n",
      "[ Epoch 282, Train ] | Loss:0.00055 Time:0.096859\n",
      "[ Epoch 282, Val ] | Loss:0.18503 Time:0.005569\n",
      "[ Epoch 283, Train ] | Loss:0.00046 Time:0.097844\n",
      "[ Epoch 283, Val ] | Loss:0.16458 Time:0.005697\n",
      "save model with val loss 0.165\n",
      "[ Epoch 284, Train ] | Loss:0.00035 Time:0.099195\n",
      "[ Epoch 284, Val ] | Loss:0.15876 Time:0.005427\n",
      "save model with val loss 0.159\n",
      "[ Epoch 285, Train ] | Loss:0.00022 Time:0.097929\n",
      "[ Epoch 285, Val ] | Loss:0.16082 Time:0.005653\n",
      "[ Epoch 286, Train ] | Loss:0.00036 Time:0.097640\n",
      "[ Epoch 286, Val ] | Loss:0.16579 Time:0.005541\n",
      "[ Epoch 287, Train ] | Loss:0.00071 Time:0.097223\n",
      "[ Epoch 287, Val ] | Loss:0.16315 Time:0.005503\n",
      "[ Epoch 288, Train ] | Loss:0.00020 Time:0.098099\n",
      "[ Epoch 288, Val ] | Loss:0.17107 Time:0.005566\n",
      "[ Epoch 289, Train ] | Loss:0.00037 Time:0.098918\n",
      "[ Epoch 289, Val ] | Loss:0.18123 Time:0.005514\n",
      "[ Epoch 290, Train ] | Loss:0.00074 Time:0.098171\n",
      "[ Epoch 290, Val ] | Loss:0.18630 Time:0.005578\n",
      "[ Epoch 291, Train ] | Loss:0.00045 Time:0.096800\n",
      "[ Epoch 291, Val ] | Loss:0.18280 Time:0.005508\n",
      "[ Epoch 292, Train ] | Loss:0.00029 Time:0.097555\n",
      "[ Epoch 292, Val ] | Loss:0.19355 Time:0.005581\n",
      "[ Epoch 293, Train ] | Loss:0.00041 Time:0.098051\n",
      "[ Epoch 293, Val ] | Loss:0.19980 Time:0.005498\n",
      "[ Epoch 294, Train ] | Loss:0.00033 Time:0.097372\n",
      "[ Epoch 294, Val ] | Loss:0.19682 Time:0.005691\n",
      "[ Epoch 295, Train ] | Loss:0.00032 Time:0.098475\n",
      "[ Epoch 295, Val ] | Loss:0.18705 Time:0.005509\n",
      "[ Epoch 296, Train ] | Loss:0.00027 Time:0.101662\n",
      "[ Epoch 296, Val ] | Loss:0.17957 Time:0.005555\n",
      "[ Epoch 297, Train ] | Loss:0.00017 Time:0.097283\n",
      "[ Epoch 297, Val ] | Loss:0.17114 Time:0.005511\n",
      "[ Epoch 298, Train ] | Loss:0.00025 Time:0.097119\n",
      "[ Epoch 298, Val ] | Loss:0.16986 Time:0.005577\n",
      "[ Epoch 299, Train ] | Loss:0.00028 Time:0.097626\n",
      "[ Epoch 299, Val ] | Loss:0.16668 Time:0.005508\n",
      "[ Epoch 300, Train ] | Loss:0.00019 Time:0.097769\n",
      "[ Epoch 300, Val ] | Loss:0.16699 Time:0.005568\n",
      "Finish training!\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1e38221-4b63-479f-9fac-989bbff2b41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin testing\n",
      "Test loss: 0.31839436292648315\n",
      "——————Test——————\n",
      "           class precision  recall f1-score support\n",
      "0              0    1.0000  1.0000   1.0000       5\n",
      "1              1    1.0000  0.6000   0.7500       5\n",
      "2              2    1.0000  1.0000   1.0000       5\n",
      "3              3    1.0000  1.0000   1.0000       5\n",
      "4              4    0.8333  1.0000   0.9091       5\n",
      "..           ...       ...     ...      ...     ...\n",
      "58            58    1.0000  0.8000   0.8889       5\n",
      "59            59    1.0000  1.0000   1.0000       5\n",
      "60      accuracy                     0.9033     300\n",
      "61     macro avg    0.9196  0.9033   0.9024     300\n",
      "62  weighted avg    0.9196  0.9033   0.9024     300\n",
      "\n",
      "[63 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70964516-9b18-46a2-9937-0268625e4436",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vanilla_CNN_60(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Vanilla_CNN_60, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 1, 1), # input channel, ouput channel, filter size, stride, padding\n",
    "            nn.BatchNorm2d(32), # num_features\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, 1, 1), \n",
    "            nn.BatchNorm2d(32), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0), # kernel size, stride, padding\n",
    "        \n",
    "            nn.Conv2d(32, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64*7*7, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512,60),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1) # resize, first axis is batch_num\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adc0fbb0-b61d-410f-ac76-bc73f5fbcf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_60 = Vanilla_CNN_60()\n",
    "epochs = 300\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4\n",
    "optimizer = Adam(cnn_60.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_dir = './output'\n",
    "model_name = 'cnn_60'\n",
    "random_seed = 2022\n",
    "trainer = Trainer(model=cnn_60, \n",
    "                  epochs=epochs, \n",
    "                  train_dataloader=train_loader, \n",
    "                  val_dataloader=val_loader, \n",
    "                  test_dataloader=test_loader,\n",
    "                  criterion=criterion, \n",
    "                  optimizer=optimizer, \n",
    "                  lr=learning_rate, \n",
    "                  device=device, \n",
    "                  model_dir=model_dir, \n",
    "                  model_name=model_name, \n",
    "                  random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8dece15-4713-4a82-816c-f12b9c4551f8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameters: 1703324, trainable parameters: 1703324\n",
      "\n",
      "        initial learning rate: 0.001, weight_decay: 0.0001,\n",
      "        total epochs: 300\n",
      "        \n",
      "Begin training, total epochs: 300\n",
      "[ Epoch 1, Train ] | Loss:4.01522 Time:0.088544\n",
      "[ Epoch 1, Val ] | Loss:4.10088 Time:0.005184\n",
      "save model with val loss 4.101\n",
      "[ Epoch 2, Train ] | Loss:3.85868 Time:0.077744\n",
      "[ Epoch 2, Val ] | Loss:4.11454 Time:0.005150\n",
      "[ Epoch 3, Train ] | Loss:3.73068 Time:0.077056\n",
      "[ Epoch 3, Val ] | Loss:4.11429 Time:0.005155\n",
      "[ Epoch 4, Train ] | Loss:3.61424 Time:0.077109\n",
      "[ Epoch 4, Val ] | Loss:4.09186 Time:0.006062\n",
      "save model with val loss 4.092\n",
      "[ Epoch 5, Train ] | Loss:3.52320 Time:0.077760\n",
      "[ Epoch 5, Val ] | Loss:4.03765 Time:0.005160\n",
      "save model with val loss 4.038\n",
      "[ Epoch 6, Train ] | Loss:3.44313 Time:0.078347\n",
      "[ Epoch 6, Val ] | Loss:3.87158 Time:0.004911\n",
      "save model with val loss 3.872\n",
      "[ Epoch 7, Train ] | Loss:3.39250 Time:0.077770\n",
      "[ Epoch 7, Val ] | Loss:3.75055 Time:0.005176\n",
      "save model with val loss 3.751\n",
      "[ Epoch 8, Train ] | Loss:3.35459 Time:0.079860\n",
      "[ Epoch 8, Val ] | Loss:3.64054 Time:0.005139\n",
      "save model with val loss 3.641\n",
      "[ Epoch 9, Train ] | Loss:3.29098 Time:0.076975\n",
      "[ Epoch 9, Val ] | Loss:3.61605 Time:0.005187\n",
      "save model with val loss 3.616\n",
      "[ Epoch 10, Train ] | Loss:3.25288 Time:0.080334\n",
      "[ Epoch 10, Val ] | Loss:3.51619 Time:0.005352\n",
      "save model with val loss 3.516\n",
      "[ Epoch 11, Train ] | Loss:3.21979 Time:0.077850\n",
      "[ Epoch 11, Val ] | Loss:3.52644 Time:0.005181\n",
      "[ Epoch 12, Train ] | Loss:3.20993 Time:0.077034\n",
      "[ Epoch 12, Val ] | Loss:3.49446 Time:0.005231\n",
      "save model with val loss 3.494\n",
      "[ Epoch 13, Train ] | Loss:3.19841 Time:0.077223\n",
      "[ Epoch 13, Val ] | Loss:3.52897 Time:0.005131\n",
      "[ Epoch 14, Train ] | Loss:3.17735 Time:0.077248\n",
      "[ Epoch 14, Val ] | Loss:3.46577 Time:0.005241\n",
      "save model with val loss 3.466\n",
      "[ Epoch 15, Train ] | Loss:3.15801 Time:0.077582\n",
      "[ Epoch 15, Val ] | Loss:3.48578 Time:0.005153\n",
      "[ Epoch 16, Train ] | Loss:3.14959 Time:0.079311\n",
      "[ Epoch 16, Val ] | Loss:3.46494 Time:0.005180\n",
      "save model with val loss 3.465\n",
      "[ Epoch 17, Train ] | Loss:3.14057 Time:0.076991\n",
      "[ Epoch 17, Val ] | Loss:3.44990 Time:0.005177\n",
      "save model with val loss 3.450\n",
      "[ Epoch 18, Train ] | Loss:3.13612 Time:0.078192\n",
      "[ Epoch 18, Val ] | Loss:3.43929 Time:0.005136\n",
      "save model with val loss 3.439\n",
      "[ Epoch 19, Train ] | Loss:3.12864 Time:0.077153\n",
      "[ Epoch 19, Val ] | Loss:3.39856 Time:0.005180\n",
      "save model with val loss 3.399\n",
      "[ Epoch 20, Train ] | Loss:3.12645 Time:0.079911\n",
      "[ Epoch 20, Val ] | Loss:3.38572 Time:0.005129\n",
      "save model with val loss 3.386\n",
      "[ Epoch 21, Train ] | Loss:3.12549 Time:0.077355\n",
      "[ Epoch 21, Val ] | Loss:3.38387 Time:0.005187\n",
      "save model with val loss 3.384\n",
      "[ Epoch 22, Train ] | Loss:3.12471 Time:0.077389\n",
      "[ Epoch 22, Val ] | Loss:3.37201 Time:0.005114\n",
      "save model with val loss 3.372\n",
      "[ Epoch 23, Train ] | Loss:3.12449 Time:0.077232\n",
      "[ Epoch 23, Val ] | Loss:3.36637 Time:0.005164\n",
      "save model with val loss 3.366\n",
      "[ Epoch 24, Train ] | Loss:3.12414 Time:0.077677\n",
      "[ Epoch 24, Val ] | Loss:3.35841 Time:0.005113\n",
      "save model with val loss 3.358\n",
      "[ Epoch 25, Train ] | Loss:3.12403 Time:0.077123\n",
      "[ Epoch 25, Val ] | Loss:3.35150 Time:0.005156\n",
      "save model with val loss 3.352\n",
      "[ Epoch 26, Train ] | Loss:3.12404 Time:0.079446\n",
      "[ Epoch 26, Val ] | Loss:3.34978 Time:0.005121\n",
      "save model with val loss 3.350\n",
      "[ Epoch 27, Train ] | Loss:3.12379 Time:0.077613\n",
      "[ Epoch 27, Val ] | Loss:3.35071 Time:0.005193\n",
      "[ Epoch 28, Train ] | Loss:3.12380 Time:0.076704\n",
      "[ Epoch 28, Val ] | Loss:3.35223 Time:0.005237\n",
      "[ Epoch 29, Train ] | Loss:3.12394 Time:0.078905\n",
      "[ Epoch 29, Val ] | Loss:3.35182 Time:0.005158\n",
      "[ Epoch 30, Train ] | Loss:3.12372 Time:0.076447\n",
      "[ Epoch 30, Val ] | Loss:3.35252 Time:0.005232\n",
      "[ Epoch 31, Train ] | Loss:3.12370 Time:0.076435\n",
      "[ Epoch 31, Val ] | Loss:3.34716 Time:0.005247\n",
      "save model with val loss 3.347\n",
      "[ Epoch 32, Train ] | Loss:3.12372 Time:0.078926\n",
      "[ Epoch 32, Val ] | Loss:3.34658 Time:0.005123\n",
      "save model with val loss 3.347\n",
      "[ Epoch 33, Train ] | Loss:3.12368 Time:0.081462\n",
      "[ Epoch 33, Val ] | Loss:3.34764 Time:0.005161\n",
      "[ Epoch 34, Train ] | Loss:3.12362 Time:0.078463\n",
      "[ Epoch 34, Val ] | Loss:3.35350 Time:0.005368\n",
      "[ Epoch 35, Train ] | Loss:3.12360 Time:0.080903\n",
      "[ Epoch 35, Val ] | Loss:3.35303 Time:0.005198\n",
      "[ Epoch 36, Train ] | Loss:3.12355 Time:0.078896\n",
      "[ Epoch 36, Val ] | Loss:3.34937 Time:0.005272\n",
      "[ Epoch 37, Train ] | Loss:3.12359 Time:0.079096\n",
      "[ Epoch 37, Val ] | Loss:3.35216 Time:0.005368\n",
      "[ Epoch 38, Train ] | Loss:3.12355 Time:0.078502\n",
      "[ Epoch 38, Val ] | Loss:3.35566 Time:0.005200\n",
      "[ Epoch 39, Train ] | Loss:3.12361 Time:0.078809\n",
      "[ Epoch 39, Val ] | Loss:3.35106 Time:0.005253\n",
      "[ Epoch 40, Train ] | Loss:3.12349 Time:0.078724\n",
      "[ Epoch 40, Val ] | Loss:3.35000 Time:0.005252\n",
      "[ Epoch 41, Train ] | Loss:3.12355 Time:0.081116\n",
      "[ Epoch 41, Val ] | Loss:3.35754 Time:0.005207\n",
      "[ Epoch 42, Train ] | Loss:3.12354 Time:0.081484\n",
      "[ Epoch 42, Val ] | Loss:3.36377 Time:0.005253\n",
      "[ Epoch 43, Train ] | Loss:3.12356 Time:0.078976\n",
      "[ Epoch 43, Val ] | Loss:3.35790 Time:0.005253\n",
      "[ Epoch 44, Train ] | Loss:3.12356 Time:0.081426\n",
      "[ Epoch 44, Val ] | Loss:3.35687 Time:0.005202\n",
      "[ Epoch 45, Train ] | Loss:3.12354 Time:0.078968\n",
      "[ Epoch 45, Val ] | Loss:3.35896 Time:0.005374\n",
      "[ Epoch 46, Train ] | Loss:3.12354 Time:0.078353\n",
      "[ Epoch 46, Val ] | Loss:3.36009 Time:0.005258\n",
      "[ Epoch 47, Train ] | Loss:3.12358 Time:0.078374\n",
      "[ Epoch 47, Val ] | Loss:3.35554 Time:0.005178\n",
      "[ Epoch 48, Train ] | Loss:3.12355 Time:0.078910\n",
      "[ Epoch 48, Val ] | Loss:3.36117 Time:0.005252\n",
      "[ Epoch 49, Train ] | Loss:3.12351 Time:0.081304\n",
      "[ Epoch 49, Val ] | Loss:3.36074 Time:0.005372\n",
      "[ Epoch 50, Train ] | Loss:3.12354 Time:0.078617\n",
      "[ Epoch 50, Val ] | Loss:3.35909 Time:0.005209\n",
      "[ Epoch 51, Train ] | Loss:3.12354 Time:0.077998\n",
      "[ Epoch 51, Val ] | Loss:3.36505 Time:0.005276\n",
      "[ Epoch 52, Train ] | Loss:3.12358 Time:0.080980\n",
      "[ Epoch 52, Val ] | Loss:3.36273 Time:0.005272\n",
      "[ Epoch 53, Train ] | Loss:3.12350 Time:0.078820\n",
      "[ Epoch 53, Val ] | Loss:3.36128 Time:0.005190\n",
      "[ Epoch 54, Train ] | Loss:3.12359 Time:0.078830\n",
      "[ Epoch 54, Val ] | Loss:3.36437 Time:0.005249\n",
      "[ Epoch 55, Train ] | Loss:3.12357 Time:0.078798\n",
      "[ Epoch 55, Val ] | Loss:3.35917 Time:0.005379\n",
      "[ Epoch 56, Train ] | Loss:3.12349 Time:0.080354\n",
      "[ Epoch 56, Val ] | Loss:3.36137 Time:0.005221\n",
      "[ Epoch 57, Train ] | Loss:3.12349 Time:0.078500\n",
      "[ Epoch 57, Val ] | Loss:3.36384 Time:0.005255\n",
      "[ Epoch 58, Train ] | Loss:3.12359 Time:0.078458\n",
      "[ Epoch 58, Val ] | Loss:3.37186 Time:0.005263\n",
      "[ Epoch 59, Train ] | Loss:3.12351 Time:0.080873\n",
      "[ Epoch 59, Val ] | Loss:3.36761 Time:0.005196\n",
      "[ Epoch 60, Train ] | Loss:3.12355 Time:0.081144\n",
      "[ Epoch 60, Val ] | Loss:3.36578 Time:0.005252\n",
      "[ Epoch 61, Train ] | Loss:3.12358 Time:0.080876\n",
      "[ Epoch 61, Val ] | Loss:3.36730 Time:0.005263\n",
      "[ Epoch 62, Train ] | Loss:3.12350 Time:0.078447\n",
      "[ Epoch 62, Val ] | Loss:3.37241 Time:0.005196\n",
      "[ Epoch 63, Train ] | Loss:3.12350 Time:0.078226\n",
      "[ Epoch 63, Val ] | Loss:3.37596 Time:0.005265\n",
      "[ Epoch 64, Train ] | Loss:3.12364 Time:0.083667\n",
      "[ Epoch 64, Val ] | Loss:3.37272 Time:0.007162\n",
      "[ Epoch 65, Train ] | Loss:3.12353 Time:0.078294\n",
      "[ Epoch 65, Val ] | Loss:3.37506 Time:0.005163\n",
      "[ Epoch 66, Train ] | Loss:3.12357 Time:0.078646\n",
      "[ Epoch 66, Val ] | Loss:3.37017 Time:0.005368\n",
      "[ Epoch 67, Train ] | Loss:3.12356 Time:0.078582\n",
      "[ Epoch 67, Val ] | Loss:3.37877 Time:0.005264\n",
      "[ Epoch 68, Train ] | Loss:3.12358 Time:0.078619\n",
      "[ Epoch 68, Val ] | Loss:3.38621 Time:0.005191\n",
      "[ Epoch 69, Train ] | Loss:3.12355 Time:0.078082\n",
      "[ Epoch 69, Val ] | Loss:3.38212 Time:0.005270\n",
      "[ Epoch 70, Train ] | Loss:3.12359 Time:0.078347\n",
      "[ Epoch 70, Val ] | Loss:3.38191 Time:0.005251\n",
      "[ Epoch 71, Train ] | Loss:3.12360 Time:0.085310\n",
      "[ Epoch 71, Val ] | Loss:3.38427 Time:0.005186\n",
      "[ Epoch 72, Train ] | Loss:3.12352 Time:0.078255\n",
      "[ Epoch 72, Val ] | Loss:3.38709 Time:0.005274\n",
      "[ Epoch 73, Train ] | Loss:3.12365 Time:0.078569\n",
      "[ Epoch 73, Val ] | Loss:3.38114 Time:0.005254\n",
      "[ Epoch 74, Train ] | Loss:3.12362 Time:0.080099\n",
      "[ Epoch 74, Val ] | Loss:3.38176 Time:0.005197\n",
      "[ Epoch 75, Train ] | Loss:3.12360 Time:0.080580\n",
      "[ Epoch 75, Val ] | Loss:3.38150 Time:0.005271\n",
      "[ Epoch 76, Train ] | Loss:3.12359 Time:0.078334\n",
      "[ Epoch 76, Val ] | Loss:3.38039 Time:0.005275\n",
      "[ Epoch 77, Train ] | Loss:3.12359 Time:0.078518\n",
      "[ Epoch 77, Val ] | Loss:3.39036 Time:0.005383\n",
      "[ Epoch 78, Train ] | Loss:3.12360 Time:0.078523\n",
      "[ Epoch 78, Val ] | Loss:3.38723 Time:0.005243\n",
      "[ Epoch 79, Train ] | Loss:3.12360 Time:0.078376\n",
      "[ Epoch 79, Val ] | Loss:3.39734 Time:0.005271\n",
      "[ Epoch 80, Train ] | Loss:3.12366 Time:0.080495\n",
      "[ Epoch 80, Val ] | Loss:3.39244 Time:0.005202\n",
      "[ Epoch 81, Train ] | Loss:3.12356 Time:0.078110\n",
      "[ Epoch 81, Val ] | Loss:3.38709 Time:0.005269\n",
      "[ Epoch 82, Train ] | Loss:3.12368 Time:0.078084\n",
      "[ Epoch 82, Val ] | Loss:3.38438 Time:0.005253\n",
      "[ Epoch 83, Train ] | Loss:3.12357 Time:0.080350\n",
      "[ Epoch 83, Val ] | Loss:3.39275 Time:0.005180\n",
      "[ Epoch 84, Train ] | Loss:3.12360 Time:0.078131\n",
      "[ Epoch 84, Val ] | Loss:3.39471 Time:0.005263\n",
      "[ Epoch 85, Train ] | Loss:3.12352 Time:0.080273\n",
      "[ Epoch 85, Val ] | Loss:3.38978 Time:0.005278\n",
      "[ Epoch 86, Train ] | Loss:3.12362 Time:0.078337\n",
      "[ Epoch 86, Val ] | Loss:3.40020 Time:0.005201\n",
      "[ Epoch 87, Train ] | Loss:3.12357 Time:0.078338\n",
      "[ Epoch 87, Val ] | Loss:3.40880 Time:0.005275\n",
      "[ Epoch 88, Train ] | Loss:3.12372 Time:0.078562\n",
      "[ Epoch 88, Val ] | Loss:3.40574 Time:0.005265\n",
      "[ Epoch 89, Train ] | Loss:3.12361 Time:0.080484\n",
      "[ Epoch 89, Val ] | Loss:3.40451 Time:0.005192\n",
      "[ Epoch 90, Train ] | Loss:3.12370 Time:0.078127\n",
      "[ Epoch 90, Val ] | Loss:3.41070 Time:0.005275\n",
      "[ Epoch 91, Train ] | Loss:3.12364 Time:0.078048\n",
      "[ Epoch 91, Val ] | Loss:3.40835 Time:0.005275\n",
      "[ Epoch 92, Train ] | Loss:3.12370 Time:0.080133\n",
      "[ Epoch 92, Val ] | Loss:3.39828 Time:0.005211\n",
      "[ Epoch 93, Train ] | Loss:3.12370 Time:0.077956\n",
      "[ Epoch 93, Val ] | Loss:3.41018 Time:0.005274\n",
      "[ Epoch 94, Train ] | Loss:3.12371 Time:0.077975\n",
      "[ Epoch 94, Val ] | Loss:3.40234 Time:0.005257\n",
      "[ Epoch 95, Train ] | Loss:3.12370 Time:0.082249\n",
      "[ Epoch 95, Val ] | Loss:3.41068 Time:0.005191\n",
      "[ Epoch 96, Train ] | Loss:3.12369 Time:0.080045\n",
      "[ Epoch 96, Val ] | Loss:3.40185 Time:0.005389\n",
      "[ Epoch 97, Train ] | Loss:3.12384 Time:0.078173\n",
      "[ Epoch 97, Val ] | Loss:3.41567 Time:0.005271\n",
      "[ Epoch 98, Train ] | Loss:3.12382 Time:0.077668\n",
      "[ Epoch 98, Val ] | Loss:3.40216 Time:0.005209\n",
      "[ Epoch 99, Train ] | Loss:3.12396 Time:0.077985\n",
      "[ Epoch 99, Val ] | Loss:3.42390 Time:0.005270\n",
      "[ Epoch 100, Train ] | Loss:3.12385 Time:0.077902\n",
      "[ Epoch 100, Val ] | Loss:3.44375 Time:0.005210\n",
      "[ Epoch 101, Train ] | Loss:3.12393 Time:0.079926\n",
      "[ Epoch 101, Val ] | Loss:3.40144 Time:0.005207\n",
      "[ Epoch 102, Train ] | Loss:3.12388 Time:0.078127\n",
      "[ Epoch 102, Val ] | Loss:3.41318 Time:0.005271\n",
      "[ Epoch 103, Train ] | Loss:3.12388 Time:0.078130\n",
      "[ Epoch 103, Val ] | Loss:3.40608 Time:0.005267\n",
      "[ Epoch 104, Train ] | Loss:3.12393 Time:0.080255\n",
      "[ Epoch 104, Val ] | Loss:3.41918 Time:0.005371\n",
      "[ Epoch 105, Train ] | Loss:3.12396 Time:0.078104\n",
      "[ Epoch 105, Val ] | Loss:3.41121 Time:0.005265\n",
      "[ Epoch 106, Train ] | Loss:3.12383 Time:0.077707\n",
      "[ Epoch 106, Val ] | Loss:3.42640 Time:0.005259\n",
      "[ Epoch 107, Train ] | Loss:3.12392 Time:0.077845\n",
      "[ Epoch 107, Val ] | Loss:3.40947 Time:0.005197\n",
      "[ Epoch 108, Train ] | Loss:3.12388 Time:0.077995\n",
      "[ Epoch 108, Val ] | Loss:3.41813 Time:0.005268\n",
      "[ Epoch 109, Train ] | Loss:3.12383 Time:0.077587\n",
      "[ Epoch 109, Val ] | Loss:3.42384 Time:0.005270\n",
      "[ Epoch 110, Train ] | Loss:3.12395 Time:0.078068\n",
      "[ Epoch 110, Val ] | Loss:3.42392 Time:0.005225\n",
      "[ Epoch 111, Train ] | Loss:3.12410 Time:0.077984\n",
      "[ Epoch 111, Val ] | Loss:3.41877 Time:0.005275\n",
      "[ Epoch 112, Train ] | Loss:3.12413 Time:0.078035\n",
      "[ Epoch 112, Val ] | Loss:3.42035 Time:0.005309\n",
      "[ Epoch 113, Train ] | Loss:3.12423 Time:0.080047\n",
      "[ Epoch 113, Val ] | Loss:3.41827 Time:0.005187\n",
      "[ Epoch 114, Train ] | Loss:3.12423 Time:0.077890\n",
      "[ Epoch 114, Val ] | Loss:3.40389 Time:0.005261\n",
      "[ Epoch 115, Train ] | Loss:3.12420 Time:0.078014\n",
      "[ Epoch 115, Val ] | Loss:3.41720 Time:0.005258\n",
      "[ Epoch 116, Train ] | Loss:3.12421 Time:0.079999\n",
      "[ Epoch 116, Val ] | Loss:3.43640 Time:0.005204\n",
      "[ Epoch 117, Train ] | Loss:3.12412 Time:0.079739\n",
      "[ Epoch 117, Val ] | Loss:3.43735 Time:0.005387\n",
      "[ Epoch 118, Train ] | Loss:3.12396 Time:0.077452\n",
      "[ Epoch 118, Val ] | Loss:3.42942 Time:0.005255\n",
      "[ Epoch 119, Train ] | Loss:3.12395 Time:0.077959\n",
      "[ Epoch 119, Val ] | Loss:3.41705 Time:0.005213\n",
      "[ Epoch 120, Train ] | Loss:3.12375 Time:0.080071\n",
      "[ Epoch 120, Val ] | Loss:3.40180 Time:0.005277\n",
      "[ Epoch 121, Train ] | Loss:3.12373 Time:0.077652\n",
      "[ Epoch 121, Val ] | Loss:3.40494 Time:0.005372\n",
      "[ Epoch 122, Train ] | Loss:3.12364 Time:0.080291\n",
      "[ Epoch 122, Val ] | Loss:3.40685 Time:0.005201\n",
      "[ Epoch 123, Train ] | Loss:3.12358 Time:0.080261\n",
      "[ Epoch 123, Val ] | Loss:3.41690 Time:0.005254\n",
      "[ Epoch 124, Train ] | Loss:3.12363 Time:0.077437\n",
      "[ Epoch 124, Val ] | Loss:3.42846 Time:0.005271\n",
      "[ Epoch 125, Train ] | Loss:3.12355 Time:0.077837\n",
      "[ Epoch 125, Val ] | Loss:3.42439 Time:0.005203\n",
      "[ Epoch 126, Train ] | Loss:3.12358 Time:0.077790\n",
      "[ Epoch 126, Val ] | Loss:3.43202 Time:0.005281\n",
      "[ Epoch 127, Train ] | Loss:3.12364 Time:0.080211\n",
      "[ Epoch 127, Val ] | Loss:3.41724 Time:0.005277\n",
      "[ Epoch 128, Train ] | Loss:3.12375 Time:0.077829\n",
      "[ Epoch 128, Val ] | Loss:3.40983 Time:0.005198\n",
      "[ Epoch 129, Train ] | Loss:3.12397 Time:0.077449\n",
      "[ Epoch 129, Val ] | Loss:3.43070 Time:0.005269\n",
      "[ Epoch 130, Train ] | Loss:3.12411 Time:0.077492\n",
      "[ Epoch 130, Val ] | Loss:3.45609 Time:0.005272\n",
      "[ Epoch 131, Train ] | Loss:3.12428 Time:0.079890\n",
      "[ Epoch 131, Val ] | Loss:3.45194 Time:0.005189\n",
      "[ Epoch 132, Train ] | Loss:3.12435 Time:0.082598\n",
      "[ Epoch 132, Val ] | Loss:3.42686 Time:0.005269\n",
      "[ Epoch 133, Train ] | Loss:3.12467 Time:0.076900\n",
      "[ Epoch 133, Val ] | Loss:3.42485 Time:0.005258\n",
      "[ Epoch 134, Train ] | Loss:3.12460 Time:0.077694\n",
      "[ Epoch 134, Val ] | Loss:3.45665 Time:0.005200\n",
      "[ Epoch 135, Train ] | Loss:3.12459 Time:0.077427\n",
      "[ Epoch 135, Val ] | Loss:3.45038 Time:0.005264\n",
      "[ Epoch 136, Train ] | Loss:3.12456 Time:0.077139\n",
      "[ Epoch 136, Val ] | Loss:3.40828 Time:0.005275\n",
      "[ Epoch 137, Train ] | Loss:3.12451 Time:0.080060\n",
      "[ Epoch 137, Val ] | Loss:3.44282 Time:0.005193\n",
      "[ Epoch 138, Train ] | Loss:3.12448 Time:0.079892\n",
      "[ Epoch 138, Val ] | Loss:3.43086 Time:0.005271\n",
      "[ Epoch 139, Train ] | Loss:3.12422 Time:0.077587\n",
      "[ Epoch 139, Val ] | Loss:3.39578 Time:0.005275\n",
      "[ Epoch 140, Train ] | Loss:3.12404 Time:0.077315\n",
      "[ Epoch 140, Val ] | Loss:3.40539 Time:0.005207\n",
      "[ Epoch 141, Train ] | Loss:3.12408 Time:0.077610\n",
      "[ Epoch 141, Val ] | Loss:3.42056 Time:0.005255\n",
      "[ Epoch 142, Train ] | Loss:3.12409 Time:0.077360\n",
      "[ Epoch 142, Val ] | Loss:3.42524 Time:0.005270\n",
      "[ Epoch 143, Train ] | Loss:3.12385 Time:0.079469\n",
      "[ Epoch 143, Val ] | Loss:3.42180 Time:0.005206\n",
      "[ Epoch 144, Train ] | Loss:3.12378 Time:0.077574\n",
      "[ Epoch 144, Val ] | Loss:3.40962 Time:0.005247\n",
      "[ Epoch 145, Train ] | Loss:3.12370 Time:0.077508\n",
      "[ Epoch 145, Val ] | Loss:3.40675 Time:0.005267\n",
      "[ Epoch 146, Train ] | Loss:3.12356 Time:0.076923\n",
      "[ Epoch 146, Val ] | Loss:3.40875 Time:0.005211\n",
      "[ Epoch 147, Train ] | Loss:3.12353 Time:0.077541\n",
      "[ Epoch 147, Val ] | Loss:3.40702 Time:0.005244\n",
      "[ Epoch 148, Train ] | Loss:3.12349 Time:0.079577\n",
      "[ Epoch 148, Val ] | Loss:3.41428 Time:0.005260\n",
      "[ Epoch 149, Train ] | Loss:3.12354 Time:0.077529\n",
      "[ Epoch 149, Val ] | Loss:3.42036 Time:0.005200\n",
      "[ Epoch 150, Train ] | Loss:3.12350 Time:0.076715\n",
      "[ Epoch 150, Val ] | Loss:3.42234 Time:0.005265\n",
      "[ Epoch 151, Train ] | Loss:3.12355 Time:0.077493\n",
      "[ Epoch 151, Val ] | Loss:3.41904 Time:0.005267\n",
      "[ Epoch 152, Train ] | Loss:3.12355 Time:0.079885\n",
      "[ Epoch 152, Val ] | Loss:3.42771 Time:0.005194\n",
      "[ Epoch 153, Train ] | Loss:3.12348 Time:0.077409\n",
      "[ Epoch 153, Val ] | Loss:3.42758 Time:0.005265\n",
      "[ Epoch 154, Train ] | Loss:3.12343 Time:0.076962\n",
      "[ Epoch 154, Val ] | Loss:3.40867 Time:0.005258\n",
      "[ Epoch 155, Train ] | Loss:3.12349 Time:0.078976\n",
      "[ Epoch 155, Val ] | Loss:3.41904 Time:0.005210\n",
      "[ Epoch 156, Train ] | Loss:3.12358 Time:0.077405\n",
      "[ Epoch 156, Val ] | Loss:3.42557 Time:0.005251\n",
      "[ Epoch 157, Train ] | Loss:3.12350 Time:0.076994\n",
      "[ Epoch 157, Val ] | Loss:3.43761 Time:0.005250\n",
      "[ Epoch 158, Train ] | Loss:3.12372 Time:0.079230\n",
      "[ Epoch 158, Val ] | Loss:3.44800 Time:0.005196\n",
      "[ Epoch 159, Train ] | Loss:3.12362 Time:0.079679\n",
      "[ Epoch 159, Val ] | Loss:3.43896 Time:0.005052\n",
      "[ Epoch 160, Train ] | Loss:3.12365 Time:0.077188\n",
      "[ Epoch 160, Val ] | Loss:3.43720 Time:0.005259\n",
      "[ Epoch 161, Train ] | Loss:3.12382 Time:0.077369\n",
      "[ Epoch 161, Val ] | Loss:3.43379 Time:0.005187\n",
      "[ Epoch 162, Train ] | Loss:3.12393 Time:0.077241\n",
      "[ Epoch 162, Val ] | Loss:3.42059 Time:0.005281\n",
      "[ Epoch 163, Train ] | Loss:3.12424 Time:0.077071\n",
      "[ Epoch 163, Val ] | Loss:3.45055 Time:0.005275\n",
      "[ Epoch 164, Train ] | Loss:3.12448 Time:0.079537\n",
      "[ Epoch 164, Val ] | Loss:3.46490 Time:0.005199\n",
      "[ Epoch 165, Train ] | Loss:3.12473 Time:0.077195\n",
      "[ Epoch 165, Val ] | Loss:3.44680 Time:0.005278\n",
      "[ Epoch 166, Train ] | Loss:3.12476 Time:0.079623\n",
      "[ Epoch 166, Val ] | Loss:3.46450 Time:0.005317\n",
      "[ Epoch 167, Train ] | Loss:3.12521 Time:0.077257\n",
      "[ Epoch 167, Val ] | Loss:3.43921 Time:0.005199\n",
      "[ Epoch 168, Train ] | Loss:3.12768 Time:0.076917\n",
      "[ Epoch 168, Val ] | Loss:3.58596 Time:0.005268\n",
      "[ Epoch 169, Train ] | Loss:3.14553 Time:0.076582\n",
      "[ Epoch 169, Val ] | Loss:3.73018 Time:0.005268\n",
      "[ Epoch 170, Train ] | Loss:3.26193 Time:0.076892\n",
      "[ Epoch 170, Val ] | Loss:3.74455 Time:0.005201\n",
      "[ Epoch 171, Train ] | Loss:3.20033 Time:0.077172\n",
      "[ Epoch 171, Val ] | Loss:3.49879 Time:0.005270\n",
      "[ Epoch 172, Train ] | Loss:3.15604 Time:0.077204\n",
      "[ Epoch 172, Val ] | Loss:3.49164 Time:0.005261\n",
      "[ Epoch 173, Train ] | Loss:3.13787 Time:0.076934\n",
      "[ Epoch 173, Val ] | Loss:3.33736 Time:0.005222\n",
      "save model with val loss 3.337\n",
      "[ Epoch 174, Train ] | Loss:3.12939 Time:0.077677\n",
      "[ Epoch 174, Val ] | Loss:3.31170 Time:0.005172\n",
      "save model with val loss 3.312\n",
      "[ Epoch 175, Train ] | Loss:3.12648 Time:0.076964\n",
      "[ Epoch 175, Val ] | Loss:3.29557 Time:0.005143\n",
      "save model with val loss 3.296\n",
      "[ Epoch 176, Train ] | Loss:3.12476 Time:0.077455\n",
      "[ Epoch 176, Val ] | Loss:3.27406 Time:0.005182\n",
      "save model with val loss 3.274\n",
      "[ Epoch 177, Train ] | Loss:3.12411 Time:0.077361\n",
      "[ Epoch 177, Val ] | Loss:3.27122 Time:0.005121\n",
      "save model with val loss 3.271\n",
      "[ Epoch 178, Train ] | Loss:3.12393 Time:0.077279\n",
      "[ Epoch 178, Val ] | Loss:3.27266 Time:0.005178\n",
      "[ Epoch 179, Train ] | Loss:3.12379 Time:0.076953\n",
      "[ Epoch 179, Val ] | Loss:3.27397 Time:0.005253\n",
      "[ Epoch 180, Train ] | Loss:3.12325 Time:0.079118\n",
      "[ Epoch 180, Val ] | Loss:3.27437 Time:0.005189\n",
      "[ Epoch 181, Train ] | Loss:3.12336 Time:0.076735\n",
      "[ Epoch 181, Val ] | Loss:3.27461 Time:0.005263\n",
      "[ Epoch 182, Train ] | Loss:3.12334 Time:0.076742\n",
      "[ Epoch 182, Val ] | Loss:3.27111 Time:0.005256\n",
      "save model with val loss 3.271\n",
      "[ Epoch 183, Train ] | Loss:3.12318 Time:0.080801\n",
      "[ Epoch 183, Val ] | Loss:3.26845 Time:0.005137\n",
      "save model with val loss 3.268\n",
      "[ Epoch 184, Train ] | Loss:3.12318 Time:0.079377\n",
      "[ Epoch 184, Val ] | Loss:3.26995 Time:0.005187\n",
      "[ Epoch 185, Train ] | Loss:3.12321 Time:0.079034\n",
      "[ Epoch 185, Val ] | Loss:3.27255 Time:0.005244\n",
      "[ Epoch 186, Train ] | Loss:3.12312 Time:0.078603\n",
      "[ Epoch 186, Val ] | Loss:3.27579 Time:0.005194\n",
      "[ Epoch 187, Train ] | Loss:3.12319 Time:0.078751\n",
      "[ Epoch 187, Val ] | Loss:3.27570 Time:0.005373\n",
      "[ Epoch 188, Train ] | Loss:3.12314 Time:0.078748\n",
      "[ Epoch 188, Val ] | Loss:3.27427 Time:0.005266\n",
      "[ Epoch 189, Train ] | Loss:3.12312 Time:0.078747\n",
      "[ Epoch 189, Val ] | Loss:3.27510 Time:0.005349\n",
      "[ Epoch 190, Train ] | Loss:3.12317 Time:0.078668\n",
      "[ Epoch 190, Val ] | Loss:3.27411 Time:0.005378\n",
      "[ Epoch 191, Train ] | Loss:3.12313 Time:0.078910\n",
      "[ Epoch 191, Val ] | Loss:3.27458 Time:0.005260\n",
      "[ Epoch 192, Train ] | Loss:3.12318 Time:0.079036\n",
      "[ Epoch 192, Val ] | Loss:3.27696 Time:0.005172\n",
      "[ Epoch 193, Train ] | Loss:3.12326 Time:0.078890\n",
      "[ Epoch 193, Val ] | Loss:3.27886 Time:0.005266\n",
      "[ Epoch 194, Train ] | Loss:3.12318 Time:0.081082\n",
      "[ Epoch 194, Val ] | Loss:3.28151 Time:0.005264\n",
      "[ Epoch 195, Train ] | Loss:3.12318 Time:0.081171\n",
      "[ Epoch 195, Val ] | Loss:3.28361 Time:0.005201\n",
      "[ Epoch 196, Train ] | Loss:3.12340 Time:0.081319\n",
      "[ Epoch 196, Val ] | Loss:3.28496 Time:0.005286\n",
      "[ Epoch 197, Train ] | Loss:3.12321 Time:0.078928\n",
      "[ Epoch 197, Val ] | Loss:3.28670 Time:0.005250\n",
      "[ Epoch 198, Train ] | Loss:3.12321 Time:0.080946\n",
      "[ Epoch 198, Val ] | Loss:3.28507 Time:0.005201\n",
      "[ Epoch 199, Train ] | Loss:3.12312 Time:0.078352\n",
      "[ Epoch 199, Val ] | Loss:3.28504 Time:0.005255\n",
      "[ Epoch 200, Train ] | Loss:3.12332 Time:0.081234\n",
      "[ Epoch 200, Val ] | Loss:3.28632 Time:0.005393\n",
      "[ Epoch 201, Train ] | Loss:3.12322 Time:0.080950\n",
      "[ Epoch 201, Val ] | Loss:3.28932 Time:0.005214\n",
      "[ Epoch 202, Train ] | Loss:3.12318 Time:0.083320\n",
      "[ Epoch 202, Val ] | Loss:3.28979 Time:0.005260\n",
      "[ Epoch 203, Train ] | Loss:3.12321 Time:0.080855\n",
      "[ Epoch 203, Val ] | Loss:3.29041 Time:0.005368\n",
      "[ Epoch 204, Train ] | Loss:3.12323 Time:0.078765\n",
      "[ Epoch 204, Val ] | Loss:3.29280 Time:0.005177\n",
      "[ Epoch 205, Train ] | Loss:3.12317 Time:0.080863\n",
      "[ Epoch 205, Val ] | Loss:3.29440 Time:0.005269\n",
      "[ Epoch 206, Train ] | Loss:3.12324 Time:0.078714\n",
      "[ Epoch 206, Val ] | Loss:3.29570 Time:0.005277\n",
      "[ Epoch 207, Train ] | Loss:3.12326 Time:0.080919\n",
      "[ Epoch 207, Val ] | Loss:3.29588 Time:0.005226\n",
      "[ Epoch 208, Train ] | Loss:3.12326 Time:0.078345\n",
      "[ Epoch 208, Val ] | Loss:3.29683 Time:0.005258\n",
      "[ Epoch 209, Train ] | Loss:3.12325 Time:0.078541\n",
      "[ Epoch 209, Val ] | Loss:3.29861 Time:0.005266\n",
      "[ Epoch 210, Train ] | Loss:3.12339 Time:0.078181\n",
      "[ Epoch 210, Val ] | Loss:3.29825 Time:0.005213\n",
      "[ Epoch 211, Train ] | Loss:3.12332 Time:0.078735\n",
      "[ Epoch 211, Val ] | Loss:3.29541 Time:0.005253\n",
      "[ Epoch 212, Train ] | Loss:3.12323 Time:0.078374\n",
      "[ Epoch 212, Val ] | Loss:3.29734 Time:0.005271\n",
      "[ Epoch 213, Train ] | Loss:3.12327 Time:0.078316\n",
      "[ Epoch 213, Val ] | Loss:3.30067 Time:0.005368\n",
      "[ Epoch 214, Train ] | Loss:3.12324 Time:0.077963\n",
      "[ Epoch 214, Val ] | Loss:3.29977 Time:0.005264\n",
      "[ Epoch 215, Train ] | Loss:3.12329 Time:0.078429\n",
      "[ Epoch 215, Val ] | Loss:3.30064 Time:0.005262\n",
      "[ Epoch 216, Train ] | Loss:3.12337 Time:0.077821\n",
      "[ Epoch 216, Val ] | Loss:3.30198 Time:0.005186\n",
      "[ Epoch 217, Train ] | Loss:3.12330 Time:0.077762\n",
      "[ Epoch 217, Val ] | Loss:3.30451 Time:0.005292\n",
      "[ Epoch 218, Train ] | Loss:3.12340 Time:0.078038\n",
      "[ Epoch 218, Val ] | Loss:3.30606 Time:0.005254\n",
      "[ Epoch 219, Train ] | Loss:3.12332 Time:0.078534\n",
      "[ Epoch 219, Val ] | Loss:3.30555 Time:0.005185\n",
      "[ Epoch 220, Train ] | Loss:3.12327 Time:0.080075\n",
      "[ Epoch 220, Val ] | Loss:3.30775 Time:0.005177\n",
      "[ Epoch 221, Train ] | Loss:3.12336 Time:0.077713\n",
      "[ Epoch 221, Val ] | Loss:3.30971 Time:0.005267\n",
      "[ Epoch 222, Train ] | Loss:3.12336 Time:0.080060\n",
      "[ Epoch 222, Val ] | Loss:3.31067 Time:0.005187\n",
      "[ Epoch 223, Train ] | Loss:3.12332 Time:0.078295\n",
      "[ Epoch 223, Val ] | Loss:3.31036 Time:0.005264\n",
      "[ Epoch 224, Train ] | Loss:3.12334 Time:0.078150\n",
      "[ Epoch 224, Val ] | Loss:3.31329 Time:0.005262\n",
      "[ Epoch 225, Train ] | Loss:3.12333 Time:0.078496\n",
      "[ Epoch 225, Val ] | Loss:3.31650 Time:0.005213\n",
      "[ Epoch 226, Train ] | Loss:3.12333 Time:0.078219\n",
      "[ Epoch 226, Val ] | Loss:3.31706 Time:0.005253\n",
      "[ Epoch 227, Train ] | Loss:3.12338 Time:0.078217\n",
      "[ Epoch 227, Val ] | Loss:3.31405 Time:0.005259\n",
      "[ Epoch 228, Train ] | Loss:3.12333 Time:0.078454\n",
      "[ Epoch 228, Val ] | Loss:3.31736 Time:0.005206\n",
      "[ Epoch 229, Train ] | Loss:3.12341 Time:0.078151\n",
      "[ Epoch 229, Val ] | Loss:3.31960 Time:0.005257\n",
      "[ Epoch 230, Train ] | Loss:3.12346 Time:0.078431\n",
      "[ Epoch 230, Val ] | Loss:3.32081 Time:0.005257\n",
      "[ Epoch 231, Train ] | Loss:3.12344 Time:0.077817\n",
      "[ Epoch 231, Val ] | Loss:3.32429 Time:0.005203\n",
      "[ Epoch 232, Train ] | Loss:3.12344 Time:0.078440\n",
      "[ Epoch 232, Val ] | Loss:3.32330 Time:0.005248\n",
      "[ Epoch 233, Train ] | Loss:3.12337 Time:0.081513\n",
      "[ Epoch 233, Val ] | Loss:3.32376 Time:0.005266\n",
      "[ Epoch 234, Train ] | Loss:3.12342 Time:0.080253\n",
      "[ Epoch 234, Val ] | Loss:3.32628 Time:0.005201\n",
      "[ Epoch 235, Train ] | Loss:3.12349 Time:0.078352\n",
      "[ Epoch 235, Val ] | Loss:3.32870 Time:0.005259\n",
      "[ Epoch 236, Train ] | Loss:3.12343 Time:0.080229\n",
      "[ Epoch 236, Val ] | Loss:3.32518 Time:0.005252\n",
      "[ Epoch 237, Train ] | Loss:3.12338 Time:0.082977\n",
      "[ Epoch 237, Val ] | Loss:3.32531 Time:0.005194\n",
      "[ Epoch 238, Train ] | Loss:3.12351 Time:0.077692\n",
      "[ Epoch 238, Val ] | Loss:3.32480 Time:0.005247\n",
      "[ Epoch 239, Train ] | Loss:3.12343 Time:0.078326\n",
      "[ Epoch 239, Val ] | Loss:3.32898 Time:0.005276\n",
      "[ Epoch 240, Train ] | Loss:3.12340 Time:0.078294\n",
      "[ Epoch 240, Val ] | Loss:3.33184 Time:0.005198\n",
      "[ Epoch 241, Train ] | Loss:3.12352 Time:0.080684\n",
      "[ Epoch 241, Val ] | Loss:3.32918 Time:0.005256\n",
      "[ Epoch 242, Train ] | Loss:3.12348 Time:0.077703\n",
      "[ Epoch 242, Val ] | Loss:3.33512 Time:0.005256\n",
      "[ Epoch 243, Train ] | Loss:3.12340 Time:0.080646\n",
      "[ Epoch 243, Val ] | Loss:3.33893 Time:0.005217\n",
      "[ Epoch 244, Train ] | Loss:3.12364 Time:0.077671\n",
      "[ Epoch 244, Val ] | Loss:3.33872 Time:0.005269\n",
      "[ Epoch 245, Train ] | Loss:3.12341 Time:0.082704\n",
      "[ Epoch 245, Val ] | Loss:3.34604 Time:0.005263\n",
      "[ Epoch 246, Train ] | Loss:3.12353 Time:0.077641\n",
      "[ Epoch 246, Val ] | Loss:3.34445 Time:0.005206\n",
      "[ Epoch 247, Train ] | Loss:3.12355 Time:0.080053\n",
      "[ Epoch 247, Val ] | Loss:3.33993 Time:0.005240\n",
      "[ Epoch 248, Train ] | Loss:3.12348 Time:0.077610\n",
      "[ Epoch 248, Val ] | Loss:3.33444 Time:0.005368\n",
      "[ Epoch 249, Train ] | Loss:3.12353 Time:0.080269\n",
      "[ Epoch 249, Val ] | Loss:3.33612 Time:0.005202\n",
      "[ Epoch 250, Train ] | Loss:3.12352 Time:0.078001\n",
      "[ Epoch 250, Val ] | Loss:3.34051 Time:0.005261\n",
      "[ Epoch 251, Train ] | Loss:3.12348 Time:0.077546\n",
      "[ Epoch 251, Val ] | Loss:3.34106 Time:0.005384\n",
      "[ Epoch 252, Train ] | Loss:3.12341 Time:0.077553\n",
      "[ Epoch 252, Val ] | Loss:3.34237 Time:0.005201\n",
      "[ Epoch 253, Train ] | Loss:3.12352 Time:0.077572\n",
      "[ Epoch 253, Val ] | Loss:3.34492 Time:0.005250\n",
      "[ Epoch 254, Train ] | Loss:3.12347 Time:0.077786\n",
      "[ Epoch 254, Val ] | Loss:3.33963 Time:0.005263\n",
      "[ Epoch 255, Train ] | Loss:3.12349 Time:0.078031\n",
      "[ Epoch 255, Val ] | Loss:3.34259 Time:0.005191\n",
      "[ Epoch 256, Train ] | Loss:3.12352 Time:0.077778\n",
      "[ Epoch 256, Val ] | Loss:3.35261 Time:0.005424\n",
      "[ Epoch 257, Train ] | Loss:3.12348 Time:0.077659\n",
      "[ Epoch 257, Val ] | Loss:3.35744 Time:0.005268\n",
      "[ Epoch 258, Train ] | Loss:3.12348 Time:0.080408\n",
      "[ Epoch 258, Val ] | Loss:3.35032 Time:0.005193\n",
      "[ Epoch 259, Train ] | Loss:3.12351 Time:0.078012\n",
      "[ Epoch 259, Val ] | Loss:3.34810 Time:0.005249\n",
      "[ Epoch 260, Train ] | Loss:3.12351 Time:0.078708\n",
      "[ Epoch 260, Val ] | Loss:3.35442 Time:0.005255\n",
      "[ Epoch 261, Train ] | Loss:3.12347 Time:0.079861\n",
      "[ Epoch 261, Val ] | Loss:3.35528 Time:0.005206\n",
      "[ Epoch 262, Train ] | Loss:3.12341 Time:0.077993\n",
      "[ Epoch 262, Val ] | Loss:3.35532 Time:0.005272\n",
      "[ Epoch 263, Train ] | Loss:3.12346 Time:0.077948\n",
      "[ Epoch 263, Val ] | Loss:3.35956 Time:0.005274\n",
      "[ Epoch 264, Train ] | Loss:3.12353 Time:0.077171\n",
      "[ Epoch 264, Val ] | Loss:3.35572 Time:0.005188\n",
      "[ Epoch 265, Train ] | Loss:3.12350 Time:0.078040\n",
      "[ Epoch 265, Val ] | Loss:3.36012 Time:0.005263\n",
      "[ Epoch 266, Train ] | Loss:3.12361 Time:0.076840\n",
      "[ Epoch 266, Val ] | Loss:3.36637 Time:0.005259\n",
      "[ Epoch 267, Train ] | Loss:3.12354 Time:0.077889\n",
      "[ Epoch 267, Val ] | Loss:3.36971 Time:0.005206\n",
      "[ Epoch 268, Train ] | Loss:3.12358 Time:0.077955\n",
      "[ Epoch 268, Val ] | Loss:3.36694 Time:0.005258\n",
      "[ Epoch 269, Train ] | Loss:3.12363 Time:0.077900\n",
      "[ Epoch 269, Val ] | Loss:3.36277 Time:0.005268\n",
      "[ Epoch 270, Train ] | Loss:3.12354 Time:0.079984\n",
      "[ Epoch 270, Val ] | Loss:3.35929 Time:0.005206\n",
      "[ Epoch 271, Train ] | Loss:3.12357 Time:0.077598\n",
      "[ Epoch 271, Val ] | Loss:3.36363 Time:0.005390\n",
      "[ Epoch 272, Train ] | Loss:3.12354 Time:0.077634\n",
      "[ Epoch 272, Val ] | Loss:3.36730 Time:0.005263\n",
      "[ Epoch 273, Train ] | Loss:3.12356 Time:0.082104\n",
      "[ Epoch 273, Val ] | Loss:3.36898 Time:0.005205\n",
      "[ Epoch 274, Train ] | Loss:3.12361 Time:0.077607\n",
      "[ Epoch 274, Val ] | Loss:3.36581 Time:0.005263\n",
      "[ Epoch 275, Train ] | Loss:3.12361 Time:0.077763\n",
      "[ Epoch 275, Val ] | Loss:3.36621 Time:0.005264\n",
      "[ Epoch 276, Train ] | Loss:3.12370 Time:0.077841\n",
      "[ Epoch 276, Val ] | Loss:3.36640 Time:0.005223\n",
      "[ Epoch 277, Train ] | Loss:3.12361 Time:0.077820\n",
      "[ Epoch 277, Val ] | Loss:3.37842 Time:0.005257\n",
      "[ Epoch 278, Train ] | Loss:3.12371 Time:0.077840\n",
      "[ Epoch 278, Val ] | Loss:3.37575 Time:0.005259\n",
      "[ Epoch 279, Train ] | Loss:3.12373 Time:0.077525\n",
      "[ Epoch 279, Val ] | Loss:3.37451 Time:0.005204\n",
      "[ Epoch 280, Train ] | Loss:3.12384 Time:0.077713\n",
      "[ Epoch 280, Val ] | Loss:3.37192 Time:0.005259\n",
      "[ Epoch 281, Train ] | Loss:3.12371 Time:0.079861\n",
      "[ Epoch 281, Val ] | Loss:3.37357 Time:0.005258\n",
      "[ Epoch 282, Train ] | Loss:3.12381 Time:0.079602\n",
      "[ Epoch 282, Val ] | Loss:3.37182 Time:0.005208\n",
      "[ Epoch 283, Train ] | Loss:3.12387 Time:0.080118\n",
      "[ Epoch 283, Val ] | Loss:3.37217 Time:0.005247\n",
      "[ Epoch 284, Train ] | Loss:3.12382 Time:0.079977\n",
      "[ Epoch 284, Val ] | Loss:3.37076 Time:0.005276\n",
      "[ Epoch 285, Train ] | Loss:3.12381 Time:0.079448\n",
      "[ Epoch 285, Val ] | Loss:3.38538 Time:0.005212\n",
      "[ Epoch 286, Train ] | Loss:3.12388 Time:0.077438\n",
      "[ Epoch 286, Val ] | Loss:3.37776 Time:0.005271\n",
      "[ Epoch 287, Train ] | Loss:3.12387 Time:0.079808\n",
      "[ Epoch 287, Val ] | Loss:3.38293 Time:0.005270\n",
      "[ Epoch 288, Train ] | Loss:3.12384 Time:0.079264\n",
      "[ Epoch 288, Val ] | Loss:3.39569 Time:0.005206\n",
      "[ Epoch 289, Train ] | Loss:3.12384 Time:0.080024\n",
      "[ Epoch 289, Val ] | Loss:3.38513 Time:0.005255\n",
      "[ Epoch 290, Train ] | Loss:3.12383 Time:0.077479\n",
      "[ Epoch 290, Val ] | Loss:3.37095 Time:0.005256\n",
      "[ Epoch 291, Train ] | Loss:3.12387 Time:0.077582\n",
      "[ Epoch 291, Val ] | Loss:3.37011 Time:0.005200\n",
      "[ Epoch 292, Train ] | Loss:3.12387 Time:0.077499\n",
      "[ Epoch 292, Val ] | Loss:3.37789 Time:0.005214\n",
      "[ Epoch 293, Train ] | Loss:3.12372 Time:0.077157\n",
      "[ Epoch 293, Val ] | Loss:3.37945 Time:0.005259\n",
      "[ Epoch 294, Train ] | Loss:3.12378 Time:0.076965\n",
      "[ Epoch 294, Val ] | Loss:3.37585 Time:0.005179\n",
      "[ Epoch 295, Train ] | Loss:3.12389 Time:0.077173\n",
      "[ Epoch 295, Val ] | Loss:3.38850 Time:0.005252\n",
      "[ Epoch 296, Train ] | Loss:3.12415 Time:0.077236\n",
      "[ Epoch 296, Val ] | Loss:3.39219 Time:0.005263\n",
      "[ Epoch 297, Train ] | Loss:3.12452 Time:0.082098\n",
      "[ Epoch 297, Val ] | Loss:3.39813 Time:0.005212\n",
      "[ Epoch 298, Train ] | Loss:3.12497 Time:0.077175\n",
      "[ Epoch 298, Val ] | Loss:3.40490 Time:0.005256\n",
      "[ Epoch 299, Train ] | Loss:3.12484 Time:0.077220\n",
      "[ Epoch 299, Val ] | Loss:3.40226 Time:0.005270\n",
      "[ Epoch 300, Train ] | Loss:3.12437 Time:0.077214\n",
      "[ Epoch 300, Val ] | Loss:3.39026 Time:0.005192\n",
      "Finish training!\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41578dc2-4163-4be3-b36e-0a2f5a822b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin testing\n",
      "Test loss: 3.2688485781351724\n",
      "——————Test——————\n",
      "           class precision  recall f1-score support\n",
      "0              0    0.7143  1.0000   0.8333       5\n",
      "1              1    1.0000  0.4000   0.5714       5\n",
      "2              2    1.0000  1.0000   1.0000       5\n",
      "3              3    1.0000  1.0000   1.0000       5\n",
      "4              4    1.0000  1.0000   1.0000       5\n",
      "..           ...       ...     ...      ...     ...\n",
      "58            58    0.8333  1.0000   0.9091       5\n",
      "59            59    1.0000  1.0000   1.0000       5\n",
      "60      accuracy                     0.8967     300\n",
      "61     macro avg    0.9139  0.8967   0.8901     300\n",
      "62  weighted avg    0.9139  0.8967   0.8901     300\n",
      "\n",
      "[63 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dd6342-13f6-4a64-adc9-b409a290d4f1",
   "metadata": {},
   "source": [
    "# 7.4 Different number of training samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ff80366-f711-4195-99ec-e0381aef57d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "num_classes = 50\n",
    "num_samples_train = 13\n",
    "num_samples_test = 7\n",
    "seed = 2022\n",
    "data_folder = './omniglot_resized'\n",
    "train_image, train_label, test_image, test_label = LoadData(num_classes, num_samples_train, \n",
    "                                                            num_samples_test, seed, data_folder)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_image, train_label, \n",
    "                                                  test_size=0.1, random_state=2022)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_data = ImgDataset(x_train.reshape(-1,1,28,28), y_train)\n",
    "val_data = ImgDataset(x_val.reshape(-1,1,28,28), y_val)\n",
    "test_data = ImgDataset(test_image.reshape(-1,1,28,28), test_label)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5568d93d-e699-47f9-bca9-8ee1a4c6016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cnn_13 = CNN_classifier()\n",
    "epochs = 300\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4\n",
    "optimizer = Adam(new_cnn_13.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_dir = './output'\n",
    "model_name = 'new_cnn_13'\n",
    "random_seed = 2022\n",
    "trainer = Trainer(model=new_cnn_13, \n",
    "                  epochs=epochs, \n",
    "                  train_dataloader=train_loader, \n",
    "                  val_dataloader=val_loader, \n",
    "                  test_dataloader=test_loader,\n",
    "                  criterion=criterion, \n",
    "                  optimizer=optimizer, \n",
    "                  lr=learning_rate, \n",
    "                  device=device, \n",
    "                  model_dir=model_dir, \n",
    "                  model_name=model_name, \n",
    "                  random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ede0260-9da1-49cc-b104-820a11eb1b03",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameters: 1761714, trainable parameters: 1761714\n",
      "\n",
      "        initial learning rate: 0.001, weight_decay: 0.0001,\n",
      "        total epochs: 300\n",
      "        \n",
      "Begin training, total epochs: 300\n",
      "[ Epoch 1, Train ] | Loss:3.88535 Time:0.071225\n",
      "[ Epoch 1, Val ] | Loss:3.91613 Time:0.005352\n",
      "save model with val loss 3.916\n",
      "[ Epoch 2, Train ] | Loss:3.58974 Time:0.069795\n",
      "[ Epoch 2, Val ] | Loss:4.06912 Time:0.005303\n",
      "[ Epoch 3, Train ] | Loss:3.17657 Time:0.069445\n",
      "[ Epoch 3, Val ] | Loss:4.96181 Time:0.005302\n",
      "[ Epoch 4, Train ] | Loss:2.62504 Time:0.068820\n",
      "[ Epoch 4, Val ] | Loss:6.22088 Time:0.005391\n",
      "[ Epoch 5, Train ] | Loss:2.10355 Time:0.069124\n",
      "[ Epoch 5, Val ] | Loss:7.56059 Time:0.005394\n",
      "[ Epoch 6, Train ] | Loss:1.56572 Time:0.068961\n",
      "[ Epoch 6, Val ] | Loss:7.30250 Time:0.005322\n",
      "[ Epoch 7, Train ] | Loss:1.09606 Time:0.069783\n",
      "[ Epoch 7, Val ] | Loss:7.04209 Time:0.005386\n",
      "[ Epoch 8, Train ] | Loss:0.83326 Time:0.069438\n",
      "[ Epoch 8, Val ] | Loss:8.90721 Time:0.005512\n",
      "[ Epoch 9, Train ] | Loss:0.61370 Time:0.070686\n",
      "[ Epoch 9, Val ] | Loss:1.23719 Time:0.005328\n",
      "save model with val loss 1.237\n",
      "[ Epoch 10, Train ] | Loss:0.41407 Time:0.070895\n",
      "[ Epoch 10, Val ] | Loss:2.60514 Time:0.005287\n",
      "[ Epoch 11, Train ] | Loss:0.28322 Time:0.068885\n",
      "[ Epoch 11, Val ] | Loss:1.42658 Time:0.005383\n",
      "[ Epoch 12, Train ] | Loss:0.18578 Time:0.069176\n",
      "[ Epoch 12, Val ] | Loss:0.68857 Time:0.005315\n",
      "save model with val loss 0.689\n",
      "[ Epoch 13, Train ] | Loss:0.12286 Time:0.070358\n",
      "[ Epoch 13, Val ] | Loss:1.48441 Time:0.005315\n",
      "[ Epoch 14, Train ] | Loss:0.09239 Time:0.068737\n",
      "[ Epoch 14, Val ] | Loss:0.58131 Time:0.005389\n",
      "save model with val loss 0.581\n",
      "[ Epoch 15, Train ] | Loss:0.06125 Time:0.068772\n",
      "[ Epoch 15, Val ] | Loss:0.57375 Time:0.005253\n",
      "save model with val loss 0.574\n",
      "[ Epoch 16, Train ] | Loss:0.04233 Time:0.069328\n",
      "[ Epoch 16, Val ] | Loss:0.64960 Time:0.005307\n",
      "[ Epoch 17, Train ] | Loss:0.03169 Time:0.069410\n",
      "[ Epoch 17, Val ] | Loss:0.35814 Time:0.005382\n",
      "save model with val loss 0.358\n",
      "[ Epoch 18, Train ] | Loss:0.02453 Time:0.069205\n",
      "[ Epoch 18, Val ] | Loss:0.43169 Time:0.005278\n",
      "[ Epoch 19, Train ] | Loss:0.02465 Time:0.069295\n",
      "[ Epoch 19, Val ] | Loss:0.53087 Time:0.005376\n",
      "[ Epoch 20, Train ] | Loss:0.01539 Time:0.068395\n",
      "[ Epoch 20, Val ] | Loss:0.34160 Time:0.005381\n",
      "save model with val loss 0.342\n",
      "[ Epoch 21, Train ] | Loss:0.01214 Time:0.068943\n",
      "[ Epoch 21, Val ] | Loss:0.32679 Time:0.005258\n",
      "save model with val loss 0.327\n",
      "[ Epoch 22, Train ] | Loss:0.01643 Time:0.071036\n",
      "[ Epoch 22, Val ] | Loss:0.47597 Time:0.005327\n",
      "[ Epoch 23, Train ] | Loss:0.01502 Time:0.068656\n",
      "[ Epoch 23, Val ] | Loss:0.50919 Time:0.005397\n",
      "[ Epoch 24, Train ] | Loss:0.01265 Time:0.069161\n",
      "[ Epoch 24, Val ] | Loss:0.32970 Time:0.005310\n",
      "[ Epoch 25, Train ] | Loss:0.00879 Time:0.069162\n",
      "[ Epoch 25, Val ] | Loss:0.29195 Time:0.005386\n",
      "save model with val loss 0.292\n",
      "[ Epoch 26, Train ] | Loss:0.00928 Time:0.070795\n",
      "[ Epoch 26, Val ] | Loss:0.34649 Time:0.005313\n",
      "[ Epoch 27, Train ] | Loss:0.00677 Time:0.068533\n",
      "[ Epoch 27, Val ] | Loss:0.42126 Time:0.005313\n",
      "[ Epoch 28, Train ] | Loss:0.00710 Time:0.068617\n",
      "[ Epoch 28, Val ] | Loss:0.32956 Time:0.005389\n",
      "[ Epoch 29, Train ] | Loss:0.00434 Time:0.068700\n",
      "[ Epoch 29, Val ] | Loss:0.31990 Time:0.005400\n",
      "[ Epoch 30, Train ] | Loss:0.00610 Time:0.068259\n",
      "[ Epoch 30, Val ] | Loss:0.23315 Time:0.005319\n",
      "save model with val loss 0.233\n",
      "[ Epoch 31, Train ] | Loss:0.00660 Time:0.072607\n",
      "[ Epoch 31, Val ] | Loss:0.21233 Time:0.005209\n",
      "save model with val loss 0.212\n",
      "[ Epoch 32, Train ] | Loss:0.00368 Time:0.070606\n",
      "[ Epoch 32, Val ] | Loss:0.26020 Time:0.005315\n",
      "[ Epoch 33, Train ] | Loss:0.00614 Time:0.069845\n",
      "[ Epoch 33, Val ] | Loss:0.23373 Time:0.005369\n",
      "[ Epoch 34, Train ] | Loss:0.00490 Time:0.068863\n",
      "[ Epoch 34, Val ] | Loss:0.39994 Time:0.005401\n",
      "[ Epoch 35, Train ] | Loss:0.00539 Time:0.067780\n",
      "[ Epoch 35, Val ] | Loss:0.29982 Time:0.005331\n",
      "[ Epoch 36, Train ] | Loss:0.00316 Time:0.068300\n",
      "[ Epoch 36, Val ] | Loss:0.22622 Time:0.005390\n",
      "[ Epoch 37, Train ] | Loss:0.00326 Time:0.069147\n",
      "[ Epoch 37, Val ] | Loss:0.21848 Time:0.004775\n",
      "[ Epoch 38, Train ] | Loss:0.00338 Time:0.070611\n",
      "[ Epoch 38, Val ] | Loss:0.23802 Time:0.005332\n",
      "[ Epoch 39, Train ] | Loss:0.00348 Time:0.070969\n",
      "[ Epoch 39, Val ] | Loss:0.24620 Time:0.005395\n",
      "[ Epoch 40, Train ] | Loss:0.00237 Time:0.068157\n",
      "[ Epoch 40, Val ] | Loss:0.26209 Time:0.005393\n",
      "[ Epoch 41, Train ] | Loss:0.00254 Time:0.070839\n",
      "[ Epoch 41, Val ] | Loss:0.27677 Time:0.005327\n",
      "[ Epoch 42, Train ] | Loss:0.00206 Time:0.070832\n",
      "[ Epoch 42, Val ] | Loss:0.28174 Time:0.005394\n",
      "[ Epoch 43, Train ] | Loss:0.00246 Time:0.069591\n",
      "[ Epoch 43, Val ] | Loss:0.26716 Time:0.005396\n",
      "[ Epoch 44, Train ] | Loss:0.00235 Time:0.070355\n",
      "[ Epoch 44, Val ] | Loss:0.28973 Time:0.005325\n",
      "[ Epoch 45, Train ] | Loss:0.00135 Time:0.070425\n",
      "[ Epoch 45, Val ] | Loss:0.25511 Time:0.005380\n",
      "[ Epoch 46, Train ] | Loss:0.00211 Time:0.070835\n",
      "[ Epoch 46, Val ] | Loss:0.20980 Time:0.005402\n",
      "save model with val loss 0.210\n",
      "[ Epoch 47, Train ] | Loss:0.00204 Time:0.070475\n",
      "[ Epoch 47, Val ] | Loss:0.21463 Time:0.005263\n",
      "[ Epoch 48, Train ] | Loss:0.00148 Time:0.070905\n",
      "[ Epoch 48, Val ] | Loss:0.26919 Time:0.005369\n",
      "[ Epoch 49, Train ] | Loss:0.00213 Time:0.070019\n",
      "[ Epoch 49, Val ] | Loss:0.27007 Time:0.005389\n",
      "[ Epoch 50, Train ] | Loss:0.00191 Time:0.070240\n",
      "[ Epoch 50, Val ] | Loss:0.24888 Time:0.005311\n",
      "[ Epoch 51, Train ] | Loss:0.00161 Time:0.070261\n",
      "[ Epoch 51, Val ] | Loss:0.24902 Time:0.005389\n",
      "[ Epoch 52, Train ] | Loss:0.00356 Time:0.070367\n",
      "[ Epoch 52, Val ] | Loss:0.37802 Time:0.005405\n",
      "[ Epoch 53, Train ] | Loss:0.00246 Time:0.070690\n",
      "[ Epoch 53, Val ] | Loss:0.41854 Time:0.005329\n",
      "[ Epoch 54, Train ] | Loss:0.00209 Time:0.070447\n",
      "[ Epoch 54, Val ] | Loss:0.56807 Time:0.005484\n",
      "[ Epoch 55, Train ] | Loss:0.00438 Time:0.070709\n",
      "[ Epoch 55, Val ] | Loss:0.48803 Time:0.005378\n",
      "[ Epoch 56, Train ] | Loss:0.00219 Time:0.070827\n",
      "[ Epoch 56, Val ] | Loss:0.46831 Time:0.005349\n",
      "[ Epoch 57, Train ] | Loss:0.00299 Time:0.070150\n",
      "[ Epoch 57, Val ] | Loss:0.45438 Time:0.005405\n",
      "[ Epoch 58, Train ] | Loss:0.00234 Time:0.070562\n",
      "[ Epoch 58, Val ] | Loss:0.34418 Time:0.005377\n",
      "[ Epoch 59, Train ] | Loss:0.00252 Time:0.070058\n",
      "[ Epoch 59, Val ] | Loss:0.30913 Time:0.005310\n",
      "[ Epoch 60, Train ] | Loss:0.00254 Time:0.070405\n",
      "[ Epoch 60, Val ] | Loss:0.32741 Time:0.005380\n",
      "[ Epoch 61, Train ] | Loss:0.00254 Time:0.070633\n",
      "[ Epoch 61, Val ] | Loss:0.28952 Time:0.005385\n",
      "[ Epoch 62, Train ] | Loss:0.00279 Time:0.070725\n",
      "[ Epoch 62, Val ] | Loss:0.23084 Time:0.005343\n",
      "[ Epoch 63, Train ] | Loss:0.00160 Time:0.069727\n",
      "[ Epoch 63, Val ] | Loss:0.27096 Time:0.005390\n",
      "[ Epoch 64, Train ] | Loss:0.00152 Time:0.070583\n",
      "[ Epoch 64, Val ] | Loss:0.33528 Time:0.005385\n",
      "[ Epoch 65, Train ] | Loss:0.00176 Time:0.070735\n",
      "[ Epoch 65, Val ] | Loss:0.33299 Time:0.005334\n",
      "[ Epoch 66, Train ] | Loss:0.00188 Time:0.070457\n",
      "[ Epoch 66, Val ] | Loss:0.32360 Time:0.005393\n",
      "[ Epoch 67, Train ] | Loss:0.00119 Time:0.070753\n",
      "[ Epoch 67, Val ] | Loss:0.33854 Time:0.005386\n",
      "[ Epoch 68, Train ] | Loss:0.00151 Time:0.070291\n",
      "[ Epoch 68, Val ] | Loss:0.31982 Time:0.005332\n",
      "[ Epoch 69, Train ] | Loss:0.00117 Time:0.070760\n",
      "[ Epoch 69, Val ] | Loss:0.30285 Time:0.005399\n",
      "[ Epoch 70, Train ] | Loss:0.00116 Time:0.070281\n",
      "[ Epoch 70, Val ] | Loss:0.33060 Time:0.005394\n",
      "[ Epoch 71, Train ] | Loss:0.00089 Time:0.070512\n",
      "[ Epoch 71, Val ] | Loss:0.35739 Time:0.005343\n",
      "[ Epoch 72, Train ] | Loss:0.00071 Time:0.070229\n",
      "[ Epoch 72, Val ] | Loss:0.40387 Time:0.005388\n",
      "[ Epoch 73, Train ] | Loss:0.00083 Time:0.070173\n",
      "[ Epoch 73, Val ] | Loss:0.40538 Time:0.005389\n",
      "[ Epoch 74, Train ] | Loss:0.00081 Time:0.070118\n",
      "[ Epoch 74, Val ] | Loss:0.37034 Time:0.005330\n",
      "[ Epoch 75, Train ] | Loss:0.00119 Time:0.069930\n",
      "[ Epoch 75, Val ] | Loss:0.34273 Time:0.005383\n",
      "[ Epoch 76, Train ] | Loss:0.00086 Time:0.070785\n",
      "[ Epoch 76, Val ] | Loss:0.30916 Time:0.005397\n",
      "[ Epoch 77, Train ] | Loss:0.00087 Time:0.070303\n",
      "[ Epoch 77, Val ] | Loss:0.27983 Time:0.005314\n",
      "[ Epoch 78, Train ] | Loss:0.00069 Time:0.070413\n",
      "[ Epoch 78, Val ] | Loss:0.27700 Time:0.005393\n",
      "[ Epoch 79, Train ] | Loss:0.00410 Time:0.070512\n",
      "[ Epoch 79, Val ] | Loss:0.42686 Time:0.005379\n",
      "[ Epoch 80, Train ] | Loss:0.00391 Time:0.070030\n",
      "[ Epoch 80, Val ] | Loss:0.97099 Time:0.005346\n",
      "[ Epoch 81, Train ] | Loss:0.01777 Time:0.070162\n",
      "[ Epoch 81, Val ] | Loss:2.16334 Time:0.005403\n",
      "[ Epoch 82, Train ] | Loss:0.01687 Time:0.069914\n",
      "[ Epoch 82, Val ] | Loss:0.65259 Time:0.005389\n",
      "[ Epoch 83, Train ] | Loss:0.02023 Time:0.069812\n",
      "[ Epoch 83, Val ] | Loss:0.58194 Time:0.005324\n",
      "[ Epoch 84, Train ] | Loss:0.03790 Time:0.069936\n",
      "[ Epoch 84, Val ] | Loss:1.83350 Time:0.005388\n",
      "[ Epoch 85, Train ] | Loss:0.06558 Time:0.070259\n",
      "[ Epoch 85, Val ] | Loss:1.54463 Time:0.005391\n",
      "[ Epoch 86, Train ] | Loss:0.08203 Time:0.070321\n",
      "[ Epoch 86, Val ] | Loss:1.58304 Time:0.005339\n",
      "[ Epoch 87, Train ] | Loss:0.10097 Time:0.070279\n",
      "[ Epoch 87, Val ] | Loss:2.75716 Time:0.005386\n",
      "[ Epoch 88, Train ] | Loss:0.09344 Time:0.069548\n",
      "[ Epoch 88, Val ] | Loss:1.37936 Time:0.005389\n",
      "[ Epoch 89, Train ] | Loss:0.04981 Time:0.069989\n",
      "[ Epoch 89, Val ] | Loss:1.52014 Time:0.005502\n",
      "[ Epoch 90, Train ] | Loss:0.07102 Time:0.069358\n",
      "[ Epoch 90, Val ] | Loss:0.85294 Time:0.005392\n",
      "[ Epoch 91, Train ] | Loss:0.04774 Time:0.069981\n",
      "[ Epoch 91, Val ] | Loss:1.39152 Time:0.005395\n",
      "[ Epoch 92, Train ] | Loss:0.02272 Time:0.070248\n",
      "[ Epoch 92, Val ] | Loss:1.48820 Time:0.005327\n",
      "[ Epoch 93, Train ] | Loss:0.02202 Time:0.070171\n",
      "[ Epoch 93, Val ] | Loss:0.67623 Time:0.005399\n",
      "[ Epoch 94, Train ] | Loss:0.02271 Time:0.069987\n",
      "[ Epoch 94, Val ] | Loss:0.32485 Time:0.005384\n",
      "[ Epoch 95, Train ] | Loss:0.01972 Time:0.069445\n",
      "[ Epoch 95, Val ] | Loss:0.55772 Time:0.005313\n",
      "[ Epoch 96, Train ] | Loss:0.00970 Time:0.070374\n",
      "[ Epoch 96, Val ] | Loss:0.51778 Time:0.005383\n",
      "[ Epoch 97, Train ] | Loss:0.01614 Time:0.069525\n",
      "[ Epoch 97, Val ] | Loss:0.37323 Time:0.005390\n",
      "[ Epoch 98, Train ] | Loss:0.00980 Time:0.069687\n",
      "[ Epoch 98, Val ] | Loss:0.48435 Time:0.005323\n",
      "[ Epoch 99, Train ] | Loss:0.01313 Time:0.070315\n",
      "[ Epoch 99, Val ] | Loss:0.61423 Time:0.005384\n",
      "[ Epoch 100, Train ] | Loss:0.01516 Time:0.069470\n",
      "[ Epoch 100, Val ] | Loss:0.41035 Time:0.005397\n",
      "[ Epoch 101, Train ] | Loss:0.01663 Time:0.069774\n",
      "[ Epoch 101, Val ] | Loss:0.85719 Time:0.005322\n",
      "[ Epoch 102, Train ] | Loss:0.01391 Time:0.069503\n",
      "[ Epoch 102, Val ] | Loss:0.64840 Time:0.005490\n",
      "[ Epoch 103, Train ] | Loss:0.00866 Time:0.069788\n",
      "[ Epoch 103, Val ] | Loss:0.79136 Time:0.005388\n",
      "[ Epoch 104, Train ] | Loss:0.00403 Time:0.069592\n",
      "[ Epoch 104, Val ] | Loss:0.52531 Time:0.005342\n",
      "[ Epoch 105, Train ] | Loss:0.00662 Time:0.070019\n",
      "[ Epoch 105, Val ] | Loss:0.37490 Time:0.005391\n",
      "[ Epoch 106, Train ] | Loss:0.00396 Time:0.069933\n",
      "[ Epoch 106, Val ] | Loss:0.67032 Time:0.005495\n",
      "[ Epoch 107, Train ] | Loss:0.00263 Time:0.070041\n",
      "[ Epoch 107, Val ] | Loss:0.35423 Time:0.005342\n",
      "[ Epoch 108, Train ] | Loss:0.00403 Time:0.069743\n",
      "[ Epoch 108, Val ] | Loss:0.39614 Time:0.005395\n",
      "[ Epoch 109, Train ] | Loss:0.00238 Time:0.069731\n",
      "[ Epoch 109, Val ] | Loss:0.49317 Time:0.005388\n",
      "[ Epoch 110, Train ] | Loss:0.00457 Time:0.069654\n",
      "[ Epoch 110, Val ] | Loss:0.33990 Time:0.005503\n",
      "[ Epoch 111, Train ] | Loss:0.00280 Time:0.069678\n",
      "[ Epoch 111, Val ] | Loss:0.63282 Time:0.005392\n",
      "[ Epoch 112, Train ] | Loss:0.00155 Time:0.071078\n",
      "[ Epoch 112, Val ] | Loss:0.65168 Time:0.005374\n",
      "[ Epoch 113, Train ] | Loss:0.00330 Time:0.069366\n",
      "[ Epoch 113, Val ] | Loss:0.34668 Time:0.005344\n",
      "[ Epoch 114, Train ] | Loss:0.00222 Time:0.069575\n",
      "[ Epoch 114, Val ] | Loss:0.47052 Time:0.005398\n",
      "[ Epoch 115, Train ] | Loss:0.00208 Time:0.069383\n",
      "[ Epoch 115, Val ] | Loss:0.37525 Time:0.005383\n",
      "[ Epoch 116, Train ] | Loss:0.00135 Time:0.069316\n",
      "[ Epoch 116, Val ] | Loss:0.35764 Time:0.005327\n",
      "[ Epoch 117, Train ] | Loss:0.00155 Time:0.069253\n",
      "[ Epoch 117, Val ] | Loss:0.33202 Time:0.005388\n",
      "[ Epoch 118, Train ] | Loss:0.00225 Time:0.069994\n",
      "[ Epoch 118, Val ] | Loss:0.25861 Time:0.005385\n",
      "[ Epoch 119, Train ] | Loss:0.00053 Time:0.070185\n",
      "[ Epoch 119, Val ] | Loss:0.25782 Time:0.005358\n",
      "[ Epoch 120, Train ] | Loss:0.00057 Time:0.069468\n",
      "[ Epoch 120, Val ] | Loss:0.26333 Time:0.005392\n",
      "[ Epoch 121, Train ] | Loss:0.00226 Time:0.069210\n",
      "[ Epoch 121, Val ] | Loss:0.34964 Time:0.005409\n",
      "[ Epoch 122, Train ] | Loss:0.00081 Time:0.069721\n",
      "[ Epoch 122, Val ] | Loss:0.42579 Time:0.005346\n",
      "[ Epoch 123, Train ] | Loss:0.00158 Time:0.069298\n",
      "[ Epoch 123, Val ] | Loss:0.40813 Time:0.005403\n",
      "[ Epoch 124, Train ] | Loss:0.00053 Time:0.069530\n",
      "[ Epoch 124, Val ] | Loss:0.37185 Time:0.005390\n",
      "[ Epoch 125, Train ] | Loss:0.00065 Time:0.069403\n",
      "[ Epoch 125, Val ] | Loss:0.32771 Time:0.005330\n",
      "[ Epoch 126, Train ] | Loss:0.00082 Time:0.069641\n",
      "[ Epoch 126, Val ] | Loss:0.28907 Time:0.005395\n",
      "[ Epoch 127, Train ] | Loss:0.00054 Time:0.068625\n",
      "[ Epoch 127, Val ] | Loss:0.26649 Time:0.005374\n",
      "[ Epoch 128, Train ] | Loss:0.00029 Time:0.069888\n",
      "[ Epoch 128, Val ] | Loss:0.25186 Time:0.005325\n",
      "[ Epoch 129, Train ] | Loss:0.00048 Time:0.069311\n",
      "[ Epoch 129, Val ] | Loss:0.23559 Time:0.005413\n",
      "[ Epoch 130, Train ] | Loss:0.00029 Time:0.069240\n",
      "[ Epoch 130, Val ] | Loss:0.23157 Time:0.005396\n",
      "[ Epoch 131, Train ] | Loss:0.00029 Time:0.071206\n",
      "[ Epoch 131, Val ] | Loss:0.22584 Time:0.005344\n",
      "[ Epoch 132, Train ] | Loss:0.00023 Time:0.069561\n",
      "[ Epoch 132, Val ] | Loss:0.21588 Time:0.005397\n",
      "[ Epoch 133, Train ] | Loss:0.00021 Time:0.068918\n",
      "[ Epoch 133, Val ] | Loss:0.21419 Time:0.005393\n",
      "[ Epoch 134, Train ] | Loss:0.00026 Time:0.068877\n",
      "[ Epoch 134, Val ] | Loss:0.21283 Time:0.005321\n",
      "[ Epoch 135, Train ] | Loss:0.00034 Time:0.068924\n",
      "[ Epoch 135, Val ] | Loss:0.21693 Time:0.005402\n",
      "[ Epoch 136, Train ] | Loss:0.00028 Time:0.069387\n",
      "[ Epoch 136, Val ] | Loss:0.21995 Time:0.005406\n",
      "[ Epoch 137, Train ] | Loss:0.00026 Time:0.069050\n",
      "[ Epoch 137, Val ] | Loss:0.22340 Time:0.005334\n",
      "[ Epoch 138, Train ] | Loss:0.00023 Time:0.069533\n",
      "[ Epoch 138, Val ] | Loss:0.22390 Time:0.005384\n",
      "[ Epoch 139, Train ] | Loss:0.00023 Time:0.069382\n",
      "[ Epoch 139, Val ] | Loss:0.22378 Time:0.005402\n",
      "[ Epoch 140, Train ] | Loss:0.00015 Time:0.069615\n",
      "[ Epoch 140, Val ] | Loss:0.22643 Time:0.005323\n",
      "[ Epoch 141, Train ] | Loss:0.00023 Time:0.069039\n",
      "[ Epoch 141, Val ] | Loss:0.22239 Time:0.005401\n",
      "[ Epoch 142, Train ] | Loss:0.00024 Time:0.068977\n",
      "[ Epoch 142, Val ] | Loss:0.22529 Time:0.005394\n",
      "[ Epoch 143, Train ] | Loss:0.00024 Time:0.068952\n",
      "[ Epoch 143, Val ] | Loss:0.23278 Time:0.005332\n",
      "[ Epoch 144, Train ] | Loss:0.00027 Time:0.069230\n",
      "[ Epoch 144, Val ] | Loss:0.23123 Time:0.005394\n",
      "[ Epoch 145, Train ] | Loss:0.00024 Time:0.069143\n",
      "[ Epoch 145, Val ] | Loss:0.22578 Time:0.005379\n",
      "[ Epoch 146, Train ] | Loss:0.00020 Time:0.068722\n",
      "[ Epoch 146, Val ] | Loss:0.21823 Time:0.005329\n",
      "[ Epoch 147, Train ] | Loss:0.00020 Time:0.069459\n",
      "[ Epoch 147, Val ] | Loss:0.21576 Time:0.005391\n",
      "[ Epoch 148, Train ] | Loss:0.00029 Time:0.068697\n",
      "[ Epoch 148, Val ] | Loss:0.21487 Time:0.005401\n",
      "[ Epoch 149, Train ] | Loss:0.00014 Time:0.069194\n",
      "[ Epoch 149, Val ] | Loss:0.21252 Time:0.005332\n",
      "[ Epoch 150, Train ] | Loss:0.00038 Time:0.069152\n",
      "[ Epoch 150, Val ] | Loss:0.21304 Time:0.005401\n",
      "[ Epoch 151, Train ] | Loss:0.00023 Time:0.069103\n",
      "[ Epoch 151, Val ] | Loss:0.20510 Time:0.005390\n",
      "save model with val loss 0.205\n",
      "[ Epoch 152, Train ] | Loss:0.00021 Time:0.072601\n",
      "[ Epoch 152, Val ] | Loss:0.20319 Time:0.005225\n",
      "save model with val loss 0.203\n",
      "[ Epoch 153, Train ] | Loss:0.00037 Time:0.069047\n",
      "[ Epoch 153, Val ] | Loss:0.20029 Time:0.005296\n",
      "save model with val loss 0.200\n",
      "[ Epoch 154, Train ] | Loss:0.00018 Time:0.069142\n",
      "[ Epoch 154, Val ] | Loss:0.20165 Time:0.005297\n",
      "[ Epoch 155, Train ] | Loss:0.00032 Time:0.070279\n",
      "[ Epoch 155, Val ] | Loss:0.20666 Time:0.005314\n",
      "[ Epoch 156, Train ] | Loss:0.00016 Time:0.068789\n",
      "[ Epoch 156, Val ] | Loss:0.21364 Time:0.005398\n",
      "[ Epoch 157, Train ] | Loss:0.00019 Time:0.068924\n",
      "[ Epoch 157, Val ] | Loss:0.22120 Time:0.005389\n",
      "[ Epoch 158, Train ] | Loss:0.00017 Time:0.068934\n",
      "[ Epoch 158, Val ] | Loss:0.23452 Time:0.005342\n",
      "[ Epoch 159, Train ] | Loss:0.00025 Time:0.069311\n",
      "[ Epoch 159, Val ] | Loss:0.24310 Time:0.005401\n",
      "[ Epoch 160, Train ] | Loss:0.00018 Time:0.069128\n",
      "[ Epoch 160, Val ] | Loss:0.23083 Time:0.005382\n",
      "[ Epoch 161, Train ] | Loss:0.00018 Time:0.069374\n",
      "[ Epoch 161, Val ] | Loss:0.23600 Time:0.005325\n",
      "[ Epoch 162, Train ] | Loss:0.00027 Time:0.069223\n",
      "[ Epoch 162, Val ] | Loss:0.22721 Time:0.005402\n",
      "[ Epoch 163, Train ] | Loss:0.00021 Time:0.069049\n",
      "[ Epoch 163, Val ] | Loss:0.22126 Time:0.005401\n",
      "[ Epoch 164, Train ] | Loss:0.00021 Time:0.069045\n",
      "[ Epoch 164, Val ] | Loss:0.21818 Time:0.005338\n",
      "[ Epoch 165, Train ] | Loss:0.00030 Time:0.069162\n",
      "[ Epoch 165, Val ] | Loss:0.21479 Time:0.005384\n",
      "[ Epoch 166, Train ] | Loss:0.00015 Time:0.069144\n",
      "[ Epoch 166, Val ] | Loss:0.21592 Time:0.005384\n",
      "[ Epoch 167, Train ] | Loss:0.00019 Time:0.068791\n",
      "[ Epoch 167, Val ] | Loss:0.21480 Time:0.005353\n",
      "[ Epoch 168, Train ] | Loss:0.00046 Time:0.068786\n",
      "[ Epoch 168, Val ] | Loss:0.22736 Time:0.005508\n",
      "[ Epoch 169, Train ] | Loss:0.00012 Time:0.068812\n",
      "[ Epoch 169, Val ] | Loss:0.22862 Time:0.005396\n",
      "[ Epoch 170, Train ] | Loss:0.00022 Time:0.068516\n",
      "[ Epoch 170, Val ] | Loss:0.22139 Time:0.005333\n",
      "[ Epoch 171, Train ] | Loss:0.00014 Time:0.069410\n",
      "[ Epoch 171, Val ] | Loss:0.21597 Time:0.005385\n",
      "[ Epoch 172, Train ] | Loss:0.00021 Time:0.069188\n",
      "[ Epoch 172, Val ] | Loss:0.21027 Time:0.005395\n",
      "[ Epoch 173, Train ] | Loss:0.00024 Time:0.069065\n",
      "[ Epoch 173, Val ] | Loss:0.21019 Time:0.005324\n",
      "[ Epoch 174, Train ] | Loss:0.00014 Time:0.068709\n",
      "[ Epoch 174, Val ] | Loss:0.21451 Time:0.005388\n",
      "[ Epoch 175, Train ] | Loss:0.00012 Time:0.068858\n",
      "[ Epoch 175, Val ] | Loss:0.22128 Time:0.005393\n",
      "[ Epoch 176, Train ] | Loss:0.00023 Time:0.068595\n",
      "[ Epoch 176, Val ] | Loss:0.23285 Time:0.005334\n",
      "[ Epoch 177, Train ] | Loss:0.00017 Time:0.068650\n",
      "[ Epoch 177, Val ] | Loss:0.23090 Time:0.005394\n",
      "[ Epoch 178, Train ] | Loss:0.00062 Time:0.068868\n",
      "[ Epoch 178, Val ] | Loss:0.21965 Time:0.005399\n",
      "[ Epoch 179, Train ] | Loss:0.00018 Time:0.069076\n",
      "[ Epoch 179, Val ] | Loss:0.22345 Time:0.005323\n",
      "[ Epoch 180, Train ] | Loss:0.00016 Time:0.068766\n",
      "[ Epoch 180, Val ] | Loss:0.23373 Time:0.005379\n",
      "[ Epoch 181, Train ] | Loss:0.00014 Time:0.068588\n",
      "[ Epoch 181, Val ] | Loss:0.23768 Time:0.005398\n",
      "[ Epoch 182, Train ] | Loss:0.00023 Time:0.068580\n",
      "[ Epoch 182, Val ] | Loss:0.24122 Time:0.005316\n",
      "[ Epoch 183, Train ] | Loss:0.00022 Time:0.068626\n",
      "[ Epoch 183, Val ] | Loss:0.24028 Time:0.005397\n",
      "[ Epoch 184, Train ] | Loss:0.00016 Time:0.068214\n",
      "[ Epoch 184, Val ] | Loss:0.24904 Time:0.005401\n",
      "[ Epoch 185, Train ] | Loss:0.00046 Time:0.070846\n",
      "[ Epoch 185, Val ] | Loss:0.24616 Time:0.005325\n",
      "[ Epoch 186, Train ] | Loss:0.00040 Time:0.068788\n",
      "[ Epoch 186, Val ] | Loss:0.24097 Time:0.005394\n",
      "[ Epoch 187, Train ] | Loss:0.00022 Time:0.068732\n",
      "[ Epoch 187, Val ] | Loss:0.24358 Time:0.005397\n",
      "[ Epoch 188, Train ] | Loss:0.00066 Time:0.068921\n",
      "[ Epoch 188, Val ] | Loss:0.23318 Time:0.005313\n",
      "[ Epoch 189, Train ] | Loss:0.00014 Time:0.071248\n",
      "[ Epoch 189, Val ] | Loss:0.22045 Time:0.005373\n",
      "[ Epoch 190, Train ] | Loss:0.00019 Time:0.070402\n",
      "[ Epoch 190, Val ] | Loss:0.23278 Time:0.005396\n",
      "[ Epoch 191, Train ] | Loss:0.00063 Time:0.070787\n",
      "[ Epoch 191, Val ] | Loss:0.25720 Time:0.005335\n",
      "[ Epoch 192, Train ] | Loss:0.00017 Time:0.071094\n",
      "[ Epoch 192, Val ] | Loss:0.27428 Time:0.005389\n",
      "[ Epoch 193, Train ] | Loss:0.00024 Time:0.070795\n",
      "[ Epoch 193, Val ] | Loss:0.26416 Time:0.005391\n",
      "[ Epoch 194, Train ] | Loss:0.00015 Time:0.070166\n",
      "[ Epoch 194, Val ] | Loss:0.24634 Time:0.005320\n",
      "[ Epoch 195, Train ] | Loss:0.00014 Time:0.070528\n",
      "[ Epoch 195, Val ] | Loss:0.23318 Time:0.005391\n",
      "[ Epoch 196, Train ] | Loss:0.00014 Time:0.070287\n",
      "[ Epoch 196, Val ] | Loss:0.22724 Time:0.005399\n",
      "[ Epoch 197, Train ] | Loss:0.00015 Time:0.070451\n",
      "[ Epoch 197, Val ] | Loss:0.22029 Time:0.005330\n",
      "[ Epoch 198, Train ] | Loss:0.00025 Time:0.070435\n",
      "[ Epoch 198, Val ] | Loss:0.21608 Time:0.005386\n",
      "[ Epoch 199, Train ] | Loss:0.00020 Time:0.070475\n",
      "[ Epoch 199, Val ] | Loss:0.21578 Time:0.005391\n",
      "[ Epoch 200, Train ] | Loss:0.00017 Time:0.070492\n",
      "[ Epoch 200, Val ] | Loss:0.22108 Time:0.005329\n",
      "[ Epoch 201, Train ] | Loss:0.00017 Time:0.070569\n",
      "[ Epoch 201, Val ] | Loss:0.22621 Time:0.005390\n",
      "[ Epoch 202, Train ] | Loss:0.00015 Time:0.070559\n",
      "[ Epoch 202, Val ] | Loss:0.22680 Time:0.005388\n",
      "[ Epoch 203, Train ] | Loss:0.00010 Time:0.070636\n",
      "[ Epoch 203, Val ] | Loss:0.22626 Time:0.005332\n",
      "[ Epoch 204, Train ] | Loss:0.00018 Time:0.070997\n",
      "[ Epoch 204, Val ] | Loss:0.22807 Time:0.005387\n",
      "[ Epoch 205, Train ] | Loss:0.00012 Time:0.070287\n",
      "[ Epoch 205, Val ] | Loss:0.22915 Time:0.005392\n",
      "[ Epoch 206, Train ] | Loss:0.00020 Time:0.070619\n",
      "[ Epoch 206, Val ] | Loss:0.23041 Time:0.005320\n",
      "[ Epoch 207, Train ] | Loss:0.00010 Time:0.070499\n",
      "[ Epoch 207, Val ] | Loss:0.24015 Time:0.005382\n",
      "[ Epoch 208, Train ] | Loss:0.00013 Time:0.070475\n",
      "[ Epoch 208, Val ] | Loss:0.24449 Time:0.005392\n",
      "[ Epoch 209, Train ] | Loss:0.00012 Time:0.070461\n",
      "[ Epoch 209, Val ] | Loss:0.24453 Time:0.005328\n",
      "[ Epoch 210, Train ] | Loss:0.00011 Time:0.068577\n",
      "[ Epoch 210, Val ] | Loss:0.24661 Time:0.005392\n",
      "[ Epoch 211, Train ] | Loss:0.00011 Time:0.070381\n",
      "[ Epoch 211, Val ] | Loss:0.23905 Time:0.005384\n",
      "[ Epoch 212, Train ] | Loss:0.00023 Time:0.070755\n",
      "[ Epoch 212, Val ] | Loss:0.24568 Time:0.005323\n",
      "[ Epoch 213, Train ] | Loss:0.00016 Time:0.070119\n",
      "[ Epoch 213, Val ] | Loss:0.25194 Time:0.005390\n",
      "[ Epoch 214, Train ] | Loss:0.00010 Time:0.069902\n",
      "[ Epoch 214, Val ] | Loss:0.25202 Time:0.005394\n",
      "[ Epoch 215, Train ] | Loss:0.00010 Time:0.069537\n",
      "[ Epoch 215, Val ] | Loss:0.24690 Time:0.005321\n",
      "[ Epoch 216, Train ] | Loss:0.00027 Time:0.070213\n",
      "[ Epoch 216, Val ] | Loss:0.24718 Time:0.005383\n",
      "[ Epoch 217, Train ] | Loss:0.00013 Time:0.070041\n",
      "[ Epoch 217, Val ] | Loss:0.24870 Time:0.005379\n",
      "[ Epoch 218, Train ] | Loss:0.00014 Time:0.070222\n",
      "[ Epoch 218, Val ] | Loss:0.24451 Time:0.005317\n",
      "[ Epoch 219, Train ] | Loss:0.00015 Time:0.069941\n",
      "[ Epoch 219, Val ] | Loss:0.23552 Time:0.005376\n",
      "[ Epoch 220, Train ] | Loss:0.00011 Time:0.069905\n",
      "[ Epoch 220, Val ] | Loss:0.23389 Time:0.005387\n",
      "[ Epoch 221, Train ] | Loss:0.00012 Time:0.069708\n",
      "[ Epoch 221, Val ] | Loss:0.23410 Time:0.005320\n",
      "[ Epoch 222, Train ] | Loss:0.00018 Time:0.070261\n",
      "[ Epoch 222, Val ] | Loss:0.23426 Time:0.005394\n",
      "[ Epoch 223, Train ] | Loss:0.00018 Time:0.069959\n",
      "[ Epoch 223, Val ] | Loss:0.23322 Time:0.005388\n",
      "[ Epoch 224, Train ] | Loss:0.00011 Time:0.069887\n",
      "[ Epoch 224, Val ] | Loss:0.23268 Time:0.005323\n",
      "[ Epoch 225, Train ] | Loss:0.00017 Time:0.069960\n",
      "[ Epoch 225, Val ] | Loss:0.23436 Time:0.005394\n",
      "[ Epoch 226, Train ] | Loss:0.00012 Time:0.069993\n",
      "[ Epoch 226, Val ] | Loss:0.23194 Time:0.005388\n",
      "[ Epoch 227, Train ] | Loss:0.00018 Time:0.069782\n",
      "[ Epoch 227, Val ] | Loss:0.23093 Time:0.005326\n",
      "[ Epoch 228, Train ] | Loss:0.00016 Time:0.069871\n",
      "[ Epoch 228, Val ] | Loss:0.23845 Time:0.005370\n",
      "[ Epoch 229, Train ] | Loss:0.00011 Time:0.070196\n",
      "[ Epoch 229, Val ] | Loss:0.23800 Time:0.005403\n",
      "[ Epoch 230, Train ] | Loss:0.00013 Time:0.069920\n",
      "[ Epoch 230, Val ] | Loss:0.23956 Time:0.005349\n",
      "[ Epoch 231, Train ] | Loss:0.00014 Time:0.070768\n",
      "[ Epoch 231, Val ] | Loss:0.24458 Time:0.005392\n",
      "[ Epoch 232, Train ] | Loss:0.00013 Time:0.070140\n",
      "[ Epoch 232, Val ] | Loss:0.24360 Time:0.005395\n",
      "[ Epoch 233, Train ] | Loss:0.00014 Time:0.069931\n",
      "[ Epoch 233, Val ] | Loss:0.23951 Time:0.005321\n",
      "[ Epoch 234, Train ] | Loss:0.00022 Time:0.070195\n",
      "[ Epoch 234, Val ] | Loss:0.24532 Time:0.005392\n",
      "[ Epoch 235, Train ] | Loss:0.00013 Time:0.070123\n",
      "[ Epoch 235, Val ] | Loss:0.24648 Time:0.005497\n",
      "[ Epoch 236, Train ] | Loss:0.00038 Time:0.069692\n",
      "[ Epoch 236, Val ] | Loss:0.24328 Time:0.005322\n",
      "[ Epoch 237, Train ] | Loss:0.00021 Time:0.070269\n",
      "[ Epoch 237, Val ] | Loss:0.23650 Time:0.005393\n",
      "[ Epoch 238, Train ] | Loss:0.00016 Time:0.070019\n",
      "[ Epoch 238, Val ] | Loss:0.23244 Time:0.005398\n",
      "[ Epoch 239, Train ] | Loss:0.00016 Time:0.069552\n",
      "[ Epoch 239, Val ] | Loss:0.23678 Time:0.005329\n",
      "[ Epoch 240, Train ] | Loss:0.00011 Time:0.069898\n",
      "[ Epoch 240, Val ] | Loss:0.23904 Time:0.005404\n",
      "[ Epoch 241, Train ] | Loss:0.00022 Time:0.070065\n",
      "[ Epoch 241, Val ] | Loss:0.24242 Time:0.005398\n",
      "[ Epoch 242, Train ] | Loss:0.00017 Time:0.072146\n",
      "[ Epoch 242, Val ] | Loss:0.23476 Time:0.005326\n",
      "[ Epoch 243, Train ] | Loss:0.00020 Time:0.069875\n",
      "[ Epoch 243, Val ] | Loss:0.23849 Time:0.005381\n",
      "[ Epoch 244, Train ] | Loss:0.00028 Time:0.070056\n",
      "[ Epoch 244, Val ] | Loss:0.24521 Time:0.005395\n",
      "[ Epoch 245, Train ] | Loss:0.00016 Time:0.070029\n",
      "[ Epoch 245, Val ] | Loss:0.24903 Time:0.005335\n",
      "[ Epoch 246, Train ] | Loss:0.00019 Time:0.070334\n",
      "[ Epoch 246, Val ] | Loss:0.24585 Time:0.005378\n",
      "[ Epoch 247, Train ] | Loss:0.00017 Time:0.070184\n",
      "[ Epoch 247, Val ] | Loss:0.24397 Time:0.005374\n",
      "[ Epoch 248, Train ] | Loss:0.00021 Time:0.069205\n",
      "[ Epoch 248, Val ] | Loss:0.22616 Time:0.005334\n",
      "[ Epoch 249, Train ] | Loss:0.00017 Time:0.069730\n",
      "[ Epoch 249, Val ] | Loss:0.22163 Time:0.005395\n",
      "[ Epoch 250, Train ] | Loss:0.00015 Time:0.069723\n",
      "[ Epoch 250, Val ] | Loss:0.22257 Time:0.005400\n",
      "[ Epoch 251, Train ] | Loss:0.00046 Time:0.069616\n",
      "[ Epoch 251, Val ] | Loss:0.24662 Time:0.005326\n",
      "[ Epoch 252, Train ] | Loss:0.00034 Time:0.070640\n",
      "[ Epoch 252, Val ] | Loss:0.28123 Time:0.005497\n",
      "[ Epoch 253, Train ] | Loss:0.00029 Time:0.069926\n",
      "[ Epoch 253, Val ] | Loss:0.28891 Time:0.005387\n",
      "[ Epoch 254, Train ] | Loss:0.00035 Time:0.069143\n",
      "[ Epoch 254, Val ] | Loss:0.26878 Time:0.005313\n",
      "[ Epoch 255, Train ] | Loss:0.00024 Time:0.069844\n",
      "[ Epoch 255, Val ] | Loss:0.25014 Time:0.005384\n",
      "[ Epoch 256, Train ] | Loss:0.00028 Time:0.069719\n",
      "[ Epoch 256, Val ] | Loss:0.24476 Time:0.005383\n",
      "[ Epoch 257, Train ] | Loss:0.00018 Time:0.069481\n",
      "[ Epoch 257, Val ] | Loss:0.23537 Time:0.005323\n",
      "[ Epoch 258, Train ] | Loss:0.00023 Time:0.069840\n",
      "[ Epoch 258, Val ] | Loss:0.22353 Time:0.005385\n",
      "[ Epoch 259, Train ] | Loss:0.00018 Time:0.070109\n",
      "[ Epoch 259, Val ] | Loss:0.21661 Time:0.005386\n",
      "[ Epoch 260, Train ] | Loss:0.00011 Time:0.069031\n",
      "[ Epoch 260, Val ] | Loss:0.21802 Time:0.005326\n",
      "[ Epoch 261, Train ] | Loss:0.00023 Time:0.069023\n",
      "[ Epoch 261, Val ] | Loss:0.22402 Time:0.005378\n",
      "[ Epoch 262, Train ] | Loss:0.00023 Time:0.069347\n",
      "[ Epoch 262, Val ] | Loss:0.23588 Time:0.005390\n",
      "[ Epoch 263, Train ] | Loss:0.00020 Time:0.070184\n",
      "[ Epoch 263, Val ] | Loss:0.24902 Time:0.005338\n",
      "[ Epoch 264, Train ] | Loss:0.00022 Time:0.069619\n",
      "[ Epoch 264, Val ] | Loss:0.25942 Time:0.005400\n",
      "[ Epoch 265, Train ] | Loss:0.00022 Time:0.070081\n",
      "[ Epoch 265, Val ] | Loss:0.26287 Time:0.005393\n",
      "[ Epoch 266, Train ] | Loss:0.00018 Time:0.072124\n",
      "[ Epoch 266, Val ] | Loss:0.25702 Time:0.005324\n",
      "[ Epoch 267, Train ] | Loss:0.00017 Time:0.070006\n",
      "[ Epoch 267, Val ] | Loss:0.25639 Time:0.005426\n",
      "[ Epoch 268, Train ] | Loss:0.00031 Time:0.069954\n",
      "[ Epoch 268, Val ] | Loss:0.31022 Time:0.005381\n",
      "[ Epoch 269, Train ] | Loss:0.00021 Time:0.069854\n",
      "[ Epoch 269, Val ] | Loss:0.32774 Time:0.005327\n",
      "[ Epoch 270, Train ] | Loss:0.00024 Time:0.068901\n",
      "[ Epoch 270, Val ] | Loss:0.29901 Time:0.005391\n",
      "[ Epoch 271, Train ] | Loss:0.00015 Time:0.069043\n",
      "[ Epoch 271, Val ] | Loss:0.24207 Time:0.005391\n",
      "[ Epoch 272, Train ] | Loss:0.00025 Time:0.069270\n",
      "[ Epoch 272, Val ] | Loss:0.22151 Time:0.005324\n",
      "[ Epoch 273, Train ] | Loss:0.00021 Time:0.069935\n",
      "[ Epoch 273, Val ] | Loss:0.22402 Time:0.005393\n",
      "[ Epoch 274, Train ] | Loss:0.00012 Time:0.069393\n",
      "[ Epoch 274, Val ] | Loss:0.22950 Time:0.005373\n",
      "[ Epoch 275, Train ] | Loss:0.00013 Time:0.069862\n",
      "[ Epoch 275, Val ] | Loss:0.23665 Time:0.005343\n",
      "[ Epoch 276, Train ] | Loss:0.00048 Time:0.069326\n",
      "[ Epoch 276, Val ] | Loss:0.23377 Time:0.005398\n",
      "[ Epoch 277, Train ] | Loss:0.00017 Time:0.069597\n",
      "[ Epoch 277, Val ] | Loss:0.24432 Time:0.005512\n",
      "[ Epoch 278, Train ] | Loss:0.00037 Time:0.068560\n",
      "[ Epoch 278, Val ] | Loss:0.25113 Time:0.005337\n",
      "[ Epoch 279, Train ] | Loss:0.00023 Time:0.068969\n",
      "[ Epoch 279, Val ] | Loss:0.24998 Time:0.005385\n",
      "[ Epoch 280, Train ] | Loss:0.00027 Time:0.069854\n",
      "[ Epoch 280, Val ] | Loss:0.24293 Time:0.005384\n",
      "[ Epoch 281, Train ] | Loss:0.00033 Time:0.069387\n",
      "[ Epoch 281, Val ] | Loss:0.23961 Time:0.005329\n",
      "[ Epoch 282, Train ] | Loss:0.00020 Time:0.068820\n",
      "[ Epoch 282, Val ] | Loss:0.26510 Time:0.005392\n",
      "[ Epoch 283, Train ] | Loss:0.00029 Time:0.069079\n",
      "[ Epoch 283, Val ] | Loss:0.25670 Time:0.005398\n",
      "[ Epoch 284, Train ] | Loss:0.00024 Time:0.068683\n",
      "[ Epoch 284, Val ] | Loss:0.27625 Time:0.005332\n",
      "[ Epoch 285, Train ] | Loss:0.00015 Time:0.069545\n",
      "[ Epoch 285, Val ] | Loss:0.27993 Time:0.005391\n",
      "[ Epoch 286, Train ] | Loss:0.00016 Time:0.069027\n",
      "[ Epoch 286, Val ] | Loss:0.27740 Time:0.005388\n",
      "[ Epoch 287, Train ] | Loss:0.00016 Time:0.069203\n",
      "[ Epoch 287, Val ] | Loss:0.25705 Time:0.005325\n",
      "[ Epoch 288, Train ] | Loss:0.00027 Time:0.069353\n",
      "[ Epoch 288, Val ] | Loss:0.27343 Time:0.005382\n",
      "[ Epoch 289, Train ] | Loss:0.00025 Time:0.068872\n",
      "[ Epoch 289, Val ] | Loss:0.26263 Time:0.004970\n",
      "[ Epoch 290, Train ] | Loss:0.00013 Time:0.069275\n",
      "[ Epoch 290, Val ] | Loss:0.22367 Time:0.005321\n",
      "[ Epoch 291, Train ] | Loss:0.00016 Time:0.069526\n",
      "[ Epoch 291, Val ] | Loss:0.20094 Time:0.005399\n",
      "[ Epoch 292, Train ] | Loss:0.00022 Time:0.069237\n",
      "[ Epoch 292, Val ] | Loss:0.18634 Time:0.005406\n",
      "save model with val loss 0.186\n",
      "[ Epoch 293, Train ] | Loss:0.00033 Time:0.070053\n",
      "[ Epoch 293, Val ] | Loss:0.17630 Time:0.005244\n",
      "save model with val loss 0.176\n",
      "[ Epoch 294, Train ] | Loss:0.00024 Time:0.070739\n",
      "[ Epoch 294, Val ] | Loss:0.19190 Time:0.005300\n",
      "[ Epoch 295, Train ] | Loss:0.00028 Time:0.069030\n",
      "[ Epoch 295, Val ] | Loss:0.21263 Time:0.005402\n",
      "[ Epoch 296, Train ] | Loss:0.00022 Time:0.069456\n",
      "[ Epoch 296, Val ] | Loss:0.23477 Time:0.005307\n",
      "[ Epoch 297, Train ] | Loss:0.00020 Time:0.069262\n",
      "[ Epoch 297, Val ] | Loss:0.24754 Time:0.005388\n",
      "[ Epoch 298, Train ] | Loss:0.00026 Time:0.068972\n",
      "[ Epoch 298, Val ] | Loss:0.28058 Time:0.005384\n",
      "[ Epoch 299, Train ] | Loss:0.00018 Time:0.068914\n",
      "[ Epoch 299, Val ] | Loss:0.31471 Time:0.005306\n",
      "[ Epoch 300, Train ] | Loss:0.00024 Time:0.068976\n",
      "[ Epoch 300, Val ] | Loss:0.30649 Time:0.005393\n",
      "Finish training!\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d27a50a9-77e2-4e03-ac1f-7f26018f30d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin testing\n",
      "Test loss: 0.2912433370947838\n",
      "——————Test——————\n",
      "           class precision  recall f1-score support\n",
      "0              0    0.8333  0.7143   0.7692       7\n",
      "1              1    1.0000  0.7143   0.8333       7\n",
      "2              2    1.0000  1.0000   1.0000       7\n",
      "3              3    1.0000  1.0000   1.0000       7\n",
      "4              4    1.0000  0.7143   0.8333       7\n",
      "5              5    0.8750  1.0000   0.9333       7\n",
      "6              6    1.0000  0.8571   0.9231       7\n",
      "7              7    1.0000  0.8571   0.9231       7\n",
      "8              8    1.0000  1.0000   1.0000       7\n",
      "9              9    1.0000  1.0000   1.0000       7\n",
      "10            10    0.8750  1.0000   0.9333       7\n",
      "11            11    0.7143  0.7143   0.7143       7\n",
      "12            12    0.7778  1.0000   0.8750       7\n",
      "13            13    1.0000  0.8571   0.9231       7\n",
      "14            14    1.0000  1.0000   1.0000       7\n",
      "15            15    0.8750  1.0000   0.9333       7\n",
      "16            16    1.0000  1.0000   1.0000       7\n",
      "17            17    1.0000  1.0000   1.0000       7\n",
      "18            18    1.0000  0.8571   0.9231       7\n",
      "19            19    1.0000  1.0000   1.0000       7\n",
      "20            20    1.0000  1.0000   1.0000       7\n",
      "21            21    1.0000  0.8571   0.9231       7\n",
      "22            22    0.7143  0.7143   0.7143       7\n",
      "23            23    0.7143  0.7143   0.7143       7\n",
      "24            24    0.8750  1.0000   0.9333       7\n",
      "25            25    1.0000  1.0000   1.0000       7\n",
      "26            26    1.0000  1.0000   1.0000       7\n",
      "27            27    1.0000  1.0000   1.0000       7\n",
      "28            28    0.7778  1.0000   0.8750       7\n",
      "29            29    1.0000  1.0000   1.0000       7\n",
      "30            30    1.0000  1.0000   1.0000       7\n",
      "31            31    0.8750  1.0000   0.9333       7\n",
      "32            32    0.8571  0.8571   0.8571       7\n",
      "33            33    1.0000  1.0000   1.0000       7\n",
      "34            34    1.0000  1.0000   1.0000       7\n",
      "35            35    1.0000  1.0000   1.0000       7\n",
      "36            36    0.8750  1.0000   0.9333       7\n",
      "37            37    1.0000  1.0000   1.0000       7\n",
      "38            38    0.8750  1.0000   0.9333       7\n",
      "39            39    0.8571  0.8571   0.8571       7\n",
      "40            40    1.0000  0.8571   0.9231       7\n",
      "41            41    1.0000  1.0000   1.0000       7\n",
      "42            42    1.0000  0.8571   0.9231       7\n",
      "43            43    1.0000  1.0000   1.0000       7\n",
      "44            44    0.8750  1.0000   0.9333       7\n",
      "45            45    0.8750  1.0000   0.9333       7\n",
      "46            46    0.8000  0.5714   0.6667       7\n",
      "47            47    1.0000  1.0000   1.0000       7\n",
      "48            48    1.0000  1.0000   1.0000       7\n",
      "49            49    0.8750  1.0000   0.9333       7\n",
      "50      accuracy                     0.9314     350\n",
      "51     macro avg    0.9359  0.9314   0.9301     350\n",
      "52  weighted avg    0.9359  0.9314   0.9301     350\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2cdb0f4a-432c-48be-bbe5-bac07b165438",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_13 = Vanilla_CNN()\n",
    "epochs = 300\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4\n",
    "optimizer = Adam(cnn_13.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_dir = './output'\n",
    "model_name = 'cnn_13'\n",
    "random_seed = 2022\n",
    "trainer = Trainer(model=cnn_13, \n",
    "                  epochs=epochs, \n",
    "                  train_dataloader=train_loader, \n",
    "                  val_dataloader=val_loader, \n",
    "                  test_dataloader=test_loader,\n",
    "                  criterion=criterion, \n",
    "                  optimizer=optimizer, \n",
    "                  lr=learning_rate, \n",
    "                  device=device, \n",
    "                  model_dir=model_dir, \n",
    "                  model_name=model_name, \n",
    "                  random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52ff6d7d-c7fb-48d8-8f9a-566e0ed5cd3f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameters: 1698194, trainable parameters: 1698194\n",
      "\n",
      "        initial learning rate: 0.001, weight_decay: 0.0001,\n",
      "        total epochs: 300\n",
      "        \n",
      "Begin training, total epochs: 300\n",
      "[ Epoch 1, Train ] | Loss:3.84800 Time:0.060621\n",
      "[ Epoch 1, Val ] | Loss:3.91467 Time:0.005083\n",
      "save model with val loss 3.915\n",
      "[ Epoch 2, Train ] | Loss:3.65622 Time:0.058178\n",
      "[ Epoch 2, Val ] | Loss:3.92964 Time:0.005040\n",
      "[ Epoch 3, Train ] | Loss:3.53620 Time:0.057484\n",
      "[ Epoch 3, Val ] | Loss:3.93535 Time:0.005038\n",
      "[ Epoch 4, Train ] | Loss:3.44576 Time:0.055184\n",
      "[ Epoch 4, Val ] | Loss:3.92990 Time:0.005119\n",
      "[ Epoch 5, Train ] | Loss:3.34780 Time:0.055123\n",
      "[ Epoch 5, Val ] | Loss:3.92090 Time:0.005105\n",
      "[ Epoch 6, Train ] | Loss:3.27684 Time:0.054637\n",
      "[ Epoch 6, Val ] | Loss:3.90070 Time:0.005118\n",
      "save model with val loss 3.901\n",
      "[ Epoch 7, Train ] | Loss:3.20508 Time:0.056441\n",
      "[ Epoch 7, Val ] | Loss:3.86263 Time:0.004992\n",
      "save model with val loss 3.863\n",
      "[ Epoch 8, Train ] | Loss:3.14542 Time:0.055779\n",
      "[ Epoch 8, Val ] | Loss:3.71709 Time:0.005056\n",
      "save model with val loss 3.717\n",
      "[ Epoch 9, Train ] | Loss:3.07904 Time:0.055399\n",
      "[ Epoch 9, Val ] | Loss:3.60044 Time:0.005049\n",
      "save model with val loss 3.600\n",
      "[ Epoch 10, Train ] | Loss:3.02859 Time:0.055986\n",
      "[ Epoch 10, Val ] | Loss:3.38766 Time:0.004988\n",
      "save model with val loss 3.388\n",
      "[ Epoch 11, Train ] | Loss:2.99134 Time:0.056014\n",
      "[ Epoch 11, Val ] | Loss:3.35827 Time:0.005040\n",
      "save model with val loss 3.358\n",
      "[ Epoch 12, Train ] | Loss:2.97434 Time:0.055545\n",
      "[ Epoch 12, Val ] | Loss:3.29592 Time:0.005052\n",
      "save model with val loss 3.296\n",
      "[ Epoch 13, Train ] | Loss:2.96185 Time:0.057288\n",
      "[ Epoch 13, Val ] | Loss:3.28807 Time:0.004994\n",
      "save model with val loss 3.288\n",
      "[ Epoch 14, Train ] | Loss:2.95476 Time:0.055655\n",
      "[ Epoch 14, Val ] | Loss:3.24538 Time:0.005042\n",
      "save model with val loss 3.245\n",
      "[ Epoch 15, Train ] | Loss:2.95123 Time:0.055877\n",
      "[ Epoch 15, Val ] | Loss:3.22992 Time:0.005043\n",
      "save model with val loss 3.230\n",
      "[ Epoch 16, Train ] | Loss:2.94957 Time:0.055464\n",
      "[ Epoch 16, Val ] | Loss:3.22446 Time:0.004989\n",
      "save model with val loss 3.224\n",
      "[ Epoch 17, Train ] | Loss:2.94868 Time:0.057950\n",
      "[ Epoch 17, Val ] | Loss:3.21053 Time:0.005034\n",
      "save model with val loss 3.211\n",
      "[ Epoch 18, Train ] | Loss:2.94824 Time:0.055442\n",
      "[ Epoch 18, Val ] | Loss:3.20796 Time:0.005045\n",
      "save model with val loss 3.208\n",
      "[ Epoch 19, Train ] | Loss:2.94796 Time:0.057504\n",
      "[ Epoch 19, Val ] | Loss:3.20638 Time:0.004989\n",
      "save model with val loss 3.206\n",
      "[ Epoch 20, Train ] | Loss:2.94757 Time:0.055691\n",
      "[ Epoch 20, Val ] | Loss:3.19944 Time:0.005041\n",
      "save model with val loss 3.199\n",
      "[ Epoch 21, Train ] | Loss:2.94750 Time:0.055538\n",
      "[ Epoch 21, Val ] | Loss:3.18747 Time:0.005041\n",
      "save model with val loss 3.187\n",
      "[ Epoch 22, Train ] | Loss:2.94733 Time:0.060228\n",
      "[ Epoch 22, Val ] | Loss:3.17912 Time:0.004992\n",
      "save model with val loss 3.179\n",
      "[ Epoch 23, Train ] | Loss:2.94724 Time:0.060044\n",
      "[ Epoch 23, Val ] | Loss:3.17880 Time:0.005031\n",
      "save model with val loss 3.179\n",
      "[ Epoch 24, Train ] | Loss:2.94719 Time:0.056583\n",
      "[ Epoch 24, Val ] | Loss:3.18376 Time:0.005055\n",
      "[ Epoch 25, Train ] | Loss:2.94716 Time:0.054644\n",
      "[ Epoch 25, Val ] | Loss:3.18567 Time:0.005018\n",
      "[ Epoch 26, Train ] | Loss:2.94708 Time:0.054810\n",
      "[ Epoch 26, Val ] | Loss:3.18482 Time:0.005126\n",
      "[ Epoch 27, Train ] | Loss:2.94695 Time:0.054862\n",
      "[ Epoch 27, Val ] | Loss:3.18106 Time:0.005120\n",
      "[ Epoch 28, Train ] | Loss:2.94699 Time:0.054390\n",
      "[ Epoch 28, Val ] | Loss:3.17926 Time:0.005129\n",
      "[ Epoch 29, Train ] | Loss:2.94692 Time:0.057151\n",
      "[ Epoch 29, Val ] | Loss:3.18128 Time:0.005049\n",
      "[ Epoch 30, Train ] | Loss:2.94684 Time:0.054519\n",
      "[ Epoch 30, Val ] | Loss:3.18556 Time:0.005123\n",
      "[ Epoch 31, Train ] | Loss:2.94680 Time:0.054813\n",
      "[ Epoch 31, Val ] | Loss:3.18710 Time:0.005140\n",
      "[ Epoch 32, Train ] | Loss:2.94677 Time:0.054773\n",
      "[ Epoch 32, Val ] | Loss:3.18821 Time:0.005128\n",
      "[ Epoch 33, Train ] | Loss:2.94678 Time:0.056871\n",
      "[ Epoch 33, Val ] | Loss:3.18669 Time:0.005044\n",
      "[ Epoch 34, Train ] | Loss:2.94685 Time:0.054747\n",
      "[ Epoch 34, Val ] | Loss:3.18473 Time:0.005118\n",
      "[ Epoch 35, Train ] | Loss:2.94675 Time:0.055035\n",
      "[ Epoch 35, Val ] | Loss:3.18120 Time:0.005117\n",
      "[ Epoch 36, Train ] | Loss:2.94680 Time:0.057168\n",
      "[ Epoch 36, Val ] | Loss:3.18192 Time:0.005111\n",
      "[ Epoch 37, Train ] | Loss:2.94670 Time:0.056851\n",
      "[ Epoch 37, Val ] | Loss:3.18677 Time:0.005051\n",
      "[ Epoch 38, Train ] | Loss:2.94672 Time:0.056802\n",
      "[ Epoch 38, Val ] | Loss:3.18836 Time:0.005105\n",
      "[ Epoch 39, Train ] | Loss:2.94667 Time:0.057053\n",
      "[ Epoch 39, Val ] | Loss:3.18827 Time:0.005129\n",
      "[ Epoch 40, Train ] | Loss:2.94675 Time:0.056753\n",
      "[ Epoch 40, Val ] | Loss:3.18791 Time:0.005120\n",
      "[ Epoch 41, Train ] | Loss:2.94671 Time:0.061625\n",
      "[ Epoch 41, Val ] | Loss:3.18771 Time:0.005062\n",
      "[ Epoch 42, Train ] | Loss:2.94670 Time:0.056719\n",
      "[ Epoch 42, Val ] | Loss:3.18850 Time:0.005109\n",
      "[ Epoch 43, Train ] | Loss:2.94672 Time:0.057022\n",
      "[ Epoch 43, Val ] | Loss:3.18776 Time:0.005120\n",
      "[ Epoch 44, Train ] | Loss:2.94669 Time:0.056765\n",
      "[ Epoch 44, Val ] | Loss:3.18640 Time:0.005106\n",
      "[ Epoch 45, Train ] | Loss:2.94672 Time:0.056708\n",
      "[ Epoch 45, Val ] | Loss:3.18828 Time:0.005063\n",
      "[ Epoch 46, Train ] | Loss:2.94674 Time:0.056688\n",
      "[ Epoch 46, Val ] | Loss:3.18958 Time:0.005115\n",
      "[ Epoch 47, Train ] | Loss:2.94669 Time:0.056027\n",
      "[ Epoch 47, Val ] | Loss:3.19123 Time:0.005118\n",
      "[ Epoch 48, Train ] | Loss:2.94670 Time:0.056971\n",
      "[ Epoch 48, Val ] | Loss:3.19055 Time:0.005122\n",
      "[ Epoch 49, Train ] | Loss:2.94663 Time:0.056607\n",
      "[ Epoch 49, Val ] | Loss:3.19111 Time:0.005047\n",
      "[ Epoch 50, Train ] | Loss:2.94662 Time:0.056880\n",
      "[ Epoch 50, Val ] | Loss:3.19265 Time:0.005224\n",
      "[ Epoch 51, Train ] | Loss:2.94665 Time:0.059208\n",
      "[ Epoch 51, Val ] | Loss:3.19451 Time:0.005124\n",
      "[ Epoch 52, Train ] | Loss:2.94668 Time:0.056633\n",
      "[ Epoch 52, Val ] | Loss:3.19325 Time:0.005131\n",
      "[ Epoch 53, Train ] | Loss:2.94669 Time:0.056895\n",
      "[ Epoch 53, Val ] | Loss:3.19024 Time:0.005152\n",
      "[ Epoch 54, Train ] | Loss:2.94666 Time:0.056224\n",
      "[ Epoch 54, Val ] | Loss:3.19185 Time:0.005124\n",
      "[ Epoch 55, Train ] | Loss:2.94665 Time:0.056559\n",
      "[ Epoch 55, Val ] | Loss:3.19891 Time:0.005134\n",
      "[ Epoch 56, Train ] | Loss:2.94668 Time:0.056002\n",
      "[ Epoch 56, Val ] | Loss:3.20596 Time:0.005125\n",
      "[ Epoch 57, Train ] | Loss:2.94663 Time:0.056866\n",
      "[ Epoch 57, Val ] | Loss:3.20008 Time:0.005063\n",
      "[ Epoch 58, Train ] | Loss:2.94661 Time:0.056803\n",
      "[ Epoch 58, Val ] | Loss:3.19291 Time:0.005114\n",
      "[ Epoch 59, Train ] | Loss:2.94663 Time:0.056469\n",
      "[ Epoch 59, Val ] | Loss:3.19349 Time:0.005128\n",
      "[ Epoch 60, Train ] | Loss:2.94667 Time:0.056765\n",
      "[ Epoch 60, Val ] | Loss:3.19798 Time:0.005126\n",
      "[ Epoch 61, Train ] | Loss:2.94665 Time:0.058690\n",
      "[ Epoch 61, Val ] | Loss:3.19602 Time:0.005051\n",
      "[ Epoch 62, Train ] | Loss:2.94663 Time:0.056782\n",
      "[ Epoch 62, Val ] | Loss:3.19810 Time:0.005115\n",
      "[ Epoch 63, Train ] | Loss:2.94658 Time:0.056086\n",
      "[ Epoch 63, Val ] | Loss:3.20093 Time:0.005112\n",
      "[ Epoch 64, Train ] | Loss:2.94661 Time:0.056352\n",
      "[ Epoch 64, Val ] | Loss:3.20111 Time:0.005105\n",
      "[ Epoch 65, Train ] | Loss:2.94660 Time:0.056158\n",
      "[ Epoch 65, Val ] | Loss:3.19649 Time:0.005062\n",
      "[ Epoch 66, Train ] | Loss:2.94659 Time:0.056700\n",
      "[ Epoch 66, Val ] | Loss:3.19922 Time:0.005119\n",
      "[ Epoch 67, Train ] | Loss:2.94667 Time:0.056641\n",
      "[ Epoch 67, Val ] | Loss:3.20203 Time:0.005109\n",
      "[ Epoch 68, Train ] | Loss:2.94663 Time:0.056141\n",
      "[ Epoch 68, Val ] | Loss:3.20437 Time:0.005118\n",
      "[ Epoch 69, Train ] | Loss:2.94666 Time:0.058845\n",
      "[ Epoch 69, Val ] | Loss:3.20684 Time:0.005054\n",
      "[ Epoch 70, Train ] | Loss:2.94668 Time:0.056339\n",
      "[ Epoch 70, Val ] | Loss:3.20846 Time:0.005128\n",
      "[ Epoch 71, Train ] | Loss:2.94659 Time:0.058473\n",
      "[ Epoch 71, Val ] | Loss:3.20849 Time:0.005113\n",
      "[ Epoch 72, Train ] | Loss:2.94662 Time:0.056484\n",
      "[ Epoch 72, Val ] | Loss:3.21426 Time:0.005132\n",
      "[ Epoch 73, Train ] | Loss:2.94657 Time:0.058754\n",
      "[ Epoch 73, Val ] | Loss:3.21910 Time:0.005054\n",
      "[ Epoch 74, Train ] | Loss:2.94662 Time:0.056051\n",
      "[ Epoch 74, Val ] | Loss:3.21207 Time:0.005119\n",
      "[ Epoch 75, Train ] | Loss:2.94665 Time:0.056553\n",
      "[ Epoch 75, Val ] | Loss:3.20975 Time:0.005128\n",
      "[ Epoch 76, Train ] | Loss:2.94665 Time:0.055953\n",
      "[ Epoch 76, Val ] | Loss:3.21480 Time:0.005055\n",
      "[ Epoch 77, Train ] | Loss:2.94662 Time:0.056272\n",
      "[ Epoch 77, Val ] | Loss:3.21848 Time:0.005122\n",
      "[ Epoch 78, Train ] | Loss:2.94667 Time:0.056499\n",
      "[ Epoch 78, Val ] | Loss:3.21602 Time:0.005103\n",
      "[ Epoch 79, Train ] | Loss:2.94665 Time:0.056206\n",
      "[ Epoch 79, Val ] | Loss:3.21264 Time:0.005108\n",
      "[ Epoch 80, Train ] | Loss:2.94670 Time:0.056258\n",
      "[ Epoch 80, Val ] | Loss:3.21411 Time:0.005048\n",
      "[ Epoch 81, Train ] | Loss:2.94662 Time:0.056254\n",
      "[ Epoch 81, Val ] | Loss:3.21207 Time:0.005108\n",
      "[ Epoch 82, Train ] | Loss:2.94664 Time:0.056484\n",
      "[ Epoch 82, Val ] | Loss:3.21933 Time:0.005130\n",
      "[ Epoch 83, Train ] | Loss:2.94665 Time:0.056165\n",
      "[ Epoch 83, Val ] | Loss:3.22562 Time:0.005110\n",
      "[ Epoch 84, Train ] | Loss:2.94659 Time:0.058416\n",
      "[ Epoch 84, Val ] | Loss:3.22298 Time:0.005045\n",
      "[ Epoch 85, Train ] | Loss:2.94658 Time:0.056176\n",
      "[ Epoch 85, Val ] | Loss:3.22398 Time:0.005117\n",
      "[ Epoch 86, Train ] | Loss:2.94662 Time:0.056128\n",
      "[ Epoch 86, Val ] | Loss:3.22539 Time:0.005133\n",
      "[ Epoch 87, Train ] | Loss:2.94661 Time:0.058841\n",
      "[ Epoch 87, Val ] | Loss:3.22493 Time:0.005118\n",
      "[ Epoch 88, Train ] | Loss:2.94653 Time:0.056050\n",
      "[ Epoch 88, Val ] | Loss:3.22080 Time:0.005053\n",
      "[ Epoch 89, Train ] | Loss:2.94656 Time:0.056350\n",
      "[ Epoch 89, Val ] | Loss:3.21774 Time:0.005102\n",
      "[ Epoch 90, Train ] | Loss:2.94659 Time:0.058431\n",
      "[ Epoch 90, Val ] | Loss:3.21846 Time:0.005128\n",
      "[ Epoch 91, Train ] | Loss:2.94662 Time:0.056028\n",
      "[ Epoch 91, Val ] | Loss:3.23045 Time:0.005114\n",
      "[ Epoch 92, Train ] | Loss:2.94658 Time:0.055932\n",
      "[ Epoch 92, Val ] | Loss:3.23224 Time:0.005060\n",
      "[ Epoch 93, Train ] | Loss:2.94668 Time:0.056019\n",
      "[ Epoch 93, Val ] | Loss:3.22940 Time:0.005237\n",
      "[ Epoch 94, Train ] | Loss:2.94665 Time:0.056731\n",
      "[ Epoch 94, Val ] | Loss:3.22698 Time:0.005118\n",
      "[ Epoch 95, Train ] | Loss:2.94660 Time:0.056491\n",
      "[ Epoch 95, Val ] | Loss:3.23255 Time:0.005119\n",
      "[ Epoch 96, Train ] | Loss:2.94668 Time:0.057587\n",
      "[ Epoch 96, Val ] | Loss:3.22316 Time:0.005058\n",
      "[ Epoch 97, Train ] | Loss:2.94666 Time:0.055440\n",
      "[ Epoch 97, Val ] | Loss:3.22131 Time:0.005123\n",
      "[ Epoch 98, Train ] | Loss:2.94663 Time:0.055695\n",
      "[ Epoch 98, Val ] | Loss:3.23387 Time:0.005113\n",
      "[ Epoch 99, Train ] | Loss:2.94666 Time:0.056205\n",
      "[ Epoch 99, Val ] | Loss:3.23887 Time:0.005111\n",
      "[ Epoch 100, Train ] | Loss:2.94667 Time:0.055419\n",
      "[ Epoch 100, Val ] | Loss:3.23715 Time:0.005062\n",
      "[ Epoch 101, Train ] | Loss:2.94669 Time:0.055620\n",
      "[ Epoch 101, Val ] | Loss:3.24316 Time:0.005130\n",
      "[ Epoch 102, Train ] | Loss:2.94673 Time:0.055919\n",
      "[ Epoch 102, Val ] | Loss:3.24847 Time:0.005132\n",
      "[ Epoch 103, Train ] | Loss:2.94663 Time:0.055905\n",
      "[ Epoch 103, Val ] | Loss:3.23418 Time:0.005105\n",
      "[ Epoch 104, Train ] | Loss:2.94666 Time:0.058379\n",
      "[ Epoch 104, Val ] | Loss:3.24418 Time:0.005065\n",
      "[ Epoch 105, Train ] | Loss:2.94670 Time:0.055819\n",
      "[ Epoch 105, Val ] | Loss:3.24292 Time:0.005121\n",
      "[ Epoch 106, Train ] | Loss:2.94670 Time:0.056141\n",
      "[ Epoch 106, Val ] | Loss:3.23466 Time:0.005117\n",
      "[ Epoch 107, Train ] | Loss:2.94668 Time:0.055852\n",
      "[ Epoch 107, Val ] | Loss:3.24659 Time:0.005118\n",
      "[ Epoch 108, Train ] | Loss:2.94672 Time:0.058060\n",
      "[ Epoch 108, Val ] | Loss:3.26129 Time:0.005060\n",
      "[ Epoch 109, Train ] | Loss:2.94676 Time:0.055581\n",
      "[ Epoch 109, Val ] | Loss:3.25248 Time:0.005107\n",
      "[ Epoch 110, Train ] | Loss:2.94670 Time:0.055830\n",
      "[ Epoch 110, Val ] | Loss:3.24222 Time:0.005118\n",
      "[ Epoch 111, Train ] | Loss:2.94673 Time:0.055797\n",
      "[ Epoch 111, Val ] | Loss:3.24312 Time:0.005117\n",
      "[ Epoch 112, Train ] | Loss:2.94670 Time:0.060359\n",
      "[ Epoch 112, Val ] | Loss:3.25769 Time:0.005054\n",
      "[ Epoch 113, Train ] | Loss:2.94676 Time:0.055777\n",
      "[ Epoch 113, Val ] | Loss:3.25353 Time:0.005133\n",
      "[ Epoch 114, Train ] | Loss:2.94669 Time:0.055470\n",
      "[ Epoch 114, Val ] | Loss:3.24777 Time:0.005222\n",
      "[ Epoch 115, Train ] | Loss:2.94668 Time:0.055419\n",
      "[ Epoch 115, Val ] | Loss:3.25892 Time:0.005116\n",
      "[ Epoch 116, Train ] | Loss:2.94670 Time:0.058168\n",
      "[ Epoch 116, Val ] | Loss:3.25314 Time:0.005051\n",
      "[ Epoch 117, Train ] | Loss:2.94669 Time:0.055966\n",
      "[ Epoch 117, Val ] | Loss:3.25398 Time:0.005117\n",
      "[ Epoch 118, Train ] | Loss:2.94676 Time:0.057998\n",
      "[ Epoch 118, Val ] | Loss:3.25138 Time:0.005122\n",
      "[ Epoch 119, Train ] | Loss:2.94677 Time:0.055959\n",
      "[ Epoch 119, Val ] | Loss:3.25332 Time:0.005114\n",
      "[ Epoch 120, Train ] | Loss:2.94690 Time:0.057806\n",
      "[ Epoch 120, Val ] | Loss:3.26413 Time:0.005084\n",
      "[ Epoch 121, Train ] | Loss:2.94686 Time:0.058024\n",
      "[ Epoch 121, Val ] | Loss:3.25679 Time:0.005100\n",
      "[ Epoch 122, Train ] | Loss:2.94685 Time:0.055920\n",
      "[ Epoch 122, Val ] | Loss:3.25742 Time:0.005134\n",
      "[ Epoch 123, Train ] | Loss:2.94682 Time:0.055614\n",
      "[ Epoch 123, Val ] | Loss:3.28352 Time:0.005116\n",
      "[ Epoch 124, Train ] | Loss:2.94679 Time:0.060130\n",
      "[ Epoch 124, Val ] | Loss:3.27177 Time:0.005069\n",
      "[ Epoch 125, Train ] | Loss:2.94678 Time:0.055611\n",
      "[ Epoch 125, Val ] | Loss:3.26500 Time:0.005118\n",
      "[ Epoch 126, Train ] | Loss:2.94678 Time:0.055622\n",
      "[ Epoch 126, Val ] | Loss:3.26357 Time:0.005119\n",
      "[ Epoch 127, Train ] | Loss:2.94685 Time:0.055586\n",
      "[ Epoch 127, Val ] | Loss:3.25260 Time:0.005105\n",
      "[ Epoch 128, Train ] | Loss:2.94684 Time:0.054968\n",
      "[ Epoch 128, Val ] | Loss:3.25561 Time:0.005063\n",
      "[ Epoch 129, Train ] | Loss:2.94678 Time:0.055529\n",
      "[ Epoch 129, Val ] | Loss:3.25180 Time:0.005116\n",
      "[ Epoch 130, Train ] | Loss:2.94685 Time:0.055752\n",
      "[ Epoch 130, Val ] | Loss:3.24825 Time:0.005117\n",
      "[ Epoch 131, Train ] | Loss:2.94677 Time:0.055508\n",
      "[ Epoch 131, Val ] | Loss:3.26233 Time:0.005131\n",
      "[ Epoch 132, Train ] | Loss:2.94688 Time:0.057676\n",
      "[ Epoch 132, Val ] | Loss:3.26972 Time:0.005053\n",
      "[ Epoch 133, Train ] | Loss:2.94691 Time:0.055761\n",
      "[ Epoch 133, Val ] | Loss:3.26735 Time:0.005111\n",
      "[ Epoch 134, Train ] | Loss:2.94693 Time:0.055175\n",
      "[ Epoch 134, Val ] | Loss:3.26624 Time:0.005098\n",
      "[ Epoch 135, Train ] | Loss:2.94682 Time:0.055743\n",
      "[ Epoch 135, Val ] | Loss:3.26831 Time:0.005109\n",
      "[ Epoch 136, Train ] | Loss:2.94694 Time:0.055415\n",
      "[ Epoch 136, Val ] | Loss:3.26269 Time:0.005059\n",
      "[ Epoch 137, Train ] | Loss:2.94691 Time:0.057549\n",
      "[ Epoch 137, Val ] | Loss:3.27494 Time:0.005229\n",
      "[ Epoch 138, Train ] | Loss:2.94684 Time:0.055701\n",
      "[ Epoch 138, Val ] | Loss:3.27641 Time:0.005114\n",
      "[ Epoch 139, Train ] | Loss:2.94690 Time:0.055423\n",
      "[ Epoch 139, Val ] | Loss:3.27122 Time:0.005128\n",
      "[ Epoch 140, Train ] | Loss:2.94673 Time:0.057750\n",
      "[ Epoch 140, Val ] | Loss:3.27909 Time:0.005059\n",
      "[ Epoch 141, Train ] | Loss:2.94689 Time:0.055544\n",
      "[ Epoch 141, Val ] | Loss:3.27499 Time:0.005105\n",
      "[ Epoch 142, Train ] | Loss:2.94684 Time:0.055561\n",
      "[ Epoch 142, Val ] | Loss:3.26990 Time:0.005106\n",
      "[ Epoch 143, Train ] | Loss:2.94693 Time:0.055614\n",
      "[ Epoch 143, Val ] | Loss:3.26587 Time:0.005123\n",
      "[ Epoch 144, Train ] | Loss:2.94693 Time:0.057750\n",
      "[ Epoch 144, Val ] | Loss:3.24151 Time:0.005063\n",
      "[ Epoch 145, Train ] | Loss:2.94692 Time:0.055331\n",
      "[ Epoch 145, Val ] | Loss:3.25216 Time:0.005223\n",
      "[ Epoch 146, Train ] | Loss:2.94687 Time:0.055585\n",
      "[ Epoch 146, Val ] | Loss:3.26666 Time:0.005235\n",
      "[ Epoch 147, Train ] | Loss:2.94689 Time:0.055219\n",
      "[ Epoch 147, Val ] | Loss:3.26150 Time:0.005121\n",
      "[ Epoch 148, Train ] | Loss:2.94686 Time:0.057726\n",
      "[ Epoch 148, Val ] | Loss:3.27188 Time:0.005061\n",
      "[ Epoch 149, Train ] | Loss:2.94694 Time:0.055047\n",
      "[ Epoch 149, Val ] | Loss:3.27420 Time:0.005112\n",
      "[ Epoch 150, Train ] | Loss:2.94705 Time:0.055014\n",
      "[ Epoch 150, Val ] | Loss:3.25919 Time:0.005118\n",
      "[ Epoch 151, Train ] | Loss:2.94698 Time:0.055263\n",
      "[ Epoch 151, Val ] | Loss:3.26665 Time:0.005130\n",
      "[ Epoch 152, Train ] | Loss:2.94705 Time:0.055180\n",
      "[ Epoch 152, Val ] | Loss:3.28945 Time:0.005063\n",
      "[ Epoch 153, Train ] | Loss:2.94687 Time:0.054889\n",
      "[ Epoch 153, Val ] | Loss:3.26635 Time:0.005103\n",
      "[ Epoch 154, Train ] | Loss:2.94705 Time:0.055512\n",
      "[ Epoch 154, Val ] | Loss:3.26855 Time:0.005125\n",
      "[ Epoch 155, Train ] | Loss:2.94693 Time:0.055582\n",
      "[ Epoch 155, Val ] | Loss:3.26086 Time:0.005113\n",
      "[ Epoch 156, Train ] | Loss:2.94691 Time:0.061854\n",
      "[ Epoch 156, Val ] | Loss:3.25954 Time:0.005055\n",
      "[ Epoch 157, Train ] | Loss:2.94686 Time:0.055151\n",
      "[ Epoch 157, Val ] | Loss:3.26405 Time:0.005127\n",
      "[ Epoch 158, Train ] | Loss:2.94686 Time:0.057524\n",
      "[ Epoch 158, Val ] | Loss:3.28736 Time:0.005139\n",
      "[ Epoch 159, Train ] | Loss:2.94691 Time:0.055357\n",
      "[ Epoch 159, Val ] | Loss:3.28071 Time:0.005128\n",
      "[ Epoch 160, Train ] | Loss:2.94686 Time:0.057164\n",
      "[ Epoch 160, Val ] | Loss:3.29291 Time:0.005063\n",
      "[ Epoch 161, Train ] | Loss:2.94694 Time:0.054790\n",
      "[ Epoch 161, Val ] | Loss:3.29975 Time:0.005110\n",
      "[ Epoch 162, Train ] | Loss:2.94692 Time:0.054759\n",
      "[ Epoch 162, Val ] | Loss:3.28107 Time:0.005133\n",
      "[ Epoch 163, Train ] | Loss:2.94698 Time:0.055060\n",
      "[ Epoch 163, Val ] | Loss:3.28085 Time:0.005106\n",
      "[ Epoch 164, Train ] | Loss:2.94698 Time:0.055279\n",
      "[ Epoch 164, Val ] | Loss:3.28166 Time:0.005050\n",
      "[ Epoch 165, Train ] | Loss:2.94694 Time:0.055366\n",
      "[ Epoch 165, Val ] | Loss:3.26910 Time:0.005120\n",
      "[ Epoch 166, Train ] | Loss:2.94697 Time:0.054755\n",
      "[ Epoch 166, Val ] | Loss:3.27021 Time:0.005119\n",
      "[ Epoch 167, Train ] | Loss:2.94682 Time:0.055038\n",
      "[ Epoch 167, Val ] | Loss:3.27702 Time:0.005115\n",
      "[ Epoch 168, Train ] | Loss:2.94690 Time:0.054942\n",
      "[ Epoch 168, Val ] | Loss:3.26921 Time:0.005047\n",
      "[ Epoch 169, Train ] | Loss:2.94692 Time:0.054919\n",
      "[ Epoch 169, Val ] | Loss:3.26210 Time:0.005117\n",
      "[ Epoch 170, Train ] | Loss:2.94689 Time:0.054900\n",
      "[ Epoch 170, Val ] | Loss:3.27149 Time:0.005111\n",
      "[ Epoch 171, Train ] | Loss:2.94691 Time:0.057020\n",
      "[ Epoch 171, Val ] | Loss:3.27699 Time:0.005113\n",
      "[ Epoch 172, Train ] | Loss:2.94690 Time:0.056899\n",
      "[ Epoch 172, Val ] | Loss:3.27009 Time:0.005066\n",
      "[ Epoch 173, Train ] | Loss:2.94694 Time:0.054884\n",
      "[ Epoch 173, Val ] | Loss:3.26712 Time:0.005115\n",
      "[ Epoch 174, Train ] | Loss:2.94695 Time:0.055195\n",
      "[ Epoch 174, Val ] | Loss:3.26419 Time:0.005127\n",
      "[ Epoch 175, Train ] | Loss:2.94687 Time:0.057360\n",
      "[ Epoch 175, Val ] | Loss:3.26142 Time:0.005104\n",
      "[ Epoch 176, Train ] | Loss:2.94686 Time:0.056950\n",
      "[ Epoch 176, Val ] | Loss:3.26996 Time:0.005060\n",
      "[ Epoch 177, Train ] | Loss:2.94698 Time:0.054868\n",
      "[ Epoch 177, Val ] | Loss:3.27417 Time:0.005114\n",
      "[ Epoch 178, Train ] | Loss:2.94698 Time:0.055164\n",
      "[ Epoch 178, Val ] | Loss:3.30121 Time:0.005118\n",
      "[ Epoch 179, Train ] | Loss:2.94697 Time:0.057181\n",
      "[ Epoch 179, Val ] | Loss:3.30410 Time:0.005113\n",
      "[ Epoch 180, Train ] | Loss:2.94690 Time:0.059316\n",
      "[ Epoch 180, Val ] | Loss:3.27714 Time:0.005060\n",
      "[ Epoch 181, Train ] | Loss:2.94698 Time:0.055046\n",
      "[ Epoch 181, Val ] | Loss:3.27295 Time:0.005115\n",
      "[ Epoch 182, Train ] | Loss:2.94686 Time:0.054717\n",
      "[ Epoch 182, Val ] | Loss:3.28174 Time:0.005116\n",
      "[ Epoch 183, Train ] | Loss:2.94697 Time:0.054471\n",
      "[ Epoch 183, Val ] | Loss:3.27594 Time:0.005126\n",
      "[ Epoch 184, Train ] | Loss:2.94694 Time:0.058910\n",
      "[ Epoch 184, Val ] | Loss:3.27854 Time:0.005063\n",
      "[ Epoch 185, Train ] | Loss:2.94680 Time:0.054422\n",
      "[ Epoch 185, Val ] | Loss:3.29886 Time:0.005115\n",
      "[ Epoch 186, Train ] | Loss:2.94683 Time:0.057107\n",
      "[ Epoch 186, Val ] | Loss:3.29980 Time:0.005123\n",
      "[ Epoch 187, Train ] | Loss:2.94690 Time:0.054888\n",
      "[ Epoch 187, Val ] | Loss:3.29104 Time:0.005115\n",
      "[ Epoch 188, Train ] | Loss:2.94684 Time:0.056839\n",
      "[ Epoch 188, Val ] | Loss:3.28367 Time:0.005080\n",
      "[ Epoch 189, Train ] | Loss:2.94688 Time:0.056508\n",
      "[ Epoch 189, Val ] | Loss:3.27381 Time:0.005118\n",
      "[ Epoch 190, Train ] | Loss:2.94696 Time:0.057121\n",
      "[ Epoch 190, Val ] | Loss:3.27828 Time:0.005116\n",
      "[ Epoch 191, Train ] | Loss:2.94688 Time:0.056512\n",
      "[ Epoch 191, Val ] | Loss:3.27115 Time:0.005315\n",
      "[ Epoch 192, Train ] | Loss:2.94692 Time:0.056188\n",
      "[ Epoch 192, Val ] | Loss:3.27075 Time:0.005117\n",
      "[ Epoch 193, Train ] | Loss:2.94692 Time:0.057039\n",
      "[ Epoch 193, Val ] | Loss:3.26453 Time:0.005115\n",
      "[ Epoch 194, Train ] | Loss:2.94701 Time:0.056707\n",
      "[ Epoch 194, Val ] | Loss:3.28083 Time:0.005112\n",
      "[ Epoch 195, Train ] | Loss:2.94696 Time:0.056707\n",
      "[ Epoch 195, Val ] | Loss:3.27228 Time:0.005052\n",
      "[ Epoch 196, Train ] | Loss:2.94709 Time:0.056992\n",
      "[ Epoch 196, Val ] | Loss:3.28129 Time:0.005101\n",
      "[ Epoch 197, Train ] | Loss:2.94697 Time:0.056633\n",
      "[ Epoch 197, Val ] | Loss:3.27438 Time:0.005113\n",
      "[ Epoch 198, Train ] | Loss:2.94704 Time:0.056358\n",
      "[ Epoch 198, Val ] | Loss:3.29316 Time:0.005320\n",
      "[ Epoch 199, Train ] | Loss:2.94691 Time:0.056322\n",
      "[ Epoch 199, Val ] | Loss:3.27615 Time:0.005111\n",
      "[ Epoch 200, Train ] | Loss:2.94700 Time:0.056657\n",
      "[ Epoch 200, Val ] | Loss:3.27594 Time:0.005120\n",
      "[ Epoch 201, Train ] | Loss:2.94698 Time:0.056622\n",
      "[ Epoch 201, Val ] | Loss:3.26160 Time:0.005121\n",
      "[ Epoch 202, Train ] | Loss:2.94703 Time:0.056594\n",
      "[ Epoch 202, Val ] | Loss:3.25121 Time:0.005036\n",
      "[ Epoch 203, Train ] | Loss:2.94706 Time:0.058728\n",
      "[ Epoch 203, Val ] | Loss:3.27105 Time:0.005108\n",
      "[ Epoch 204, Train ] | Loss:2.94729 Time:0.056532\n",
      "[ Epoch 204, Val ] | Loss:3.26977 Time:0.005124\n",
      "[ Epoch 205, Train ] | Loss:2.94726 Time:0.056507\n",
      "[ Epoch 205, Val ] | Loss:3.25892 Time:0.007504\n",
      "[ Epoch 206, Train ] | Loss:2.94745 Time:0.058905\n",
      "[ Epoch 206, Val ] | Loss:3.27952 Time:0.005102\n",
      "[ Epoch 207, Train ] | Loss:2.94732 Time:0.056767\n",
      "[ Epoch 207, Val ] | Loss:3.32508 Time:0.005106\n",
      "[ Epoch 208, Train ] | Loss:2.94775 Time:0.058591\n",
      "[ Epoch 208, Val ] | Loss:3.29646 Time:0.007000\n",
      "[ Epoch 209, Train ] | Loss:2.94822 Time:0.058773\n",
      "[ Epoch 209, Val ] | Loss:3.30286 Time:0.005125\n",
      "[ Epoch 210, Train ] | Loss:2.94863 Time:0.056729\n",
      "[ Epoch 210, Val ] | Loss:3.32295 Time:0.005135\n",
      "[ Epoch 211, Train ] | Loss:2.95007 Time:0.056700\n",
      "[ Epoch 211, Val ] | Loss:3.31767 Time:0.005234\n",
      "[ Epoch 212, Train ] | Loss:2.95137 Time:0.055351\n",
      "[ Epoch 212, Val ] | Loss:3.29978 Time:0.005064\n",
      "[ Epoch 213, Train ] | Loss:2.95349 Time:0.056471\n",
      "[ Epoch 213, Val ] | Loss:3.38615 Time:0.005127\n",
      "[ Epoch 214, Train ] | Loss:2.95877 Time:0.056726\n",
      "[ Epoch 214, Val ] | Loss:3.39019 Time:0.005110\n",
      "[ Epoch 215, Train ] | Loss:2.97450 Time:0.055427\n",
      "[ Epoch 215, Val ] | Loss:3.49369 Time:0.005123\n",
      "[ Epoch 216, Train ] | Loss:2.98728 Time:0.056797\n",
      "[ Epoch 216, Val ] | Loss:3.49684 Time:0.005057\n",
      "[ Epoch 217, Train ] | Loss:2.98520 Time:0.056671\n",
      "[ Epoch 217, Val ] | Loss:3.36939 Time:0.005116\n",
      "[ Epoch 218, Train ] | Loss:2.96563 Time:0.056147\n",
      "[ Epoch 218, Val ] | Loss:3.40798 Time:0.005119\n",
      "[ Epoch 219, Train ] | Loss:2.95486 Time:0.056337\n",
      "[ Epoch 219, Val ] | Loss:3.32159 Time:0.007004\n",
      "[ Epoch 220, Train ] | Loss:2.95134 Time:0.056240\n",
      "[ Epoch 220, Val ] | Loss:3.27857 Time:0.005129\n",
      "[ Epoch 221, Train ] | Loss:2.94910 Time:0.056070\n",
      "[ Epoch 221, Val ] | Loss:3.19665 Time:0.005120\n",
      "[ Epoch 222, Train ] | Loss:2.94739 Time:0.056327\n",
      "[ Epoch 222, Val ] | Loss:3.17621 Time:0.005129\n",
      "save model with val loss 3.176\n",
      "[ Epoch 223, Train ] | Loss:2.94680 Time:0.055771\n",
      "[ Epoch 223, Val ] | Loss:3.15860 Time:0.004968\n",
      "save model with val loss 3.159\n",
      "[ Epoch 224, Train ] | Loss:2.94654 Time:0.058543\n",
      "[ Epoch 224, Val ] | Loss:3.14206 Time:0.005036\n",
      "save model with val loss 3.142\n",
      "[ Epoch 225, Train ] | Loss:2.94640 Time:0.056836\n",
      "[ Epoch 225, Val ] | Loss:3.13259 Time:0.005037\n",
      "save model with val loss 3.133\n",
      "[ Epoch 226, Train ] | Loss:2.94633 Time:0.058818\n",
      "[ Epoch 226, Val ] | Loss:3.12891 Time:0.005000\n",
      "save model with val loss 3.129\n",
      "[ Epoch 227, Train ] | Loss:2.94631 Time:0.056661\n",
      "[ Epoch 227, Val ] | Loss:3.12962 Time:0.005035\n",
      "[ Epoch 228, Train ] | Loss:2.94620 Time:0.056301\n",
      "[ Epoch 228, Val ] | Loss:3.13164 Time:0.005111\n",
      "[ Epoch 229, Train ] | Loss:2.94624 Time:0.056215\n",
      "[ Epoch 229, Val ] | Loss:3.13589 Time:0.005046\n",
      "[ Epoch 230, Train ] | Loss:2.94623 Time:0.056195\n",
      "[ Epoch 230, Val ] | Loss:3.13885 Time:0.005110\n",
      "[ Epoch 231, Train ] | Loss:2.94616 Time:0.059145\n",
      "[ Epoch 231, Val ] | Loss:3.14080 Time:0.005121\n",
      "[ Epoch 232, Train ] | Loss:2.94613 Time:0.058282\n",
      "[ Epoch 232, Val ] | Loss:3.14157 Time:0.005041\n",
      "[ Epoch 233, Train ] | Loss:2.94617 Time:0.055651\n",
      "[ Epoch 233, Val ] | Loss:3.14150 Time:0.005227\n",
      "[ Epoch 234, Train ] | Loss:2.94619 Time:0.055220\n",
      "[ Epoch 234, Val ] | Loss:3.14211 Time:0.005107\n",
      "[ Epoch 235, Train ] | Loss:2.94623 Time:0.060057\n",
      "[ Epoch 235, Val ] | Loss:3.14255 Time:0.007048\n",
      "[ Epoch 236, Train ] | Loss:2.94617 Time:0.055629\n",
      "[ Epoch 236, Val ] | Loss:3.14300 Time:0.005130\n",
      "[ Epoch 237, Train ] | Loss:2.94620 Time:0.056432\n",
      "[ Epoch 237, Val ] | Loss:3.14357 Time:0.005121\n",
      "[ Epoch 238, Train ] | Loss:2.94627 Time:0.056075\n",
      "[ Epoch 238, Val ] | Loss:3.14446 Time:0.005106\n",
      "[ Epoch 239, Train ] | Loss:2.94620 Time:0.055788\n",
      "[ Epoch 239, Val ] | Loss:3.14722 Time:0.005041\n",
      "[ Epoch 240, Train ] | Loss:2.94620 Time:0.056442\n",
      "[ Epoch 240, Val ] | Loss:3.14989 Time:0.005111\n",
      "[ Epoch 241, Train ] | Loss:2.94623 Time:0.056058\n",
      "[ Epoch 241, Val ] | Loss:3.15134 Time:0.005117\n",
      "[ Epoch 242, Train ] | Loss:2.94620 Time:0.055752\n",
      "[ Epoch 242, Val ] | Loss:3.15072 Time:0.007091\n",
      "[ Epoch 243, Train ] | Loss:2.94622 Time:0.056053\n",
      "[ Epoch 243, Val ] | Loss:3.15240 Time:0.005109\n",
      "[ Epoch 244, Train ] | Loss:2.94621 Time:0.056114\n",
      "[ Epoch 244, Val ] | Loss:3.15413 Time:0.005120\n",
      "[ Epoch 245, Train ] | Loss:2.94625 Time:0.058347\n",
      "[ Epoch 245, Val ] | Loss:3.15487 Time:0.005114\n",
      "[ Epoch 246, Train ] | Loss:2.94628 Time:0.056377\n",
      "[ Epoch 246, Val ] | Loss:3.15435 Time:0.005044\n",
      "[ Epoch 247, Train ] | Loss:2.94631 Time:0.055982\n",
      "[ Epoch 247, Val ] | Loss:3.15526 Time:0.005107\n",
      "[ Epoch 248, Train ] | Loss:2.94630 Time:0.055935\n",
      "[ Epoch 248, Val ] | Loss:3.15634 Time:0.005110\n",
      "[ Epoch 249, Train ] | Loss:2.94630 Time:0.055649\n",
      "[ Epoch 249, Val ] | Loss:3.15907 Time:0.007075\n",
      "[ Epoch 250, Train ] | Loss:2.94628 Time:0.058672\n",
      "[ Epoch 250, Val ] | Loss:3.16142 Time:0.005240\n",
      "[ Epoch 251, Train ] | Loss:2.94627 Time:0.055915\n",
      "[ Epoch 251, Val ] | Loss:3.16239 Time:0.005118\n",
      "[ Epoch 252, Train ] | Loss:2.94630 Time:0.055645\n",
      "[ Epoch 252, Val ] | Loss:3.16415 Time:0.005116\n",
      "[ Epoch 253, Train ] | Loss:2.94630 Time:0.056241\n",
      "[ Epoch 253, Val ] | Loss:3.16588 Time:0.005073\n",
      "[ Epoch 254, Train ] | Loss:2.94630 Time:0.056146\n",
      "[ Epoch 254, Val ] | Loss:3.16614 Time:0.005124\n",
      "[ Epoch 255, Train ] | Loss:2.94629 Time:0.058499\n",
      "[ Epoch 255, Val ] | Loss:3.16562 Time:0.005117\n",
      "[ Epoch 256, Train ] | Loss:2.94633 Time:0.055848\n",
      "[ Epoch 256, Val ] | Loss:3.16588 Time:0.007469\n",
      "[ Epoch 257, Train ] | Loss:2.94631 Time:0.055583\n",
      "[ Epoch 257, Val ] | Loss:3.16534 Time:0.005102\n",
      "[ Epoch 258, Train ] | Loss:2.94629 Time:0.056187\n",
      "[ Epoch 258, Val ] | Loss:3.16760 Time:0.005125\n",
      "[ Epoch 259, Train ] | Loss:2.94630 Time:0.055757\n",
      "[ Epoch 259, Val ] | Loss:3.17074 Time:0.005106\n",
      "[ Epoch 260, Train ] | Loss:2.94630 Time:0.058166\n",
      "[ Epoch 260, Val ] | Loss:3.17243 Time:0.005058\n",
      "[ Epoch 261, Train ] | Loss:2.94632 Time:0.055478\n",
      "[ Epoch 261, Val ] | Loss:3.17098 Time:0.005115\n",
      "[ Epoch 262, Train ] | Loss:2.94640 Time:0.055941\n",
      "[ Epoch 262, Val ] | Loss:3.17177 Time:0.005117\n",
      "[ Epoch 263, Train ] | Loss:2.94639 Time:0.055719\n",
      "[ Epoch 263, Val ] | Loss:3.17432 Time:0.007052\n",
      "[ Epoch 264, Train ] | Loss:2.94633 Time:0.056110\n",
      "[ Epoch 264, Val ] | Loss:3.17653 Time:0.005135\n",
      "[ Epoch 265, Train ] | Loss:2.94634 Time:0.055140\n",
      "[ Epoch 265, Val ] | Loss:3.17825 Time:0.005063\n",
      "[ Epoch 266, Train ] | Loss:2.94640 Time:0.057865\n",
      "[ Epoch 266, Val ] | Loss:3.17897 Time:0.005115\n",
      "[ Epoch 267, Train ] | Loss:2.94641 Time:0.055701\n",
      "[ Epoch 267, Val ] | Loss:3.17933 Time:0.005044\n",
      "[ Epoch 268, Train ] | Loss:2.94637 Time:0.055656\n",
      "[ Epoch 268, Val ] | Loss:3.17933 Time:0.005121\n",
      "[ Epoch 269, Train ] | Loss:2.94641 Time:0.055416\n",
      "[ Epoch 269, Val ] | Loss:3.18128 Time:0.005120\n",
      "[ Epoch 270, Train ] | Loss:2.94641 Time:0.055067\n",
      "[ Epoch 270, Val ] | Loss:3.18383 Time:0.005121\n",
      "[ Epoch 271, Train ] | Loss:2.94641 Time:0.055647\n",
      "[ Epoch 271, Val ] | Loss:3.18477 Time:0.005120\n",
      "[ Epoch 272, Train ] | Loss:2.94636 Time:0.055307\n",
      "[ Epoch 272, Val ] | Loss:3.18946 Time:0.005104\n",
      "[ Epoch 273, Train ] | Loss:2.94635 Time:0.055832\n",
      "[ Epoch 273, Val ] | Loss:3.19127 Time:0.005128\n",
      "[ Epoch 274, Train ] | Loss:2.94639 Time:0.057427\n",
      "[ Epoch 274, Val ] | Loss:3.19169 Time:0.005053\n",
      "[ Epoch 275, Train ] | Loss:2.94644 Time:0.055296\n",
      "[ Epoch 275, Val ] | Loss:3.19477 Time:0.005119\n",
      "[ Epoch 276, Train ] | Loss:2.94643 Time:0.054952\n",
      "[ Epoch 276, Val ] | Loss:3.19593 Time:0.005130\n",
      "[ Epoch 277, Train ] | Loss:2.94643 Time:0.055551\n",
      "[ Epoch 277, Val ] | Loss:3.19641 Time:0.005105\n",
      "[ Epoch 278, Train ] | Loss:2.94645 Time:0.055510\n",
      "[ Epoch 278, Val ] | Loss:3.19857 Time:0.005117\n",
      "[ Epoch 279, Train ] | Loss:2.94642 Time:0.055251\n",
      "[ Epoch 279, Val ] | Loss:3.20085 Time:0.005119\n",
      "[ Epoch 280, Train ] | Loss:2.94647 Time:0.055268\n",
      "[ Epoch 280, Val ] | Loss:3.20489 Time:0.005133\n",
      "[ Epoch 281, Train ] | Loss:2.94643 Time:0.055500\n",
      "[ Epoch 281, Val ] | Loss:3.20983 Time:0.005039\n",
      "[ Epoch 282, Train ] | Loss:2.94649 Time:0.055485\n",
      "[ Epoch 282, Val ] | Loss:3.20957 Time:0.005105\n",
      "[ Epoch 283, Train ] | Loss:2.94651 Time:0.055706\n",
      "[ Epoch 283, Val ] | Loss:3.20833 Time:0.005110\n",
      "[ Epoch 284, Train ] | Loss:2.94648 Time:0.055404\n",
      "[ Epoch 284, Val ] | Loss:3.20536 Time:0.007045\n",
      "[ Epoch 285, Train ] | Loss:2.94643 Time:0.057997\n",
      "[ Epoch 285, Val ] | Loss:3.20537 Time:0.005110\n",
      "[ Epoch 286, Train ] | Loss:2.94645 Time:0.055114\n",
      "[ Epoch 286, Val ] | Loss:3.20889 Time:0.005122\n",
      "[ Epoch 287, Train ] | Loss:2.94653 Time:0.057897\n",
      "[ Epoch 287, Val ] | Loss:3.21297 Time:0.005430\n",
      "[ Epoch 288, Train ] | Loss:2.94646 Time:0.056258\n",
      "[ Epoch 288, Val ] | Loss:3.21530 Time:0.005112\n",
      "[ Epoch 289, Train ] | Loss:2.94648 Time:0.055044\n",
      "[ Epoch 289, Val ] | Loss:3.21804 Time:0.005107\n",
      "[ Epoch 290, Train ] | Loss:2.94656 Time:0.055665\n",
      "[ Epoch 290, Val ] | Loss:3.21787 Time:0.005129\n",
      "[ Epoch 291, Train ] | Loss:2.94645 Time:0.055804\n",
      "[ Epoch 291, Val ] | Loss:3.21576 Time:0.005051\n",
      "[ Epoch 292, Train ] | Loss:2.94647 Time:0.055286\n",
      "[ Epoch 292, Val ] | Loss:3.21622 Time:0.005124\n",
      "[ Epoch 293, Train ] | Loss:2.94658 Time:0.054760\n",
      "[ Epoch 293, Val ] | Loss:3.22160 Time:0.005116\n",
      "[ Epoch 294, Train ] | Loss:2.94651 Time:0.055331\n",
      "[ Epoch 294, Val ] | Loss:3.22738 Time:0.007041\n",
      "[ Epoch 295, Train ] | Loss:2.94651 Time:0.055702\n",
      "[ Epoch 295, Val ] | Loss:3.23102 Time:0.005117\n",
      "[ Epoch 296, Train ] | Loss:2.94657 Time:0.055317\n",
      "[ Epoch 296, Val ] | Loss:3.23360 Time:0.005124\n",
      "[ Epoch 297, Train ] | Loss:2.94650 Time:0.055524\n",
      "[ Epoch 297, Val ] | Loss:3.22963 Time:0.005083\n",
      "[ Epoch 298, Train ] | Loss:2.94654 Time:0.055539\n",
      "[ Epoch 298, Val ] | Loss:3.22506 Time:0.005064\n",
      "[ Epoch 299, Train ] | Loss:2.94656 Time:0.054935\n",
      "[ Epoch 299, Val ] | Loss:3.22288 Time:0.005116\n",
      "[ Epoch 300, Train ] | Loss:2.94660 Time:0.057130\n",
      "[ Epoch 300, Val ] | Loss:3.22688 Time:0.004770\n",
      "Finish training!\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47ed951b-98bc-4186-9d53-097045e581d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin testing\n",
      "Test loss: 3.114447593688965\n",
      "——————Test——————\n",
      "           class precision  recall f1-score support\n",
      "0              0    1.0000  0.7143   0.8333       7\n",
      "1              1    0.7500  0.4286   0.5455       7\n",
      "2              2    1.0000  1.0000   1.0000       7\n",
      "3              3    1.0000  1.0000   1.0000       7\n",
      "4              4    1.0000  0.5714   0.7273       7\n",
      "5              5    0.8750  1.0000   0.9333       7\n",
      "6              6    0.8333  0.7143   0.7692       7\n",
      "7              7    1.0000  0.8571   0.9231       7\n",
      "8              8    1.0000  1.0000   1.0000       7\n",
      "9              9    0.8750  1.0000   0.9333       7\n",
      "10            10    0.8750  1.0000   0.9333       7\n",
      "11            11    0.6250  0.7143   0.6667       7\n",
      "12            12    0.5833  1.0000   0.7368       7\n",
      "13            13    1.0000  1.0000   1.0000       7\n",
      "14            14    1.0000  1.0000   1.0000       7\n",
      "15            15    0.8750  1.0000   0.9333       7\n",
      "16            16    0.8571  0.8571   0.8571       7\n",
      "17            17    1.0000  1.0000   1.0000       7\n",
      "18            18    1.0000  0.5714   0.7273       7\n",
      "19            19    0.7000  1.0000   0.8235       7\n",
      "20            20    1.0000  0.8571   0.9231       7\n",
      "21            21    1.0000  0.7143   0.8333       7\n",
      "22            22    0.7500  0.4286   0.5455       7\n",
      "23            23    0.8333  0.7143   0.7692       7\n",
      "24            24    1.0000  1.0000   1.0000       7\n",
      "25            25    1.0000  1.0000   1.0000       7\n",
      "26            26    1.0000  0.8571   0.9231       7\n",
      "27            27    1.0000  1.0000   1.0000       7\n",
      "28            28    0.7778  1.0000   0.8750       7\n",
      "29            29    0.8571  0.8571   0.8571       7\n",
      "30            30    1.0000  1.0000   1.0000       7\n",
      "31            31    1.0000  1.0000   1.0000       7\n",
      "32            32    0.6667  0.8571   0.7500       7\n",
      "33            33    1.0000  1.0000   1.0000       7\n",
      "34            34    1.0000  0.7143   0.8333       7\n",
      "35            35    1.0000  1.0000   1.0000       7\n",
      "36            36    0.7778  1.0000   0.8750       7\n",
      "37            37    1.0000  1.0000   1.0000       7\n",
      "38            38    0.7778  1.0000   0.8750       7\n",
      "39            39    0.8571  0.8571   0.8571       7\n",
      "40            40    1.0000  0.8571   0.9231       7\n",
      "41            41    0.8571  0.8571   0.8571       7\n",
      "42            42    1.0000  0.8571   0.9231       7\n",
      "43            43    1.0000  1.0000   1.0000       7\n",
      "44            44    0.8571  0.8571   0.8571       7\n",
      "45            45    0.8750  1.0000   0.9333       7\n",
      "46            46    0.7500  0.8571   0.8000       7\n",
      "47            47    0.8750  1.0000   0.9333       7\n",
      "48            48    1.0000  1.0000   1.0000       7\n",
      "49            49    0.8750  1.0000   0.9333       7\n",
      "50      accuracy                     0.8914     350\n",
      "51     macro avg    0.9047  0.8914   0.8884     350\n",
      "52  weighted avg    0.9047  0.8914   0.8884     350\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3b45d6b0-85c8-4fd9-808d-cf959d15315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "num_classes = 50\n",
    "num_samples_train = 17\n",
    "num_samples_test = 3\n",
    "seed = 2022\n",
    "train_image, train_label, test_image, test_label = LoadData(num_classes, num_samples_train, \n",
    "                                                            num_samples_test, seed)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_image, train_label, \n",
    "                                                  test_size=0.1, random_state=2022)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_data = ImgDataset(x_train.reshape(-1,1,28,28), y_train)\n",
    "val_data = ImgDataset(x_val.reshape(-1,1,28,28), y_val)\n",
    "test_data = ImgDataset(test_image.reshape(-1,1,28,28), test_label)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc9ec199-6354-497d-a4d5-7c8f8184e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cnn_17 = CNN_classifier()\n",
    "epochs = 300\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4\n",
    "optimizer = Adam(new_cnn_17.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_dir = './output'\n",
    "model_name = 'new_cnn_17'\n",
    "random_seed = 2022\n",
    "trainer = Trainer(model=new_cnn_17, \n",
    "                  epochs=epochs, \n",
    "                  train_dataloader=train_loader, \n",
    "                  val_dataloader=val_loader, \n",
    "                  test_dataloader=test_loader,\n",
    "                  criterion=criterion, \n",
    "                  optimizer=optimizer, \n",
    "                  lr=learning_rate, \n",
    "                  device=device, \n",
    "                  model_dir=model_dir, \n",
    "                  model_name=model_name, \n",
    "                  random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "075fbeb1-72cd-4227-a67e-5fea036657f2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameters: 1761714, trainable parameters: 1761714\n",
      "\n",
      "        initial learning rate: 0.001, weight_decay: 0.0001,\n",
      "        total epochs: 300\n",
      "        \n",
      "Begin training, total epochs: 300\n",
      "[ Epoch 1, Train ] | Loss:3.86027 Time:0.098560\n",
      "[ Epoch 1, Val ] | Loss:3.91030 Time:0.005486\n",
      "save model with val loss 3.910\n",
      "[ Epoch 2, Train ] | Loss:3.44589 Time:0.088750\n",
      "[ Epoch 2, Val ] | Loss:4.17662 Time:0.005428\n",
      "[ Epoch 3, Train ] | Loss:2.82542 Time:0.087448\n",
      "[ Epoch 3, Val ] | Loss:5.38447 Time:0.005524\n",
      "[ Epoch 4, Train ] | Loss:2.06125 Time:0.087780\n",
      "[ Epoch 4, Val ] | Loss:6.87770 Time:0.005543\n",
      "[ Epoch 5, Train ] | Loss:1.49672 Time:0.087819\n",
      "[ Epoch 5, Val ] | Loss:7.37508 Time:0.005644\n",
      "[ Epoch 6, Train ] | Loss:1.00300 Time:0.088161\n",
      "[ Epoch 6, Val ] | Loss:6.45453 Time:0.005445\n",
      "[ Epoch 7, Train ] | Loss:0.62204 Time:0.088132\n",
      "[ Epoch 7, Val ] | Loss:4.76542 Time:0.005540\n",
      "[ Epoch 8, Train ] | Loss:0.37595 Time:0.087244\n",
      "[ Epoch 8, Val ] | Loss:1.39050 Time:0.004771\n",
      "save model with val loss 1.391\n",
      "[ Epoch 9, Train ] | Loss:0.23876 Time:0.087550\n",
      "[ Epoch 9, Val ] | Loss:1.29793 Time:0.005457\n",
      "save model with val loss 1.298\n",
      "[ Epoch 10, Train ] | Loss:0.16807 Time:0.089337\n",
      "[ Epoch 10, Val ] | Loss:1.18607 Time:0.005413\n",
      "save model with val loss 1.186\n",
      "[ Epoch 11, Train ] | Loss:0.12129 Time:0.089942\n",
      "[ Epoch 11, Val ] | Loss:2.09621 Time:0.005605\n",
      "[ Epoch 12, Train ] | Loss:0.11470 Time:0.087628\n",
      "[ Epoch 12, Val ] | Loss:0.43590 Time:0.005458\n",
      "save model with val loss 0.436\n",
      "[ Epoch 13, Train ] | Loss:0.06135 Time:0.089402\n",
      "[ Epoch 13, Val ] | Loss:0.60002 Time:0.005459\n",
      "[ Epoch 14, Train ] | Loss:0.04021 Time:0.087761\n",
      "[ Epoch 14, Val ] | Loss:0.41621 Time:0.005466\n",
      "save model with val loss 0.416\n",
      "[ Epoch 15, Train ] | Loss:0.02816 Time:0.089346\n",
      "[ Epoch 15, Val ] | Loss:0.57523 Time:0.005461\n",
      "[ Epoch 16, Train ] | Loss:0.02477 Time:0.087600\n",
      "[ Epoch 16, Val ] | Loss:0.41037 Time:0.005476\n",
      "save model with val loss 0.410\n",
      "[ Epoch 17, Train ] | Loss:0.01742 Time:0.088672\n",
      "[ Epoch 17, Val ] | Loss:0.42407 Time:0.005445\n",
      "[ Epoch 18, Train ] | Loss:0.01471 Time:0.087758\n",
      "[ Epoch 18, Val ] | Loss:0.36776 Time:0.005465\n",
      "save model with val loss 0.368\n",
      "[ Epoch 19, Train ] | Loss:0.00932 Time:0.089513\n",
      "[ Epoch 19, Val ] | Loss:0.35416 Time:0.005456\n",
      "save model with val loss 0.354\n",
      "[ Epoch 20, Train ] | Loss:0.01079 Time:0.092013\n",
      "[ Epoch 20, Val ] | Loss:0.32348 Time:0.005412\n",
      "save model with val loss 0.323\n",
      "[ Epoch 21, Train ] | Loss:0.00905 Time:0.089515\n",
      "[ Epoch 21, Val ] | Loss:0.38336 Time:0.005467\n",
      "[ Epoch 22, Train ] | Loss:0.00787 Time:0.089204\n",
      "[ Epoch 22, Val ] | Loss:0.53273 Time:0.005633\n",
      "[ Epoch 23, Train ] | Loss:0.00458 Time:0.087316\n",
      "[ Epoch 23, Val ] | Loss:0.43432 Time:0.005513\n",
      "[ Epoch 24, Train ] | Loss:0.00866 Time:0.087216\n",
      "[ Epoch 24, Val ] | Loss:0.26550 Time:0.005539\n",
      "save model with val loss 0.265\n",
      "[ Epoch 25, Train ] | Loss:0.00624 Time:0.087267\n",
      "[ Epoch 25, Val ] | Loss:0.38027 Time:0.005383\n",
      "[ Epoch 26, Train ] | Loss:0.00588 Time:0.087258\n",
      "[ Epoch 26, Val ] | Loss:0.42521 Time:0.005537\n",
      "[ Epoch 27, Train ] | Loss:0.00661 Time:0.087234\n",
      "[ Epoch 27, Val ] | Loss:0.53226 Time:0.005529\n",
      "[ Epoch 28, Train ] | Loss:0.00559 Time:0.087579\n",
      "[ Epoch 28, Val ] | Loss:0.43805 Time:0.005476\n",
      "[ Epoch 29, Train ] | Loss:0.00522 Time:0.087697\n",
      "[ Epoch 29, Val ] | Loss:0.25845 Time:0.005530\n",
      "save model with val loss 0.258\n",
      "[ Epoch 30, Train ] | Loss:0.00495 Time:0.088936\n",
      "[ Epoch 30, Val ] | Loss:0.25337 Time:0.005417\n",
      "save model with val loss 0.253\n",
      "[ Epoch 31, Train ] | Loss:0.00268 Time:0.088943\n",
      "[ Epoch 31, Val ] | Loss:0.33300 Time:0.005462\n",
      "[ Epoch 32, Train ] | Loss:0.00460 Time:0.089860\n",
      "[ Epoch 32, Val ] | Loss:0.35281 Time:0.005479\n",
      "[ Epoch 33, Train ] | Loss:0.00369 Time:0.087316\n",
      "[ Epoch 33, Val ] | Loss:0.32418 Time:0.005532\n",
      "[ Epoch 34, Train ] | Loss:0.00305 Time:0.087802\n",
      "[ Epoch 34, Val ] | Loss:0.27287 Time:0.005545\n",
      "[ Epoch 35, Train ] | Loss:0.00247 Time:0.087771\n",
      "[ Epoch 35, Val ] | Loss:0.22554 Time:0.005468\n",
      "save model with val loss 0.226\n",
      "[ Epoch 36, Train ] | Loss:0.00265 Time:0.088627\n",
      "[ Epoch 36, Val ] | Loss:0.30280 Time:0.005594\n",
      "[ Epoch 37, Train ] | Loss:0.00165 Time:0.089099\n",
      "[ Epoch 37, Val ] | Loss:0.34072 Time:0.005467\n",
      "[ Epoch 38, Train ] | Loss:0.00309 Time:0.089298\n",
      "[ Epoch 38, Val ] | Loss:0.39658 Time:0.005537\n",
      "[ Epoch 39, Train ] | Loss:0.00184 Time:0.089430\n",
      "[ Epoch 39, Val ] | Loss:0.40187 Time:0.005541\n",
      "[ Epoch 40, Train ] | Loss:0.00173 Time:0.089120\n",
      "[ Epoch 40, Val ] | Loss:0.40484 Time:0.005480\n",
      "[ Epoch 41, Train ] | Loss:0.00171 Time:0.088713\n",
      "[ Epoch 41, Val ] | Loss:0.35373 Time:0.005535\n",
      "[ Epoch 42, Train ] | Loss:0.00119 Time:0.089350\n",
      "[ Epoch 42, Val ] | Loss:0.30666 Time:0.005544\n",
      "[ Epoch 43, Train ] | Loss:0.00128 Time:0.089380\n",
      "[ Epoch 43, Val ] | Loss:0.31645 Time:0.005484\n",
      "[ Epoch 44, Train ] | Loss:0.00085 Time:0.088806\n",
      "[ Epoch 44, Val ] | Loss:0.33484 Time:0.005545\n",
      "[ Epoch 45, Train ] | Loss:0.00108 Time:0.089360\n",
      "[ Epoch 45, Val ] | Loss:0.34343 Time:0.005531\n",
      "[ Epoch 46, Train ] | Loss:0.00116 Time:0.089426\n",
      "[ Epoch 46, Val ] | Loss:0.38472 Time:0.005478\n",
      "[ Epoch 47, Train ] | Loss:0.00088 Time:0.088669\n",
      "[ Epoch 47, Val ] | Loss:0.35876 Time:0.005552\n",
      "[ Epoch 48, Train ] | Loss:0.00146 Time:0.089316\n",
      "[ Epoch 48, Val ] | Loss:0.31269 Time:0.005541\n",
      "[ Epoch 49, Train ] | Loss:0.00095 Time:0.089602\n",
      "[ Epoch 49, Val ] | Loss:0.29699 Time:0.005476\n",
      "[ Epoch 50, Train ] | Loss:0.00088 Time:0.089060\n",
      "[ Epoch 50, Val ] | Loss:0.31167 Time:0.005547\n",
      "[ Epoch 51, Train ] | Loss:0.00080 Time:0.089202\n",
      "[ Epoch 51, Val ] | Loss:0.34974 Time:0.005541\n",
      "[ Epoch 52, Train ] | Loss:0.00063 Time:0.089294\n",
      "[ Epoch 52, Val ] | Loss:0.37038 Time:0.005482\n",
      "[ Epoch 53, Train ] | Loss:0.00157 Time:0.089171\n",
      "[ Epoch 53, Val ] | Loss:0.34328 Time:0.005553\n",
      "[ Epoch 54, Train ] | Loss:0.00127 Time:0.088629\n",
      "[ Epoch 54, Val ] | Loss:0.35021 Time:0.005535\n",
      "[ Epoch 55, Train ] | Loss:0.00077 Time:0.088892\n",
      "[ Epoch 55, Val ] | Loss:0.36300 Time:0.005465\n",
      "[ Epoch 56, Train ] | Loss:0.00071 Time:0.088733\n",
      "[ Epoch 56, Val ] | Loss:0.36522 Time:0.006290\n",
      "[ Epoch 57, Train ] | Loss:0.00092 Time:0.087904\n",
      "[ Epoch 57, Val ] | Loss:0.37645 Time:0.007044\n",
      "[ Epoch 58, Train ] | Loss:0.00087 Time:0.087272\n",
      "[ Epoch 58, Val ] | Loss:0.39530 Time:0.005546\n",
      "[ Epoch 59, Train ] | Loss:0.00084 Time:0.089511\n",
      "[ Epoch 59, Val ] | Loss:0.36783 Time:0.005549\n",
      "[ Epoch 60, Train ] | Loss:0.00082 Time:0.089204\n",
      "[ Epoch 60, Val ] | Loss:0.31468 Time:0.005486\n",
      "[ Epoch 61, Train ] | Loss:0.00109 Time:0.088619\n",
      "[ Epoch 61, Val ] | Loss:0.34191 Time:0.005537\n",
      "[ Epoch 62, Train ] | Loss:0.00056 Time:0.089523\n",
      "[ Epoch 62, Val ] | Loss:0.37938 Time:0.005679\n",
      "[ Epoch 63, Train ] | Loss:0.00121 Time:0.090953\n",
      "[ Epoch 63, Val ] | Loss:0.36623 Time:0.005472\n",
      "[ Epoch 64, Train ] | Loss:0.00091 Time:0.089123\n",
      "[ Epoch 64, Val ] | Loss:0.37168 Time:0.005534\n",
      "[ Epoch 65, Train ] | Loss:0.00073 Time:0.088719\n",
      "[ Epoch 65, Val ] | Loss:0.38666 Time:0.005545\n",
      "[ Epoch 66, Train ] | Loss:0.00119 Time:0.089356\n",
      "[ Epoch 66, Val ] | Loss:0.35392 Time:0.005474\n",
      "[ Epoch 67, Train ] | Loss:0.00099 Time:0.089347\n",
      "[ Epoch 67, Val ] | Loss:0.31856 Time:0.005543\n",
      "[ Epoch 68, Train ] | Loss:0.00103 Time:0.088900\n",
      "[ Epoch 68, Val ] | Loss:0.29847 Time:0.005547\n",
      "[ Epoch 69, Train ] | Loss:0.00098 Time:0.090617\n",
      "[ Epoch 69, Val ] | Loss:0.28723 Time:0.005548\n",
      "[ Epoch 70, Train ] | Loss:0.00081 Time:0.087331\n",
      "[ Epoch 70, Val ] | Loss:0.28227 Time:0.005529\n",
      "[ Epoch 71, Train ] | Loss:0.00071 Time:0.089046\n",
      "[ Epoch 71, Val ] | Loss:0.31796 Time:0.005546\n",
      "[ Epoch 72, Train ] | Loss:0.00102 Time:0.088932\n",
      "[ Epoch 72, Val ] | Loss:0.43213 Time:0.005462\n",
      "[ Epoch 73, Train ] | Loss:0.00115 Time:0.088805\n",
      "[ Epoch 73, Val ] | Loss:0.46698 Time:0.005534\n",
      "[ Epoch 74, Train ] | Loss:0.00085 Time:0.089107\n",
      "[ Epoch 74, Val ] | Loss:0.53814 Time:0.005551\n",
      "[ Epoch 75, Train ] | Loss:0.00084 Time:0.091554\n",
      "[ Epoch 75, Val ] | Loss:0.41942 Time:0.005470\n",
      "[ Epoch 76, Train ] | Loss:0.00115 Time:0.088994\n",
      "[ Epoch 76, Val ] | Loss:0.32750 Time:0.005550\n",
      "[ Epoch 77, Train ] | Loss:0.00110 Time:0.088972\n",
      "[ Epoch 77, Val ] | Loss:0.37621 Time:0.005532\n",
      "[ Epoch 78, Train ] | Loss:0.00084 Time:0.090817\n",
      "[ Epoch 78, Val ] | Loss:0.37843 Time:0.005478\n",
      "[ Epoch 79, Train ] | Loss:0.00082 Time:0.089090\n",
      "[ Epoch 79, Val ] | Loss:0.24713 Time:0.005534\n",
      "[ Epoch 80, Train ] | Loss:0.00088 Time:0.088691\n",
      "[ Epoch 80, Val ] | Loss:0.22337 Time:0.005524\n",
      "save model with val loss 0.223\n",
      "[ Epoch 81, Train ] | Loss:0.00122 Time:0.087385\n",
      "[ Epoch 81, Val ] | Loss:0.29020 Time:0.005372\n",
      "[ Epoch 82, Train ] | Loss:0.00106 Time:0.089123\n",
      "[ Epoch 82, Val ] | Loss:0.48628 Time:0.005835\n",
      "[ Epoch 83, Train ] | Loss:0.00148 Time:0.088636\n",
      "[ Epoch 83, Val ] | Loss:0.41530 Time:0.005552\n",
      "[ Epoch 84, Train ] | Loss:0.00132 Time:0.090186\n",
      "[ Epoch 84, Val ] | Loss:0.52693 Time:0.005448\n",
      "[ Epoch 85, Train ] | Loss:0.00306 Time:0.089371\n",
      "[ Epoch 85, Val ] | Loss:0.49656 Time:0.005529\n",
      "[ Epoch 86, Train ] | Loss:0.00296 Time:0.088871\n",
      "[ Epoch 86, Val ] | Loss:1.61290 Time:0.005532\n",
      "[ Epoch 87, Train ] | Loss:0.01106 Time:0.090966\n",
      "[ Epoch 87, Val ] | Loss:0.97768 Time:0.005466\n",
      "[ Epoch 88, Train ] | Loss:0.01596 Time:0.088292\n",
      "[ Epoch 88, Val ] | Loss:1.60579 Time:0.005547\n",
      "[ Epoch 89, Train ] | Loss:0.03863 Time:0.088251\n",
      "[ Epoch 89, Val ] | Loss:0.44942 Time:0.005534\n",
      "[ Epoch 90, Train ] | Loss:0.02988 Time:0.087857\n",
      "[ Epoch 90, Val ] | Loss:1.04880 Time:0.005482\n",
      "[ Epoch 91, Train ] | Loss:0.03835 Time:0.088621\n",
      "[ Epoch 91, Val ] | Loss:0.51500 Time:0.005542\n",
      "[ Epoch 92, Train ] | Loss:0.03658 Time:0.088674\n",
      "[ Epoch 92, Val ] | Loss:1.37624 Time:0.005531\n",
      "[ Epoch 93, Train ] | Loss:0.04108 Time:0.090952\n",
      "[ Epoch 93, Val ] | Loss:0.85903 Time:0.005475\n",
      "[ Epoch 94, Train ] | Loss:0.06049 Time:0.088802\n",
      "[ Epoch 94, Val ] | Loss:1.03435 Time:0.005537\n",
      "[ Epoch 95, Train ] | Loss:0.08615 Time:0.088592\n",
      "[ Epoch 95, Val ] | Loss:2.61599 Time:0.005533\n",
      "[ Epoch 96, Train ] | Loss:0.10892 Time:0.090049\n",
      "[ Epoch 96, Val ] | Loss:2.12189 Time:0.005471\n",
      "[ Epoch 97, Train ] | Loss:0.09928 Time:0.088967\n",
      "[ Epoch 97, Val ] | Loss:1.75908 Time:0.005547\n",
      "[ Epoch 98, Train ] | Loss:0.08962 Time:0.088951\n",
      "[ Epoch 98, Val ] | Loss:0.82233 Time:0.005543\n",
      "[ Epoch 99, Train ] | Loss:0.09311 Time:0.091012\n",
      "[ Epoch 99, Val ] | Loss:1.47562 Time:0.005475\n",
      "[ Epoch 100, Train ] | Loss:0.03230 Time:0.088463\n",
      "[ Epoch 100, Val ] | Loss:0.88708 Time:0.005550\n",
      "[ Epoch 101, Train ] | Loss:0.02634 Time:0.089127\n",
      "[ Epoch 101, Val ] | Loss:0.42840 Time:0.005542\n",
      "[ Epoch 102, Train ] | Loss:0.01598 Time:0.090337\n",
      "[ Epoch 102, Val ] | Loss:0.62266 Time:0.005480\n",
      "[ Epoch 103, Train ] | Loss:0.00722 Time:0.088690\n",
      "[ Epoch 103, Val ] | Loss:0.56525 Time:0.005549\n",
      "[ Epoch 104, Train ] | Loss:0.00712 Time:0.088402\n",
      "[ Epoch 104, Val ] | Loss:0.56486 Time:0.005459\n",
      "[ Epoch 105, Train ] | Loss:0.00488 Time:0.088745\n",
      "[ Epoch 105, Val ] | Loss:0.50779 Time:0.005531\n",
      "[ Epoch 106, Train ] | Loss:0.00317 Time:0.088704\n",
      "[ Epoch 106, Val ] | Loss:0.47881 Time:0.005543\n",
      "[ Epoch 107, Train ] | Loss:0.00206 Time:0.090536\n",
      "[ Epoch 107, Val ] | Loss:0.44369 Time:0.005487\n",
      "[ Epoch 108, Train ] | Loss:0.00123 Time:0.088094\n",
      "[ Epoch 108, Val ] | Loss:0.49453 Time:0.005547\n",
      "[ Epoch 109, Train ] | Loss:0.00147 Time:0.088828\n",
      "[ Epoch 109, Val ] | Loss:0.41456 Time:0.005553\n",
      "[ Epoch 110, Train ] | Loss:0.00095 Time:0.090444\n",
      "[ Epoch 110, Val ] | Loss:0.41081 Time:0.005645\n",
      "[ Epoch 111, Train ] | Loss:0.00082 Time:0.088025\n",
      "[ Epoch 111, Val ] | Loss:0.37266 Time:0.005531\n",
      "[ Epoch 112, Train ] | Loss:0.00062 Time:0.088548\n",
      "[ Epoch 112, Val ] | Loss:0.35525 Time:0.005532\n",
      "[ Epoch 113, Train ] | Loss:0.00061 Time:0.092059\n",
      "[ Epoch 113, Val ] | Loss:0.34247 Time:0.005475\n",
      "[ Epoch 114, Train ] | Loss:0.00053 Time:0.088627\n",
      "[ Epoch 114, Val ] | Loss:0.34178 Time:0.005544\n",
      "[ Epoch 115, Train ] | Loss:0.00028 Time:0.088150\n",
      "[ Epoch 115, Val ] | Loss:0.34693 Time:0.005545\n",
      "[ Epoch 116, Train ] | Loss:0.00063 Time:0.088147\n",
      "[ Epoch 116, Val ] | Loss:0.34805 Time:0.005487\n",
      "[ Epoch 117, Train ] | Loss:0.00053 Time:0.088321\n",
      "[ Epoch 117, Val ] | Loss:0.35240 Time:0.005533\n",
      "[ Epoch 118, Train ] | Loss:0.00029 Time:0.088065\n",
      "[ Epoch 118, Val ] | Loss:0.34832 Time:0.005556\n",
      "[ Epoch 119, Train ] | Loss:0.00036 Time:0.089949\n",
      "[ Epoch 119, Val ] | Loss:0.34825 Time:0.005468\n",
      "[ Epoch 120, Train ] | Loss:0.00047 Time:0.088245\n",
      "[ Epoch 120, Val ] | Loss:0.34345 Time:0.005544\n",
      "[ Epoch 121, Train ] | Loss:0.00032 Time:0.087426\n",
      "[ Epoch 121, Val ] | Loss:0.33663 Time:0.005542\n",
      "[ Epoch 122, Train ] | Loss:0.00022 Time:0.090368\n",
      "[ Epoch 122, Val ] | Loss:0.33117 Time:0.005476\n",
      "[ Epoch 123, Train ] | Loss:0.00025 Time:0.087523\n",
      "[ Epoch 123, Val ] | Loss:0.32036 Time:0.005536\n",
      "[ Epoch 124, Train ] | Loss:0.00033 Time:0.088185\n",
      "[ Epoch 124, Val ] | Loss:0.32084 Time:0.005529\n",
      "[ Epoch 125, Train ] | Loss:0.00026 Time:0.090264\n",
      "[ Epoch 125, Val ] | Loss:0.32277 Time:0.005474\n",
      "[ Epoch 126, Train ] | Loss:0.00028 Time:0.088396\n",
      "[ Epoch 126, Val ] | Loss:0.31799 Time:0.005544\n",
      "[ Epoch 127, Train ] | Loss:0.00041 Time:0.088137\n",
      "[ Epoch 127, Val ] | Loss:0.31014 Time:0.005543\n",
      "[ Epoch 128, Train ] | Loss:0.00037 Time:0.090155\n",
      "[ Epoch 128, Val ] | Loss:0.31111 Time:0.005479\n",
      "[ Epoch 129, Train ] | Loss:0.00021 Time:0.087451\n",
      "[ Epoch 129, Val ] | Loss:0.31283 Time:0.005536\n",
      "[ Epoch 130, Train ] | Loss:0.00035 Time:0.088056\n",
      "[ Epoch 130, Val ] | Loss:0.31136 Time:0.005536\n",
      "[ Epoch 131, Train ] | Loss:0.00112 Time:0.090074\n",
      "[ Epoch 131, Val ] | Loss:0.44426 Time:0.005473\n",
      "[ Epoch 132, Train ] | Loss:0.00103 Time:0.087933\n",
      "[ Epoch 132, Val ] | Loss:0.41250 Time:0.005549\n",
      "[ Epoch 133, Train ] | Loss:0.00059 Time:0.089132\n",
      "[ Epoch 133, Val ] | Loss:0.28443 Time:0.005556\n",
      "[ Epoch 134, Train ] | Loss:0.00058 Time:0.088770\n",
      "[ Epoch 134, Val ] | Loss:0.29242 Time:0.005549\n",
      "[ Epoch 135, Train ] | Loss:0.00047 Time:0.088097\n",
      "[ Epoch 135, Val ] | Loss:0.32223 Time:0.005550\n",
      "[ Epoch 136, Train ] | Loss:0.00041 Time:0.088419\n",
      "[ Epoch 136, Val ] | Loss:0.34533 Time:0.005476\n",
      "[ Epoch 137, Train ] | Loss:0.00032 Time:0.087032\n",
      "[ Epoch 137, Val ] | Loss:0.34177 Time:0.005543\n",
      "[ Epoch 138, Train ] | Loss:0.00040 Time:0.088203\n",
      "[ Epoch 138, Val ] | Loss:0.33848 Time:0.005531\n",
      "[ Epoch 139, Train ] | Loss:0.00025 Time:0.090030\n",
      "[ Epoch 139, Val ] | Loss:0.33651 Time:0.005482\n",
      "[ Epoch 140, Train ] | Loss:0.00029 Time:0.087777\n",
      "[ Epoch 140, Val ] | Loss:0.34962 Time:0.005550\n",
      "[ Epoch 141, Train ] | Loss:0.00035 Time:0.088261\n",
      "[ Epoch 141, Val ] | Loss:0.33306 Time:0.005533\n",
      "[ Epoch 142, Train ] | Loss:0.00030 Time:0.088422\n",
      "[ Epoch 142, Val ] | Loss:0.32820 Time:0.005470\n",
      "[ Epoch 143, Train ] | Loss:0.00032 Time:0.088075\n",
      "[ Epoch 143, Val ] | Loss:0.32756 Time:0.005541\n",
      "[ Epoch 144, Train ] | Loss:0.00042 Time:0.088811\n",
      "[ Epoch 144, Val ] | Loss:0.33367 Time:0.005524\n",
      "[ Epoch 145, Train ] | Loss:0.00019 Time:0.088071\n",
      "[ Epoch 145, Val ] | Loss:0.33849 Time:0.005539\n",
      "[ Epoch 146, Train ] | Loss:0.00026 Time:0.088093\n",
      "[ Epoch 146, Val ] | Loss:0.33857 Time:0.005533\n",
      "[ Epoch 147, Train ] | Loss:0.00019 Time:0.088065\n",
      "[ Epoch 147, Val ] | Loss:0.33243 Time:0.005486\n",
      "[ Epoch 148, Train ] | Loss:0.00015 Time:0.087832\n",
      "[ Epoch 148, Val ] | Loss:0.32753 Time:0.005551\n",
      "[ Epoch 149, Train ] | Loss:0.00015 Time:0.088290\n",
      "[ Epoch 149, Val ] | Loss:0.32293 Time:0.005537\n",
      "[ Epoch 150, Train ] | Loss:0.00028 Time:0.090033\n",
      "[ Epoch 150, Val ] | Loss:0.31587 Time:0.005476\n",
      "[ Epoch 151, Train ] | Loss:0.00019 Time:0.087373\n",
      "[ Epoch 151, Val ] | Loss:0.30979 Time:0.005542\n",
      "[ Epoch 152, Train ] | Loss:0.00013 Time:0.088178\n",
      "[ Epoch 152, Val ] | Loss:0.30219 Time:0.005549\n",
      "[ Epoch 153, Train ] | Loss:0.00019 Time:0.090082\n",
      "[ Epoch 153, Val ] | Loss:0.29763 Time:0.005471\n",
      "[ Epoch 154, Train ] | Loss:0.00023 Time:0.087821\n",
      "[ Epoch 154, Val ] | Loss:0.29704 Time:0.005549\n",
      "[ Epoch 155, Train ] | Loss:0.00019 Time:0.087346\n",
      "[ Epoch 155, Val ] | Loss:0.29797 Time:0.005547\n",
      "[ Epoch 156, Train ] | Loss:0.00014 Time:0.089992\n",
      "[ Epoch 156, Val ] | Loss:0.29486 Time:0.005491\n",
      "[ Epoch 157, Train ] | Loss:0.00019 Time:0.087496\n",
      "[ Epoch 157, Val ] | Loss:0.29755 Time:0.005548\n",
      "[ Epoch 158, Train ] | Loss:0.00049 Time:0.087903\n",
      "[ Epoch 158, Val ] | Loss:0.31165 Time:0.005567\n",
      "[ Epoch 159, Train ] | Loss:0.00024 Time:0.087805\n",
      "[ Epoch 159, Val ] | Loss:0.32124 Time:0.005493\n",
      "[ Epoch 160, Train ] | Loss:0.00014 Time:0.087633\n",
      "[ Epoch 160, Val ] | Loss:0.30789 Time:0.005551\n",
      "[ Epoch 161, Train ] | Loss:0.00015 Time:0.087805\n",
      "[ Epoch 161, Val ] | Loss:0.30291 Time:0.005544\n",
      "[ Epoch 162, Train ] | Loss:0.00017 Time:0.087770\n",
      "[ Epoch 162, Val ] | Loss:0.30019 Time:0.005482\n",
      "[ Epoch 163, Train ] | Loss:0.00014 Time:0.087722\n",
      "[ Epoch 163, Val ] | Loss:0.29897 Time:0.005535\n",
      "[ Epoch 164, Train ] | Loss:0.00021 Time:0.087206\n",
      "[ Epoch 164, Val ] | Loss:0.29878 Time:0.005560\n",
      "[ Epoch 165, Train ] | Loss:0.00016 Time:0.089169\n",
      "[ Epoch 165, Val ] | Loss:0.30377 Time:0.005484\n",
      "[ Epoch 166, Train ] | Loss:0.00019 Time:0.086816\n",
      "[ Epoch 166, Val ] | Loss:0.32429 Time:0.005538\n",
      "[ Epoch 167, Train ] | Loss:0.00015 Time:0.087367\n",
      "[ Epoch 167, Val ] | Loss:0.33940 Time:0.005538\n",
      "[ Epoch 168, Train ] | Loss:0.00024 Time:0.089085\n",
      "[ Epoch 168, Val ] | Loss:0.34917 Time:0.005473\n",
      "[ Epoch 169, Train ] | Loss:0.00022 Time:0.087803\n",
      "[ Epoch 169, Val ] | Loss:0.35416 Time:0.005546\n",
      "[ Epoch 170, Train ] | Loss:0.00026 Time:0.087403\n",
      "[ Epoch 170, Val ] | Loss:0.34693 Time:0.005541\n",
      "[ Epoch 171, Train ] | Loss:0.00027 Time:0.087417\n",
      "[ Epoch 171, Val ] | Loss:0.33151 Time:0.005474\n",
      "[ Epoch 172, Train ] | Loss:0.00016 Time:0.087972\n",
      "[ Epoch 172, Val ] | Loss:0.31905 Time:0.005539\n",
      "[ Epoch 173, Train ] | Loss:0.00015 Time:0.087219\n",
      "[ Epoch 173, Val ] | Loss:0.31353 Time:0.005538\n",
      "[ Epoch 174, Train ] | Loss:0.00013 Time:0.087779\n",
      "[ Epoch 174, Val ] | Loss:0.30856 Time:0.005482\n",
      "[ Epoch 175, Train ] | Loss:0.00020 Time:0.087785\n",
      "[ Epoch 175, Val ] | Loss:0.30295 Time:0.005534\n",
      "[ Epoch 176, Train ] | Loss:0.00011 Time:0.087758\n",
      "[ Epoch 176, Val ] | Loss:0.30466 Time:0.005547\n",
      "[ Epoch 177, Train ] | Loss:0.00018 Time:0.086607\n",
      "[ Epoch 177, Val ] | Loss:0.29595 Time:0.005486\n",
      "[ Epoch 178, Train ] | Loss:0.00011 Time:0.087434\n",
      "[ Epoch 178, Val ] | Loss:0.28766 Time:0.005536\n",
      "[ Epoch 179, Train ] | Loss:0.00011 Time:0.087425\n",
      "[ Epoch 179, Val ] | Loss:0.28064 Time:0.005536\n",
      "[ Epoch 180, Train ] | Loss:0.00012 Time:0.087476\n",
      "[ Epoch 180, Val ] | Loss:0.28141 Time:0.005477\n",
      "[ Epoch 181, Train ] | Loss:0.00012 Time:0.087478\n",
      "[ Epoch 181, Val ] | Loss:0.28562 Time:0.005547\n",
      "[ Epoch 182, Train ] | Loss:0.00015 Time:0.087630\n",
      "[ Epoch 182, Val ] | Loss:0.29090 Time:0.005531\n",
      "[ Epoch 183, Train ] | Loss:0.00020 Time:0.087144\n",
      "[ Epoch 183, Val ] | Loss:0.30222 Time:0.005468\n",
      "[ Epoch 184, Train ] | Loss:0.00011 Time:0.089694\n",
      "[ Epoch 184, Val ] | Loss:0.31354 Time:0.005532\n",
      "[ Epoch 185, Train ] | Loss:0.00011 Time:0.089615\n",
      "[ Epoch 185, Val ] | Loss:0.31935 Time:0.005568\n",
      "[ Epoch 186, Train ] | Loss:0.00033 Time:0.089310\n",
      "[ Epoch 186, Val ] | Loss:0.32992 Time:0.005545\n",
      "[ Epoch 187, Train ] | Loss:0.00019 Time:0.089126\n",
      "[ Epoch 187, Val ] | Loss:0.33735 Time:0.005550\n",
      "[ Epoch 188, Train ] | Loss:0.00017 Time:0.089610\n",
      "[ Epoch 188, Val ] | Loss:0.34550 Time:0.005472\n",
      "[ Epoch 189, Train ] | Loss:0.00016 Time:0.088959\n",
      "[ Epoch 189, Val ] | Loss:0.33668 Time:0.005558\n",
      "[ Epoch 190, Train ] | Loss:0.00019 Time:0.089509\n",
      "[ Epoch 190, Val ] | Loss:0.32037 Time:0.006009\n",
      "[ Epoch 191, Train ] | Loss:0.00020 Time:0.088227\n",
      "[ Epoch 191, Val ] | Loss:0.30393 Time:0.005544\n",
      "[ Epoch 192, Train ] | Loss:0.00017 Time:0.089581\n",
      "[ Epoch 192, Val ] | Loss:0.30446 Time:0.005534\n",
      "[ Epoch 193, Train ] | Loss:0.00015 Time:0.089369\n",
      "[ Epoch 193, Val ] | Loss:0.30689 Time:0.005654\n",
      "[ Epoch 194, Train ] | Loss:0.00014 Time:0.090103\n",
      "[ Epoch 194, Val ] | Loss:0.31319 Time:0.005544\n",
      "[ Epoch 195, Train ] | Loss:0.00012 Time:0.089531\n",
      "[ Epoch 195, Val ] | Loss:0.31933 Time:0.005572\n",
      "[ Epoch 196, Train ] | Loss:0.00009 Time:0.088623\n",
      "[ Epoch 196, Val ] | Loss:0.32547 Time:0.005554\n",
      "[ Epoch 197, Train ] | Loss:0.00016 Time:0.089371\n",
      "[ Epoch 197, Val ] | Loss:0.32960 Time:0.005525\n",
      "[ Epoch 198, Train ] | Loss:0.00012 Time:0.092617\n",
      "[ Epoch 198, Val ] | Loss:0.33429 Time:0.006538\n",
      "[ Epoch 199, Train ] | Loss:0.00020 Time:0.088679\n",
      "[ Epoch 199, Val ] | Loss:0.34611 Time:0.005525\n",
      "[ Epoch 200, Train ] | Loss:0.00088 Time:0.088885\n",
      "[ Epoch 200, Val ] | Loss:0.34788 Time:0.006004\n",
      "[ Epoch 201, Train ] | Loss:0.00025 Time:0.088901\n",
      "[ Epoch 201, Val ] | Loss:0.34470 Time:0.005540\n",
      "[ Epoch 202, Train ] | Loss:0.00039 Time:0.089384\n",
      "[ Epoch 202, Val ] | Loss:0.34273 Time:0.005535\n",
      "[ Epoch 203, Train ] | Loss:0.00038 Time:0.091446\n",
      "[ Epoch 203, Val ] | Loss:0.35631 Time:0.005473\n",
      "[ Epoch 204, Train ] | Loss:0.00021 Time:0.088974\n",
      "[ Epoch 204, Val ] | Loss:0.36415 Time:0.005555\n",
      "[ Epoch 205, Train ] | Loss:0.00030 Time:0.089026\n",
      "[ Epoch 205, Val ] | Loss:0.37107 Time:0.005996\n",
      "[ Epoch 206, Train ] | Loss:0.00018 Time:0.088610\n",
      "[ Epoch 206, Val ] | Loss:0.36566 Time:0.005550\n",
      "[ Epoch 207, Train ] | Loss:0.00024 Time:0.088323\n",
      "[ Epoch 207, Val ] | Loss:0.35305 Time:0.005528\n",
      "[ Epoch 208, Train ] | Loss:0.00022 Time:0.090681\n",
      "[ Epoch 208, Val ] | Loss:0.34248 Time:0.005482\n",
      "[ Epoch 209, Train ] | Loss:0.00015 Time:0.088676\n",
      "[ Epoch 209, Val ] | Loss:0.33991 Time:0.005545\n",
      "[ Epoch 210, Train ] | Loss:0.00017 Time:0.088849\n",
      "[ Epoch 210, Val ] | Loss:0.33795 Time:0.007010\n",
      "[ Epoch 211, Train ] | Loss:0.00023 Time:0.086964\n",
      "[ Epoch 211, Val ] | Loss:0.33159 Time:0.005545\n",
      "[ Epoch 212, Train ] | Loss:0.00032 Time:0.089238\n",
      "[ Epoch 212, Val ] | Loss:0.33737 Time:0.005544\n",
      "[ Epoch 213, Train ] | Loss:0.00023 Time:0.089040\n",
      "[ Epoch 213, Val ] | Loss:0.34532 Time:0.005490\n",
      "[ Epoch 214, Train ] | Loss:0.00018 Time:0.089102\n",
      "[ Epoch 214, Val ] | Loss:0.34780 Time:0.005539\n",
      "[ Epoch 215, Train ] | Loss:0.00015 Time:0.089494\n",
      "[ Epoch 215, Val ] | Loss:0.36141 Time:0.005559\n",
      "[ Epoch 216, Train ] | Loss:0.00028 Time:0.092476\n",
      "[ Epoch 216, Val ] | Loss:0.34181 Time:0.005545\n",
      "[ Epoch 217, Train ] | Loss:0.00013 Time:0.088823\n",
      "[ Epoch 217, Val ] | Loss:0.32700 Time:0.005528\n",
      "[ Epoch 218, Train ] | Loss:0.00012 Time:0.088478\n",
      "[ Epoch 218, Val ] | Loss:0.32145 Time:0.005542\n",
      "[ Epoch 219, Train ] | Loss:0.00017 Time:0.088486\n",
      "[ Epoch 219, Val ] | Loss:0.31648 Time:0.005544\n",
      "[ Epoch 220, Train ] | Loss:0.00021 Time:0.088798\n",
      "[ Epoch 220, Val ] | Loss:0.30697 Time:0.005478\n",
      "[ Epoch 221, Train ] | Loss:0.00019 Time:0.089072\n",
      "[ Epoch 221, Val ] | Loss:0.29362 Time:0.005700\n",
      "[ Epoch 222, Train ] | Loss:0.00010 Time:0.088741\n",
      "[ Epoch 222, Val ] | Loss:0.28885 Time:0.005533\n",
      "[ Epoch 223, Train ] | Loss:0.00015 Time:0.088584\n",
      "[ Epoch 223, Val ] | Loss:0.28939 Time:0.005535\n",
      "[ Epoch 224, Train ] | Loss:0.00026 Time:0.088656\n",
      "[ Epoch 224, Val ] | Loss:0.29388 Time:0.005544\n",
      "[ Epoch 225, Train ] | Loss:0.00020 Time:0.091160\n",
      "[ Epoch 225, Val ] | Loss:0.30370 Time:0.005469\n",
      "[ Epoch 226, Train ] | Loss:0.00038 Time:0.088982\n",
      "[ Epoch 226, Val ] | Loss:0.30973 Time:0.005538\n",
      "[ Epoch 227, Train ] | Loss:0.00017 Time:0.089015\n",
      "[ Epoch 227, Val ] | Loss:0.31833 Time:0.005567\n",
      "[ Epoch 228, Train ] | Loss:0.00014 Time:0.088477\n",
      "[ Epoch 228, Val ] | Loss:0.31004 Time:0.005532\n",
      "[ Epoch 229, Train ] | Loss:0.00021 Time:0.088932\n",
      "[ Epoch 229, Val ] | Loss:0.30863 Time:0.005555\n",
      "[ Epoch 230, Train ] | Loss:0.00020 Time:0.090821\n",
      "[ Epoch 230, Val ] | Loss:0.30813 Time:0.005475\n",
      "[ Epoch 231, Train ] | Loss:0.00016 Time:0.088999\n",
      "[ Epoch 231, Val ] | Loss:0.30219 Time:0.005354\n",
      "[ Epoch 232, Train ] | Loss:0.00018 Time:0.089119\n",
      "[ Epoch 232, Val ] | Loss:0.30592 Time:0.005540\n",
      "[ Epoch 233, Train ] | Loss:0.00019 Time:0.088589\n",
      "[ Epoch 233, Val ] | Loss:0.32254 Time:0.005546\n",
      "[ Epoch 234, Train ] | Loss:0.00015 Time:0.088633\n",
      "[ Epoch 234, Val ] | Loss:0.32129 Time:0.005543\n",
      "[ Epoch 235, Train ] | Loss:0.00015 Time:0.090725\n",
      "[ Epoch 235, Val ] | Loss:0.31427 Time:0.005486\n",
      "[ Epoch 236, Train ] | Loss:0.00018 Time:0.088521\n",
      "[ Epoch 236, Val ] | Loss:0.31062 Time:0.005540\n",
      "[ Epoch 237, Train ] | Loss:0.00011 Time:0.089118\n",
      "[ Epoch 237, Val ] | Loss:0.30382 Time:0.005542\n",
      "[ Epoch 238, Train ] | Loss:0.00009 Time:0.088331\n",
      "[ Epoch 238, Val ] | Loss:0.30017 Time:0.005550\n",
      "[ Epoch 239, Train ] | Loss:0.00014 Time:0.088353\n",
      "[ Epoch 239, Val ] | Loss:0.30012 Time:0.005536\n",
      "[ Epoch 240, Train ] | Loss:0.00014 Time:0.090176\n",
      "[ Epoch 240, Val ] | Loss:0.30033 Time:0.005463\n",
      "[ Epoch 241, Train ] | Loss:0.00026 Time:0.088595\n",
      "[ Epoch 241, Val ] | Loss:0.28280 Time:0.005528\n",
      "[ Epoch 242, Train ] | Loss:0.00012 Time:0.088599\n",
      "[ Epoch 242, Val ] | Loss:0.28876 Time:0.006018\n",
      "[ Epoch 243, Train ] | Loss:0.00015 Time:0.087807\n",
      "[ Epoch 243, Val ] | Loss:0.28758 Time:0.005534\n",
      "[ Epoch 244, Train ] | Loss:0.00016 Time:0.088618\n",
      "[ Epoch 244, Val ] | Loss:0.29026 Time:0.005550\n",
      "[ Epoch 245, Train ] | Loss:0.00012 Time:0.088073\n",
      "[ Epoch 245, Val ] | Loss:0.29803 Time:0.005478\n",
      "[ Epoch 246, Train ] | Loss:0.00012 Time:0.088965\n",
      "[ Epoch 246, Val ] | Loss:0.29956 Time:0.005527\n",
      "[ Epoch 247, Train ] | Loss:0.00023 Time:0.088634\n",
      "[ Epoch 247, Val ] | Loss:0.33074 Time:0.005565\n",
      "[ Epoch 248, Train ] | Loss:0.00013 Time:0.088366\n",
      "[ Epoch 248, Val ] | Loss:0.34016 Time:0.005551\n",
      "[ Epoch 249, Train ] | Loss:0.00019 Time:0.088676\n",
      "[ Epoch 249, Val ] | Loss:0.32222 Time:0.005535\n",
      "[ Epoch 250, Train ] | Loss:0.00015 Time:0.090899\n",
      "[ Epoch 250, Val ] | Loss:0.30421 Time:0.005488\n",
      "[ Epoch 251, Train ] | Loss:0.00021 Time:0.088231\n",
      "[ Epoch 251, Val ] | Loss:0.29555 Time:0.005538\n",
      "[ Epoch 252, Train ] | Loss:0.00014 Time:0.088705\n",
      "[ Epoch 252, Val ] | Loss:0.29202 Time:0.006034\n",
      "[ Epoch 253, Train ] | Loss:0.00013 Time:0.087728\n",
      "[ Epoch 253, Val ] | Loss:0.28850 Time:0.005522\n",
      "[ Epoch 254, Train ] | Loss:0.00013 Time:0.088446\n",
      "[ Epoch 254, Val ] | Loss:0.29400 Time:0.005547\n",
      "[ Epoch 255, Train ] | Loss:0.00076 Time:0.088189\n",
      "[ Epoch 255, Val ] | Loss:0.26778 Time:0.005473\n",
      "[ Epoch 256, Train ] | Loss:0.00086 Time:0.088359\n",
      "[ Epoch 256, Val ] | Loss:0.23203 Time:0.005538\n",
      "[ Epoch 257, Train ] | Loss:0.00031 Time:0.088258\n",
      "[ Epoch 257, Val ] | Loss:0.25074 Time:0.006044\n",
      "[ Epoch 258, Train ] | Loss:0.00048 Time:0.087681\n",
      "[ Epoch 258, Val ] | Loss:0.26279 Time:0.005541\n",
      "[ Epoch 259, Train ] | Loss:0.00053 Time:0.088342\n",
      "[ Epoch 259, Val ] | Loss:0.33578 Time:0.005385\n",
      "[ Epoch 260, Train ] | Loss:0.00031 Time:0.087778\n",
      "[ Epoch 260, Val ] | Loss:0.35887 Time:0.005525\n",
      "[ Epoch 261, Train ] | Loss:0.00036 Time:0.088158\n",
      "[ Epoch 261, Val ] | Loss:0.33807 Time:0.007028\n",
      "[ Epoch 262, Train ] | Loss:0.00046 Time:0.089428\n",
      "[ Epoch 262, Val ] | Loss:0.34850 Time:0.005547\n",
      "[ Epoch 263, Train ] | Loss:0.00029 Time:0.088093\n",
      "[ Epoch 263, Val ] | Loss:0.36624 Time:0.005525\n",
      "[ Epoch 264, Train ] | Loss:0.00025 Time:0.088061\n",
      "[ Epoch 264, Val ] | Loss:0.37080 Time:0.005477\n",
      "[ Epoch 265, Train ] | Loss:0.00027 Time:0.088509\n",
      "[ Epoch 265, Val ] | Loss:0.36775 Time:0.005539\n",
      "[ Epoch 266, Train ] | Loss:0.00028 Time:0.088235\n",
      "[ Epoch 266, Val ] | Loss:0.35683 Time:0.006011\n",
      "[ Epoch 267, Train ] | Loss:0.00031 Time:0.087291\n",
      "[ Epoch 267, Val ] | Loss:0.35050 Time:0.005543\n",
      "[ Epoch 268, Train ] | Loss:0.00019 Time:0.088797\n",
      "[ Epoch 268, Val ] | Loss:0.34513 Time:0.005542\n",
      "[ Epoch 269, Train ] | Loss:0.00037 Time:0.090470\n",
      "[ Epoch 269, Val ] | Loss:0.32928 Time:0.005478\n",
      "[ Epoch 270, Train ] | Loss:0.00020 Time:0.088489\n",
      "[ Epoch 270, Val ] | Loss:0.30218 Time:0.005548\n",
      "[ Epoch 271, Train ] | Loss:0.00014 Time:0.088012\n",
      "[ Epoch 271, Val ] | Loss:0.28997 Time:0.006037\n",
      "[ Epoch 272, Train ] | Loss:0.00023 Time:0.087390\n",
      "[ Epoch 272, Val ] | Loss:0.28775 Time:0.005549\n",
      "[ Epoch 273, Train ] | Loss:0.00027 Time:0.087996\n",
      "[ Epoch 273, Val ] | Loss:0.30045 Time:0.005539\n",
      "[ Epoch 274, Train ] | Loss:0.00024 Time:0.088303\n",
      "[ Epoch 274, Val ] | Loss:0.32091 Time:0.005463\n",
      "[ Epoch 275, Train ] | Loss:0.00026 Time:0.087809\n",
      "[ Epoch 275, Val ] | Loss:0.31956 Time:0.005536\n",
      "[ Epoch 276, Train ] | Loss:0.00021 Time:0.088003\n",
      "[ Epoch 276, Val ] | Loss:0.31828 Time:0.006056\n",
      "[ Epoch 277, Train ] | Loss:0.00019 Time:0.087327\n",
      "[ Epoch 277, Val ] | Loss:0.31688 Time:0.005533\n",
      "[ Epoch 278, Train ] | Loss:0.00017 Time:0.088221\n",
      "[ Epoch 278, Val ] | Loss:0.32194 Time:0.005544\n",
      "[ Epoch 279, Train ] | Loss:0.00016 Time:0.088281\n",
      "[ Epoch 279, Val ] | Loss:0.31778 Time:0.005475\n",
      "[ Epoch 280, Train ] | Loss:0.00018 Time:0.087862\n",
      "[ Epoch 280, Val ] | Loss:0.33059 Time:0.005531\n",
      "[ Epoch 281, Train ] | Loss:0.00011 Time:0.088429\n",
      "[ Epoch 281, Val ] | Loss:0.33239 Time:0.005962\n",
      "[ Epoch 282, Train ] | Loss:0.00018 Time:0.087329\n",
      "[ Epoch 282, Val ] | Loss:0.32618 Time:0.005531\n",
      "[ Epoch 283, Train ] | Loss:0.00013 Time:0.087771\n",
      "[ Epoch 283, Val ] | Loss:0.32689 Time:0.005528\n",
      "[ Epoch 284, Train ] | Loss:0.00017 Time:0.089937\n",
      "[ Epoch 284, Val ] | Loss:0.33729 Time:0.005484\n",
      "[ Epoch 285, Train ] | Loss:0.00013 Time:0.089703\n",
      "[ Epoch 285, Val ] | Loss:0.32155 Time:0.005534\n",
      "[ Epoch 286, Train ] | Loss:0.00017 Time:0.088463\n",
      "[ Epoch 286, Val ] | Loss:0.30823 Time:0.005694\n",
      "[ Epoch 287, Train ] | Loss:0.00024 Time:0.088309\n",
      "[ Epoch 287, Val ] | Loss:0.30502 Time:0.005538\n",
      "[ Epoch 288, Train ] | Loss:0.00019 Time:0.088196\n",
      "[ Epoch 288, Val ] | Loss:0.29720 Time:0.005543\n",
      "[ Epoch 289, Train ] | Loss:0.00011 Time:0.087588\n",
      "[ Epoch 289, Val ] | Loss:0.28460 Time:0.005479\n",
      "[ Epoch 290, Train ] | Loss:0.00015 Time:0.088418\n",
      "[ Epoch 290, Val ] | Loss:0.28428 Time:0.005548\n",
      "[ Epoch 291, Train ] | Loss:0.00012 Time:0.087971\n",
      "[ Epoch 291, Val ] | Loss:0.29473 Time:0.008037\n",
      "[ Epoch 292, Train ] | Loss:0.00016 Time:0.087682\n",
      "[ Epoch 292, Val ] | Loss:0.29771 Time:0.005540\n",
      "[ Epoch 293, Train ] | Loss:0.00017 Time:0.087715\n",
      "[ Epoch 293, Val ] | Loss:0.30484 Time:0.005528\n",
      "[ Epoch 294, Train ] | Loss:0.00011 Time:0.088146\n",
      "[ Epoch 294, Val ] | Loss:0.31196 Time:0.005491\n",
      "[ Epoch 295, Train ] | Loss:0.00020 Time:0.087672\n",
      "[ Epoch 295, Val ] | Loss:0.30881 Time:0.005543\n",
      "[ Epoch 296, Train ] | Loss:0.00015 Time:0.087532\n",
      "[ Epoch 296, Val ] | Loss:0.30937 Time:0.006034\n",
      "[ Epoch 297, Train ] | Loss:0.00015 Time:0.089230\n",
      "[ Epoch 297, Val ] | Loss:0.30358 Time:0.005543\n",
      "[ Epoch 298, Train ] | Loss:0.00026 Time:0.087842\n",
      "[ Epoch 298, Val ] | Loss:0.29575 Time:0.005545\n",
      "[ Epoch 299, Train ] | Loss:0.00018 Time:0.088345\n",
      "[ Epoch 299, Val ] | Loss:0.28444 Time:0.005469\n",
      "[ Epoch 300, Train ] | Loss:0.00010 Time:0.088384\n",
      "[ Epoch 300, Val ] | Loss:0.27660 Time:0.005550\n",
      "Finish training!\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c84cc89-e554-4182-be77-34420847babf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin testing\n",
      "Test loss: 0.17882429622113705\n",
      "——————Test——————\n",
      "           class precision  recall f1-score support\n",
      "0              0    1.0000  1.0000   1.0000       3\n",
      "1              1    1.0000  0.6667   0.8000       3\n",
      "2              2    1.0000  1.0000   1.0000       3\n",
      "3              3    1.0000  1.0000   1.0000       3\n",
      "4              4    1.0000  1.0000   1.0000       3\n",
      "5              5    0.7500  1.0000   0.8571       3\n",
      "6              6    1.0000  1.0000   1.0000       3\n",
      "7              7    1.0000  1.0000   1.0000       3\n",
      "8              8    1.0000  1.0000   1.0000       3\n",
      "9              9    1.0000  1.0000   1.0000       3\n",
      "10            10    1.0000  1.0000   1.0000       3\n",
      "11            11    1.0000  1.0000   1.0000       3\n",
      "12            12    0.7500  1.0000   0.8571       3\n",
      "13            13    1.0000  1.0000   1.0000       3\n",
      "14            14    1.0000  0.6667   0.8000       3\n",
      "15            15    1.0000  1.0000   1.0000       3\n",
      "16            16    1.0000  1.0000   1.0000       3\n",
      "17            17    1.0000  1.0000   1.0000       3\n",
      "18            18    1.0000  0.6667   0.8000       3\n",
      "19            19    0.7500  1.0000   0.8571       3\n",
      "20            20    1.0000  0.6667   0.8000       3\n",
      "21            21    1.0000  1.0000   1.0000       3\n",
      "22            22    1.0000  1.0000   1.0000       3\n",
      "23            23    1.0000  0.6667   0.8000       3\n",
      "24            24    1.0000  1.0000   1.0000       3\n",
      "25            25    1.0000  1.0000   1.0000       3\n",
      "26            26    1.0000  1.0000   1.0000       3\n",
      "27            27    1.0000  1.0000   1.0000       3\n",
      "28            28    1.0000  1.0000   1.0000       3\n",
      "29            29    0.7500  1.0000   0.8571       3\n",
      "30            30    1.0000  1.0000   1.0000       3\n",
      "31            31    1.0000  1.0000   1.0000       3\n",
      "32            32    1.0000  1.0000   1.0000       3\n",
      "33            33    1.0000  1.0000   1.0000       3\n",
      "34            34    1.0000  1.0000   1.0000       3\n",
      "35            35    1.0000  1.0000   1.0000       3\n",
      "36            36    1.0000  1.0000   1.0000       3\n",
      "37            37    0.7500  1.0000   0.8571       3\n",
      "38            38    0.7500  1.0000   0.8571       3\n",
      "39            39    1.0000  1.0000   1.0000       3\n",
      "40            40    1.0000  1.0000   1.0000       3\n",
      "41            41    1.0000  1.0000   1.0000       3\n",
      "42            42    1.0000  1.0000   1.0000       3\n",
      "43            43    1.0000  1.0000   1.0000       3\n",
      "44            44    1.0000  0.6667   0.8000       3\n",
      "45            45    1.0000  1.0000   1.0000       3\n",
      "46            46    0.7500  1.0000   0.8571       3\n",
      "47            47    1.0000  1.0000   1.0000       3\n",
      "48            48    1.0000  1.0000   1.0000       3\n",
      "49            49    1.0000  0.6667   0.8000       3\n",
      "50      accuracy                     0.9533     150\n",
      "51     macro avg    0.9650  0.9533   0.9520     150\n",
      "52  weighted avg    0.9650  0.9533   0.9520     150\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f61db322-01dc-4678-aa26-9f41eb05809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_17 = Vanilla_CNN()\n",
    "epochs = 300\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4\n",
    "optimizer = Adam(cnn_17.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_dir = './output'\n",
    "model_name = 'cnn_17'\n",
    "random_seed = 2022\n",
    "trainer = Trainer(model=cnn_17, \n",
    "                  epochs=epochs, \n",
    "                  train_dataloader=train_loader, \n",
    "                  val_dataloader=val_loader, \n",
    "                  test_dataloader=test_loader,\n",
    "                  criterion=criterion, \n",
    "                  optimizer=optimizer, \n",
    "                  lr=learning_rate, \n",
    "                  device=device, \n",
    "                  model_dir=model_dir, \n",
    "                  model_name=model_name, \n",
    "                  random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "68ec2dfd-f3b2-4b6f-8757-20e51c4f625e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameters: 1698194, trainable parameters: 1698194\n",
      "\n",
      "        initial learning rate: 0.001, weight_decay: 0.0001,\n",
      "        total epochs: 300\n",
      "        \n",
      "Begin training, total epochs: 300\n",
      "[ Epoch 1, Train ] | Loss:3.83657 Time:0.076515\n",
      "[ Epoch 1, Val ] | Loss:3.91358 Time:0.005180\n",
      "save model with val loss 3.914\n",
      "[ Epoch 2, Train ] | Loss:3.64815 Time:0.070791\n",
      "[ Epoch 2, Val ] | Loss:3.93743 Time:0.005148\n",
      "[ Epoch 3, Train ] | Loss:3.51907 Time:0.070081\n",
      "[ Epoch 3, Val ] | Loss:3.94337 Time:0.005152\n",
      "[ Epoch 4, Train ] | Loss:3.37651 Time:0.069662\n",
      "[ Epoch 4, Val ] | Loss:3.94157 Time:0.005223\n",
      "[ Epoch 5, Train ] | Loss:3.28708 Time:0.072652\n",
      "[ Epoch 5, Val ] | Loss:3.92901 Time:0.005224\n",
      "[ Epoch 6, Train ] | Loss:3.21705 Time:0.069751\n",
      "[ Epoch 6, Val ] | Loss:3.84407 Time:0.005136\n",
      "save model with val loss 3.844\n",
      "[ Epoch 7, Train ] | Loss:3.16397 Time:0.070055\n",
      "[ Epoch 7, Val ] | Loss:3.69769 Time:0.005136\n",
      "save model with val loss 3.698\n",
      "[ Epoch 8, Train ] | Loss:3.12227 Time:0.071096\n",
      "[ Epoch 8, Val ] | Loss:3.50971 Time:0.005148\n",
      "save model with val loss 3.510\n",
      "[ Epoch 9, Train ] | Loss:3.08544 Time:0.072408\n",
      "[ Epoch 9, Val ] | Loss:3.43467 Time:0.005072\n",
      "save model with val loss 3.435\n",
      "[ Epoch 10, Train ] | Loss:3.05128 Time:0.073116\n",
      "[ Epoch 10, Val ] | Loss:3.31568 Time:0.005150\n",
      "save model with val loss 3.316\n",
      "[ Epoch 11, Train ] | Loss:3.02552 Time:0.070537\n",
      "[ Epoch 11, Val ] | Loss:3.32219 Time:0.005155\n",
      "[ Epoch 12, Train ] | Loss:2.99068 Time:0.072046\n",
      "[ Epoch 12, Val ] | Loss:3.27976 Time:0.005144\n",
      "save model with val loss 3.280\n",
      "[ Epoch 13, Train ] | Loss:2.96633 Time:0.075063\n",
      "[ Epoch 13, Val ] | Loss:3.19092 Time:0.004958\n",
      "save model with val loss 3.191\n",
      "[ Epoch 14, Train ] | Loss:2.95647 Time:0.070065\n",
      "[ Epoch 14, Val ] | Loss:3.16930 Time:0.004986\n",
      "save model with val loss 3.169\n",
      "[ Epoch 15, Train ] | Loss:2.95081 Time:0.069477\n",
      "[ Epoch 15, Val ] | Loss:3.19777 Time:0.005133\n",
      "[ Epoch 16, Train ] | Loss:2.94954 Time:0.069837\n",
      "[ Epoch 16, Val ] | Loss:3.16367 Time:0.005197\n",
      "save model with val loss 3.164\n",
      "[ Epoch 17, Train ] | Loss:2.94870 Time:0.074678\n",
      "[ Epoch 17, Val ] | Loss:3.15194 Time:0.005116\n",
      "save model with val loss 3.152\n",
      "[ Epoch 18, Train ] | Loss:2.94781 Time:0.069902\n",
      "[ Epoch 18, Val ] | Loss:3.14840 Time:0.005150\n",
      "save model with val loss 3.148\n",
      "[ Epoch 19, Train ] | Loss:2.94749 Time:0.071956\n",
      "[ Epoch 19, Val ] | Loss:3.14428 Time:0.005094\n",
      "save model with val loss 3.144\n",
      "[ Epoch 20, Train ] | Loss:2.94728 Time:0.070562\n",
      "[ Epoch 20, Val ] | Loss:3.13882 Time:0.005153\n",
      "save model with val loss 3.139\n",
      "[ Epoch 21, Train ] | Loss:2.94713 Time:0.070431\n",
      "[ Epoch 21, Val ] | Loss:3.13894 Time:0.005138\n",
      "[ Epoch 22, Train ] | Loss:2.94716 Time:0.074251\n",
      "[ Epoch 22, Val ] | Loss:3.14258 Time:0.005133\n",
      "[ Epoch 23, Train ] | Loss:2.94711 Time:0.069958\n",
      "[ Epoch 23, Val ] | Loss:3.14567 Time:0.005229\n",
      "[ Epoch 24, Train ] | Loss:2.94695 Time:0.069137\n",
      "[ Epoch 24, Val ] | Loss:3.14380 Time:0.005221\n",
      "[ Epoch 25, Train ] | Loss:2.94696 Time:0.070024\n",
      "[ Epoch 25, Val ] | Loss:3.14027 Time:0.005160\n",
      "[ Epoch 26, Train ] | Loss:2.94687 Time:0.069596\n",
      "[ Epoch 26, Val ] | Loss:3.13891 Time:0.005237\n",
      "[ Epoch 27, Train ] | Loss:2.94680 Time:0.069339\n",
      "[ Epoch 27, Val ] | Loss:3.13931 Time:0.005347\n",
      "[ Epoch 28, Train ] | Loss:2.94691 Time:0.071748\n",
      "[ Epoch 28, Val ] | Loss:3.14245 Time:0.005071\n",
      "[ Epoch 29, Train ] | Loss:2.94685 Time:0.069854\n",
      "[ Epoch 29, Val ] | Loss:3.14197 Time:0.005216\n",
      "[ Epoch 30, Train ] | Loss:2.94679 Time:0.069540\n",
      "[ Epoch 30, Val ] | Loss:3.14284 Time:0.005232\n",
      "[ Epoch 31, Train ] | Loss:2.94678 Time:0.069890\n",
      "[ Epoch 31, Val ] | Loss:3.14092 Time:0.005162\n",
      "[ Epoch 32, Train ] | Loss:2.94669 Time:0.069812\n",
      "[ Epoch 32, Val ] | Loss:3.13890 Time:0.005226\n",
      "[ Epoch 33, Train ] | Loss:2.94669 Time:0.071604\n",
      "[ Epoch 33, Val ] | Loss:3.14114 Time:0.005230\n",
      "[ Epoch 34, Train ] | Loss:2.94671 Time:0.070615\n",
      "[ Epoch 34, Val ] | Loss:3.14302 Time:0.006311\n",
      "[ Epoch 35, Train ] | Loss:2.94679 Time:0.069365\n",
      "[ Epoch 35, Val ] | Loss:3.14330 Time:0.005230\n",
      "[ Epoch 36, Train ] | Loss:2.94671 Time:0.072052\n",
      "[ Epoch 36, Val ] | Loss:3.14545 Time:0.005232\n",
      "[ Epoch 37, Train ] | Loss:2.94667 Time:0.069443\n",
      "[ Epoch 37, Val ] | Loss:3.14568 Time:0.005171\n",
      "[ Epoch 38, Train ] | Loss:2.94665 Time:0.069331\n",
      "[ Epoch 38, Val ] | Loss:3.14562 Time:0.005224\n",
      "[ Epoch 39, Train ] | Loss:2.94664 Time:0.069429\n",
      "[ Epoch 39, Val ] | Loss:3.14604 Time:0.005232\n",
      "[ Epoch 40, Train ] | Loss:2.94665 Time:0.071223\n",
      "[ Epoch 40, Val ] | Loss:3.14582 Time:0.005147\n",
      "[ Epoch 41, Train ] | Loss:2.94663 Time:0.071826\n",
      "[ Epoch 41, Val ] | Loss:3.14649 Time:0.005227\n",
      "[ Epoch 42, Train ] | Loss:2.94667 Time:0.071781\n",
      "[ Epoch 42, Val ] | Loss:3.14701 Time:0.005219\n",
      "[ Epoch 43, Train ] | Loss:2.94665 Time:0.073874\n",
      "[ Epoch 43, Val ] | Loss:3.14775 Time:0.005161\n",
      "[ Epoch 44, Train ] | Loss:2.94664 Time:0.071489\n",
      "[ Epoch 44, Val ] | Loss:3.14843 Time:0.005237\n",
      "[ Epoch 45, Train ] | Loss:2.94662 Time:0.071553\n",
      "[ Epoch 45, Val ] | Loss:3.15091 Time:0.005219\n",
      "[ Epoch 46, Train ] | Loss:2.94660 Time:0.073906\n",
      "[ Epoch 46, Val ] | Loss:3.15123 Time:0.005151\n",
      "[ Epoch 47, Train ] | Loss:2.94663 Time:0.071892\n",
      "[ Epoch 47, Val ] | Loss:3.15186 Time:0.005236\n",
      "[ Epoch 48, Train ] | Loss:2.94665 Time:0.071258\n",
      "[ Epoch 48, Val ] | Loss:3.15489 Time:0.005241\n",
      "[ Epoch 49, Train ] | Loss:2.94664 Time:0.076055\n",
      "[ Epoch 49, Val ] | Loss:3.15445 Time:0.005158\n",
      "[ Epoch 50, Train ] | Loss:2.94660 Time:0.071837\n",
      "[ Epoch 50, Val ] | Loss:3.15497 Time:0.005349\n",
      "[ Epoch 51, Train ] | Loss:2.94664 Time:0.071305\n",
      "[ Epoch 51, Val ] | Loss:3.15244 Time:0.005234\n",
      "[ Epoch 52, Train ] | Loss:2.94664 Time:0.073569\n",
      "[ Epoch 52, Val ] | Loss:3.15009 Time:0.005169\n",
      "[ Epoch 53, Train ] | Loss:2.94662 Time:0.071020\n",
      "[ Epoch 53, Val ] | Loss:3.15363 Time:0.005220\n",
      "[ Epoch 54, Train ] | Loss:2.94665 Time:0.071321\n",
      "[ Epoch 54, Val ] | Loss:3.15419 Time:0.005231\n",
      "[ Epoch 55, Train ] | Loss:2.94662 Time:0.074021\n",
      "[ Epoch 55, Val ] | Loss:3.15783 Time:0.005175\n",
      "[ Epoch 56, Train ] | Loss:2.94665 Time:0.071266\n",
      "[ Epoch 56, Val ] | Loss:3.16110 Time:0.005215\n",
      "[ Epoch 57, Train ] | Loss:2.94663 Time:0.071333\n",
      "[ Epoch 57, Val ] | Loss:3.16189 Time:0.005232\n",
      "[ Epoch 58, Train ] | Loss:2.94660 Time:0.076138\n",
      "[ Epoch 58, Val ] | Loss:3.16090 Time:0.005164\n",
      "[ Epoch 59, Train ] | Loss:2.94661 Time:0.070789\n",
      "[ Epoch 59, Val ] | Loss:3.15875 Time:0.005227\n",
      "[ Epoch 60, Train ] | Loss:2.94662 Time:0.071276\n",
      "[ Epoch 60, Val ] | Loss:3.16088 Time:0.005264\n",
      "[ Epoch 61, Train ] | Loss:2.94661 Time:0.073627\n",
      "[ Epoch 61, Val ] | Loss:3.16406 Time:0.005166\n",
      "[ Epoch 62, Train ] | Loss:2.94660 Time:0.071105\n",
      "[ Epoch 62, Val ] | Loss:3.16067 Time:0.005238\n",
      "[ Epoch 63, Train ] | Loss:2.94665 Time:0.073442\n",
      "[ Epoch 63, Val ] | Loss:3.16678 Time:0.005234\n",
      "[ Epoch 64, Train ] | Loss:2.94657 Time:0.073335\n",
      "[ Epoch 64, Val ] | Loss:3.16739 Time:0.005157\n",
      "[ Epoch 65, Train ] | Loss:2.94663 Time:0.071047\n",
      "[ Epoch 65, Val ] | Loss:3.16519 Time:0.005229\n",
      "[ Epoch 66, Train ] | Loss:2.94668 Time:0.070844\n",
      "[ Epoch 66, Val ] | Loss:3.16272 Time:0.005234\n",
      "[ Epoch 67, Train ] | Loss:2.94659 Time:0.071592\n",
      "[ Epoch 67, Val ] | Loss:3.16538 Time:0.005161\n",
      "[ Epoch 68, Train ] | Loss:2.94664 Time:0.074205\n",
      "[ Epoch 68, Val ] | Loss:3.17009 Time:0.005229\n",
      "[ Epoch 69, Train ] | Loss:2.94662 Time:0.071342\n",
      "[ Epoch 69, Val ] | Loss:3.16890 Time:0.005224\n",
      "[ Epoch 70, Train ] | Loss:2.94665 Time:0.073496\n",
      "[ Epoch 70, Val ] | Loss:3.16871 Time:0.005168\n",
      "[ Epoch 71, Train ] | Loss:2.94666 Time:0.070813\n",
      "[ Epoch 71, Val ] | Loss:3.17485 Time:0.005220\n",
      "[ Epoch 72, Train ] | Loss:2.94663 Time:0.071122\n",
      "[ Epoch 72, Val ] | Loss:3.17686 Time:0.005220\n",
      "[ Epoch 73, Train ] | Loss:2.94673 Time:0.073958\n",
      "[ Epoch 73, Val ] | Loss:3.17935 Time:0.005168\n",
      "[ Epoch 74, Train ] | Loss:2.94666 Time:0.071054\n",
      "[ Epoch 74, Val ] | Loss:3.17626 Time:0.005219\n",
      "[ Epoch 75, Train ] | Loss:2.94668 Time:0.070700\n",
      "[ Epoch 75, Val ] | Loss:3.17668 Time:0.005226\n",
      "[ Epoch 76, Train ] | Loss:2.94663 Time:0.075677\n",
      "[ Epoch 76, Val ] | Loss:3.17506 Time:0.005169\n",
      "[ Epoch 77, Train ] | Loss:2.94661 Time:0.071304\n",
      "[ Epoch 77, Val ] | Loss:3.17819 Time:0.005230\n",
      "[ Epoch 78, Train ] | Loss:2.94665 Time:0.071299\n",
      "[ Epoch 78, Val ] | Loss:3.17305 Time:0.005238\n",
      "[ Epoch 79, Train ] | Loss:2.94669 Time:0.073453\n",
      "[ Epoch 79, Val ] | Loss:3.17933 Time:0.005167\n",
      "[ Epoch 80, Train ] | Loss:2.94669 Time:0.071036\n",
      "[ Epoch 80, Val ] | Loss:3.18069 Time:0.005229\n",
      "[ Epoch 81, Train ] | Loss:2.94661 Time:0.071451\n",
      "[ Epoch 81, Val ] | Loss:3.18995 Time:0.005231\n",
      "[ Epoch 82, Train ] | Loss:2.94666 Time:0.073027\n",
      "[ Epoch 82, Val ] | Loss:3.18597 Time:0.005167\n",
      "[ Epoch 83, Train ] | Loss:2.94666 Time:0.071160\n",
      "[ Epoch 83, Val ] | Loss:3.18042 Time:0.005225\n",
      "[ Epoch 84, Train ] | Loss:2.94669 Time:0.071270\n",
      "[ Epoch 84, Val ] | Loss:3.18754 Time:0.007600\n",
      "[ Epoch 85, Train ] | Loss:2.94670 Time:0.072968\n",
      "[ Epoch 85, Val ] | Loss:3.18728 Time:0.005119\n",
      "[ Epoch 86, Train ] | Loss:2.94662 Time:0.071370\n",
      "[ Epoch 86, Val ] | Loss:3.18000 Time:0.005203\n",
      "[ Epoch 87, Train ] | Loss:2.94664 Time:0.070974\n",
      "[ Epoch 87, Val ] | Loss:3.18540 Time:0.007768\n",
      "[ Epoch 88, Train ] | Loss:2.94666 Time:0.071244\n",
      "[ Epoch 88, Val ] | Loss:3.18913 Time:0.005162\n",
      "[ Epoch 89, Train ] | Loss:2.94670 Time:0.070676\n",
      "[ Epoch 89, Val ] | Loss:3.20031 Time:0.005240\n",
      "[ Epoch 90, Train ] | Loss:2.94671 Time:0.071385\n",
      "[ Epoch 90, Val ] | Loss:3.19025 Time:0.005228\n",
      "[ Epoch 91, Train ] | Loss:2.94670 Time:0.073196\n",
      "[ Epoch 91, Val ] | Loss:3.18605 Time:0.005167\n",
      "[ Epoch 92, Train ] | Loss:2.94671 Time:0.070855\n",
      "[ Epoch 92, Val ] | Loss:3.19881 Time:0.005209\n",
      "[ Epoch 93, Train ] | Loss:2.94673 Time:0.070940\n",
      "[ Epoch 93, Val ] | Loss:3.19980 Time:0.005220\n",
      "[ Epoch 94, Train ] | Loss:2.94669 Time:0.072901\n",
      "[ Epoch 94, Val ] | Loss:3.20026 Time:0.005171\n",
      "[ Epoch 95, Train ] | Loss:2.94668 Time:0.070776\n",
      "[ Epoch 95, Val ] | Loss:3.19596 Time:0.005232\n",
      "[ Epoch 96, Train ] | Loss:2.94671 Time:0.070930\n",
      "[ Epoch 96, Val ] | Loss:3.19461 Time:0.005242\n",
      "[ Epoch 97, Train ] | Loss:2.94672 Time:0.073417\n",
      "[ Epoch 97, Val ] | Loss:3.19689 Time:0.005161\n",
      "[ Epoch 98, Train ] | Loss:2.94669 Time:0.070512\n",
      "[ Epoch 98, Val ] | Loss:3.19040 Time:0.005212\n",
      "[ Epoch 99, Train ] | Loss:2.94672 Time:0.070825\n",
      "[ Epoch 99, Val ] | Loss:3.19956 Time:0.005231\n",
      "[ Epoch 100, Train ] | Loss:2.94672 Time:0.075766\n",
      "[ Epoch 100, Val ] | Loss:3.20235 Time:0.005171\n",
      "[ Epoch 101, Train ] | Loss:2.94680 Time:0.070428\n",
      "[ Epoch 101, Val ] | Loss:3.20194 Time:0.005230\n",
      "[ Epoch 102, Train ] | Loss:2.94680 Time:0.070660\n",
      "[ Epoch 102, Val ] | Loss:3.20837 Time:0.005221\n",
      "[ Epoch 103, Train ] | Loss:2.94677 Time:0.073214\n",
      "[ Epoch 103, Val ] | Loss:3.20021 Time:0.005152\n",
      "[ Epoch 104, Train ] | Loss:2.94683 Time:0.070116\n",
      "[ Epoch 104, Val ] | Loss:3.20722 Time:0.005559\n",
      "[ Epoch 105, Train ] | Loss:2.94688 Time:0.070850\n",
      "[ Epoch 105, Val ] | Loss:3.21089 Time:0.005246\n",
      "[ Epoch 106, Train ] | Loss:2.94680 Time:0.074585\n",
      "[ Epoch 106, Val ] | Loss:3.19826 Time:0.005167\n",
      "[ Epoch 107, Train ] | Loss:2.94684 Time:0.073226\n",
      "[ Epoch 107, Val ] | Loss:3.21836 Time:0.005219\n",
      "[ Epoch 108, Train ] | Loss:2.94691 Time:0.070576\n",
      "[ Epoch 108, Val ] | Loss:3.19867 Time:0.005234\n",
      "[ Epoch 109, Train ] | Loss:2.94684 Time:0.074939\n",
      "[ Epoch 109, Val ] | Loss:3.21186 Time:0.005162\n",
      "[ Epoch 110, Train ] | Loss:2.94697 Time:0.070912\n",
      "[ Epoch 110, Val ] | Loss:3.21416 Time:0.005223\n",
      "[ Epoch 111, Train ] | Loss:2.94702 Time:0.070764\n",
      "[ Epoch 111, Val ] | Loss:3.22149 Time:0.005223\n",
      "[ Epoch 112, Train ] | Loss:2.94698 Time:0.077611\n",
      "[ Epoch 112, Val ] | Loss:3.20482 Time:0.004788\n",
      "[ Epoch 113, Train ] | Loss:2.94708 Time:0.070440\n",
      "[ Epoch 113, Val ] | Loss:3.21219 Time:0.005214\n",
      "[ Epoch 114, Train ] | Loss:2.94697 Time:0.070488\n",
      "[ Epoch 114, Val ] | Loss:3.20309 Time:0.005230\n",
      "[ Epoch 115, Train ] | Loss:2.94701 Time:0.073385\n",
      "[ Epoch 115, Val ] | Loss:3.21886 Time:0.005162\n",
      "[ Epoch 116, Train ] | Loss:2.94699 Time:0.070611\n",
      "[ Epoch 116, Val ] | Loss:3.19885 Time:0.005232\n",
      "[ Epoch 117, Train ] | Loss:2.94695 Time:0.070368\n",
      "[ Epoch 117, Val ] | Loss:3.21451 Time:0.005212\n",
      "[ Epoch 118, Train ] | Loss:2.94693 Time:0.073114\n",
      "[ Epoch 118, Val ] | Loss:3.20869 Time:0.005171\n",
      "[ Epoch 119, Train ] | Loss:2.94697 Time:0.070613\n",
      "[ Epoch 119, Val ] | Loss:3.20463 Time:0.005249\n",
      "[ Epoch 120, Train ] | Loss:2.94690 Time:0.070716\n",
      "[ Epoch 120, Val ] | Loss:3.21645 Time:0.005239\n",
      "[ Epoch 121, Train ] | Loss:2.94686 Time:0.073010\n",
      "[ Epoch 121, Val ] | Loss:3.19754 Time:0.005162\n",
      "[ Epoch 122, Train ] | Loss:2.94682 Time:0.072624\n",
      "[ Epoch 122, Val ] | Loss:3.21208 Time:0.005219\n",
      "[ Epoch 123, Train ] | Loss:2.94685 Time:0.070316\n",
      "[ Epoch 123, Val ] | Loss:3.20270 Time:0.005220\n",
      "[ Epoch 124, Train ] | Loss:2.94681 Time:0.070333\n",
      "[ Epoch 124, Val ] | Loss:3.21440 Time:0.005185\n",
      "[ Epoch 125, Train ] | Loss:2.94685 Time:0.073688\n",
      "[ Epoch 125, Val ] | Loss:3.21879 Time:0.005223\n",
      "[ Epoch 126, Train ] | Loss:2.94682 Time:0.070291\n",
      "[ Epoch 126, Val ] | Loss:3.21968 Time:0.005216\n",
      "[ Epoch 127, Train ] | Loss:2.94684 Time:0.070431\n",
      "[ Epoch 127, Val ] | Loss:3.20482 Time:0.005177\n",
      "[ Epoch 128, Train ] | Loss:2.94690 Time:0.073261\n",
      "[ Epoch 128, Val ] | Loss:3.21387 Time:0.005208\n",
      "[ Epoch 129, Train ] | Loss:2.94686 Time:0.069880\n",
      "[ Epoch 129, Val ] | Loss:3.19804 Time:0.005215\n",
      "[ Epoch 130, Train ] | Loss:2.94692 Time:0.070595\n",
      "[ Epoch 130, Val ] | Loss:3.20252 Time:0.005171\n",
      "[ Epoch 131, Train ] | Loss:2.94678 Time:0.070846\n",
      "[ Epoch 131, Val ] | Loss:3.23672 Time:0.005241\n",
      "[ Epoch 132, Train ] | Loss:2.94687 Time:0.070487\n",
      "[ Epoch 132, Val ] | Loss:3.20716 Time:0.005229\n",
      "[ Epoch 133, Train ] | Loss:2.94689 Time:0.073229\n",
      "[ Epoch 133, Val ] | Loss:3.19346 Time:0.005165\n",
      "[ Epoch 134, Train ] | Loss:2.94689 Time:0.070441\n",
      "[ Epoch 134, Val ] | Loss:3.21051 Time:0.005227\n",
      "[ Epoch 135, Train ] | Loss:2.94684 Time:0.069949\n",
      "[ Epoch 135, Val ] | Loss:3.20870 Time:0.005234\n",
      "[ Epoch 136, Train ] | Loss:2.94699 Time:0.072553\n",
      "[ Epoch 136, Val ] | Loss:3.20738 Time:0.005148\n",
      "[ Epoch 137, Train ] | Loss:2.94700 Time:0.070055\n",
      "[ Epoch 137, Val ] | Loss:3.21991 Time:0.005238\n",
      "[ Epoch 138, Train ] | Loss:2.94702 Time:0.070647\n",
      "[ Epoch 138, Val ] | Loss:3.20781 Time:0.005215\n",
      "[ Epoch 139, Train ] | Loss:2.94700 Time:0.072449\n",
      "[ Epoch 139, Val ] | Loss:3.21988 Time:0.005162\n",
      "[ Epoch 140, Train ] | Loss:2.94701 Time:0.070060\n",
      "[ Epoch 140, Val ] | Loss:3.20230 Time:0.005363\n",
      "[ Epoch 141, Train ] | Loss:2.94701 Time:0.070038\n",
      "[ Epoch 141, Val ] | Loss:3.20703 Time:0.005229\n",
      "[ Epoch 142, Train ] | Loss:2.94700 Time:0.072127\n",
      "[ Epoch 142, Val ] | Loss:3.21465 Time:0.005332\n",
      "[ Epoch 143, Train ] | Loss:2.94701 Time:0.069654\n",
      "[ Epoch 143, Val ] | Loss:3.19341 Time:0.005234\n",
      "[ Epoch 144, Train ] | Loss:2.94698 Time:0.070368\n",
      "[ Epoch 144, Val ] | Loss:3.21004 Time:0.005229\n",
      "[ Epoch 145, Train ] | Loss:2.94694 Time:0.072582\n",
      "[ Epoch 145, Val ] | Loss:3.20415 Time:0.005163\n",
      "[ Epoch 146, Train ] | Loss:2.94704 Time:0.070137\n",
      "[ Epoch 146, Val ] | Loss:3.21959 Time:0.005221\n",
      "[ Epoch 147, Train ] | Loss:2.94703 Time:0.070067\n",
      "[ Epoch 147, Val ] | Loss:3.22589 Time:0.005219\n",
      "[ Epoch 148, Train ] | Loss:2.94697 Time:0.069769\n",
      "[ Epoch 148, Val ] | Loss:3.22619 Time:0.005342\n",
      "[ Epoch 149, Train ] | Loss:2.94696 Time:0.070031\n",
      "[ Epoch 149, Val ] | Loss:3.21444 Time:0.005206\n",
      "[ Epoch 150, Train ] | Loss:2.94698 Time:0.070831\n",
      "[ Epoch 150, Val ] | Loss:3.20807 Time:0.005217\n",
      "[ Epoch 151, Train ] | Loss:2.94691 Time:0.072457\n",
      "[ Epoch 151, Val ] | Loss:3.22542 Time:0.005183\n",
      "[ Epoch 152, Train ] | Loss:2.94685 Time:0.069730\n",
      "[ Epoch 152, Val ] | Loss:3.22166 Time:0.005236\n",
      "[ Epoch 153, Train ] | Loss:2.94689 Time:0.070323\n",
      "[ Epoch 153, Val ] | Loss:3.20800 Time:0.005225\n",
      "[ Epoch 154, Train ] | Loss:2.94694 Time:0.070751\n",
      "[ Epoch 154, Val ] | Loss:3.21827 Time:0.005151\n",
      "[ Epoch 155, Train ] | Loss:2.94699 Time:0.069757\n",
      "[ Epoch 155, Val ] | Loss:3.21315 Time:0.005228\n",
      "[ Epoch 156, Train ] | Loss:2.94693 Time:0.069691\n",
      "[ Epoch 156, Val ] | Loss:3.23176 Time:0.005232\n",
      "[ Epoch 157, Train ] | Loss:2.94686 Time:0.072228\n",
      "[ Epoch 157, Val ] | Loss:3.21809 Time:0.005155\n",
      "[ Epoch 158, Train ] | Loss:2.94693 Time:0.069870\n",
      "[ Epoch 158, Val ] | Loss:3.21462 Time:0.005218\n",
      "[ Epoch 159, Train ] | Loss:2.94689 Time:0.069897\n",
      "[ Epoch 159, Val ] | Loss:3.21805 Time:0.005220\n",
      "[ Epoch 160, Train ] | Loss:2.94696 Time:0.071902\n",
      "[ Epoch 160, Val ] | Loss:3.23571 Time:0.005158\n",
      "[ Epoch 161, Train ] | Loss:2.94704 Time:0.069863\n",
      "[ Epoch 161, Val ] | Loss:3.20018 Time:0.005224\n",
      "[ Epoch 162, Train ] | Loss:2.94707 Time:0.069590\n",
      "[ Epoch 162, Val ] | Loss:3.22634 Time:0.005228\n",
      "[ Epoch 163, Train ] | Loss:2.94702 Time:0.072621\n",
      "[ Epoch 163, Val ] | Loss:3.21808 Time:0.005172\n",
      "[ Epoch 164, Train ] | Loss:2.94711 Time:0.069492\n",
      "[ Epoch 164, Val ] | Loss:3.22067 Time:0.005227\n",
      "[ Epoch 165, Train ] | Loss:2.94709 Time:0.069581\n",
      "[ Epoch 165, Val ] | Loss:3.20505 Time:0.005216\n",
      "[ Epoch 166, Train ] | Loss:2.94709 Time:0.069907\n",
      "[ Epoch 166, Val ] | Loss:3.20347 Time:0.005165\n",
      "[ Epoch 167, Train ] | Loss:2.94707 Time:0.069578\n",
      "[ Epoch 167, Val ] | Loss:3.21539 Time:0.005217\n",
      "[ Epoch 168, Train ] | Loss:2.94720 Time:0.069873\n",
      "[ Epoch 168, Val ] | Loss:3.23973 Time:0.005226\n",
      "[ Epoch 169, Train ] | Loss:2.94734 Time:0.072216\n",
      "[ Epoch 169, Val ] | Loss:3.22192 Time:0.005162\n",
      "[ Epoch 170, Train ] | Loss:2.94744 Time:0.070077\n",
      "[ Epoch 170, Val ] | Loss:3.23348 Time:0.005217\n",
      "[ Epoch 171, Train ] | Loss:2.94743 Time:0.070033\n",
      "[ Epoch 171, Val ] | Loss:3.22218 Time:0.008104\n",
      "[ Epoch 172, Train ] | Loss:2.94737 Time:0.070560\n",
      "[ Epoch 172, Val ] | Loss:3.21867 Time:0.005152\n",
      "[ Epoch 173, Train ] | Loss:2.94720 Time:0.069950\n",
      "[ Epoch 173, Val ] | Loss:3.20808 Time:0.005220\n",
      "[ Epoch 174, Train ] | Loss:2.94717 Time:0.069673\n",
      "[ Epoch 174, Val ] | Loss:3.20431 Time:0.005245\n",
      "[ Epoch 175, Train ] | Loss:2.94707 Time:0.072538\n",
      "[ Epoch 175, Val ] | Loss:3.21858 Time:0.005187\n",
      "[ Epoch 176, Train ] | Loss:2.94679 Time:0.070274\n",
      "[ Epoch 176, Val ] | Loss:3.20809 Time:0.005246\n",
      "[ Epoch 177, Train ] | Loss:2.94680 Time:0.069382\n",
      "[ Epoch 177, Val ] | Loss:3.20157 Time:0.007905\n",
      "[ Epoch 178, Train ] | Loss:2.94673 Time:0.073502\n",
      "[ Epoch 178, Val ] | Loss:3.20112 Time:0.005176\n",
      "[ Epoch 179, Train ] | Loss:2.94660 Time:0.069868\n",
      "[ Epoch 179, Val ] | Loss:3.20514 Time:0.005216\n",
      "[ Epoch 180, Train ] | Loss:2.94658 Time:0.069955\n",
      "[ Epoch 180, Val ] | Loss:3.20827 Time:0.005220\n",
      "[ Epoch 181, Train ] | Loss:2.94656 Time:0.072121\n",
      "[ Epoch 181, Val ] | Loss:3.22026 Time:0.005164\n",
      "[ Epoch 182, Train ] | Loss:2.94657 Time:0.069910\n",
      "[ Epoch 182, Val ] | Loss:3.23788 Time:0.005224\n",
      "[ Epoch 183, Train ] | Loss:2.94659 Time:0.069608\n",
      "[ Epoch 183, Val ] | Loss:3.21572 Time:0.007620\n",
      "[ Epoch 184, Train ] | Loss:2.94659 Time:0.073796\n",
      "[ Epoch 184, Val ] | Loss:3.21139 Time:0.005156\n",
      "[ Epoch 185, Train ] | Loss:2.94658 Time:0.069624\n",
      "[ Epoch 185, Val ] | Loss:3.21924 Time:0.005235\n",
      "[ Epoch 186, Train ] | Loss:2.94664 Time:0.069481\n",
      "[ Epoch 186, Val ] | Loss:3.22851 Time:0.005216\n",
      "[ Epoch 187, Train ] | Loss:2.94671 Time:0.071646\n",
      "[ Epoch 187, Val ] | Loss:3.23492 Time:0.005170\n",
      "[ Epoch 188, Train ] | Loss:2.94667 Time:0.069375\n",
      "[ Epoch 188, Val ] | Loss:3.23070 Time:0.005224\n",
      "[ Epoch 189, Train ] | Loss:2.94676 Time:0.074916\n",
      "[ Epoch 189, Val ] | Loss:3.22410 Time:0.004775\n",
      "[ Epoch 190, Train ] | Loss:2.94686 Time:0.071782\n",
      "[ Epoch 190, Val ] | Loss:3.23551 Time:0.005153\n",
      "[ Epoch 191, Train ] | Loss:2.94683 Time:0.071929\n",
      "[ Epoch 191, Val ] | Loss:3.22412 Time:0.005224\n",
      "[ Epoch 192, Train ] | Loss:2.94698 Time:0.071722\n",
      "[ Epoch 192, Val ] | Loss:3.23014 Time:0.005237\n",
      "[ Epoch 193, Train ] | Loss:2.94711 Time:0.071473\n",
      "[ Epoch 193, Val ] | Loss:3.23885 Time:0.005161\n",
      "[ Epoch 194, Train ] | Loss:2.94731 Time:0.071618\n",
      "[ Epoch 194, Val ] | Loss:3.22144 Time:0.005228\n",
      "[ Epoch 195, Train ] | Loss:2.94759 Time:0.073760\n",
      "[ Epoch 195, Val ] | Loss:3.22279 Time:0.005151\n",
      "[ Epoch 196, Train ] | Loss:2.94756 Time:0.071274\n",
      "[ Epoch 196, Val ] | Loss:3.22525 Time:0.005165\n",
      "[ Epoch 197, Train ] | Loss:2.94780 Time:0.071340\n",
      "[ Epoch 197, Val ] | Loss:3.23842 Time:0.005227\n",
      "[ Epoch 198, Train ] | Loss:2.94785 Time:0.071522\n",
      "[ Epoch 198, Val ] | Loss:3.22849 Time:0.004875\n",
      "[ Epoch 199, Train ] | Loss:2.94779 Time:0.071247\n",
      "[ Epoch 199, Val ] | Loss:3.23981 Time:0.005162\n",
      "[ Epoch 200, Train ] | Loss:2.94761 Time:0.071249\n",
      "[ Epoch 200, Val ] | Loss:3.22494 Time:0.005220\n",
      "[ Epoch 201, Train ] | Loss:2.94751 Time:0.074016\n",
      "[ Epoch 201, Val ] | Loss:3.22658 Time:0.005232\n",
      "[ Epoch 202, Train ] | Loss:2.94761 Time:0.071768\n",
      "[ Epoch 202, Val ] | Loss:3.20884 Time:0.005157\n",
      "[ Epoch 203, Train ] | Loss:2.94762 Time:0.071517\n",
      "[ Epoch 203, Val ] | Loss:3.22692 Time:0.005215\n",
      "[ Epoch 204, Train ] | Loss:2.94744 Time:0.073850\n",
      "[ Epoch 204, Val ] | Loss:3.21626 Time:0.005209\n",
      "[ Epoch 205, Train ] | Loss:2.94725 Time:0.071141\n",
      "[ Epoch 205, Val ] | Loss:3.22736 Time:0.005158\n",
      "[ Epoch 206, Train ] | Loss:2.94697 Time:0.071846\n",
      "[ Epoch 206, Val ] | Loss:3.21963 Time:0.005227\n",
      "[ Epoch 207, Train ] | Loss:2.94684 Time:0.071360\n",
      "[ Epoch 207, Val ] | Loss:3.21209 Time:0.005234\n",
      "[ Epoch 208, Train ] | Loss:2.94674 Time:0.071418\n",
      "[ Epoch 208, Val ] | Loss:3.21283 Time:0.005168\n",
      "[ Epoch 209, Train ] | Loss:2.94661 Time:0.074273\n",
      "[ Epoch 209, Val ] | Loss:3.20467 Time:0.005218\n",
      "[ Epoch 210, Train ] | Loss:2.94652 Time:0.071083\n",
      "[ Epoch 210, Val ] | Loss:3.19158 Time:0.005229\n",
      "[ Epoch 211, Train ] | Loss:2.94646 Time:0.070213\n",
      "[ Epoch 211, Val ] | Loss:3.18808 Time:0.005165\n",
      "[ Epoch 212, Train ] | Loss:2.94647 Time:0.073845\n",
      "[ Epoch 212, Val ] | Loss:3.20220 Time:0.005220\n",
      "[ Epoch 213, Train ] | Loss:2.94644 Time:0.071335\n",
      "[ Epoch 213, Val ] | Loss:3.21616 Time:0.005216\n",
      "[ Epoch 214, Train ] | Loss:2.94646 Time:0.071652\n",
      "[ Epoch 214, Val ] | Loss:3.20772 Time:0.005157\n",
      "[ Epoch 215, Train ] | Loss:2.94646 Time:0.071507\n",
      "[ Epoch 215, Val ] | Loss:3.21040 Time:0.005232\n",
      "[ Epoch 216, Train ] | Loss:2.94648 Time:0.071086\n",
      "[ Epoch 216, Val ] | Loss:3.21129 Time:0.005219\n",
      "[ Epoch 217, Train ] | Loss:2.94651 Time:0.071543\n",
      "[ Epoch 217, Val ] | Loss:3.21231 Time:0.005170\n",
      "[ Epoch 218, Train ] | Loss:2.94655 Time:0.071625\n",
      "[ Epoch 218, Val ] | Loss:3.23249 Time:0.005208\n",
      "[ Epoch 219, Train ] | Loss:2.94650 Time:0.071477\n",
      "[ Epoch 219, Val ] | Loss:3.22444 Time:0.005218\n",
      "[ Epoch 220, Train ] | Loss:2.94660 Time:0.071639\n",
      "[ Epoch 220, Val ] | Loss:3.22581 Time:0.005157\n",
      "[ Epoch 221, Train ] | Loss:2.94659 Time:0.070929\n",
      "[ Epoch 221, Val ] | Loss:3.22190 Time:0.005221\n",
      "[ Epoch 222, Train ] | Loss:2.94669 Time:0.070920\n",
      "[ Epoch 222, Val ] | Loss:3.23066 Time:0.005214\n",
      "[ Epoch 223, Train ] | Loss:2.94672 Time:0.070685\n",
      "[ Epoch 223, Val ] | Loss:3.23939 Time:0.005164\n",
      "[ Epoch 224, Train ] | Loss:2.94678 Time:0.071449\n",
      "[ Epoch 224, Val ] | Loss:3.24781 Time:0.005203\n",
      "[ Epoch 225, Train ] | Loss:2.94699 Time:0.070746\n",
      "[ Epoch 225, Val ] | Loss:3.23763 Time:0.005232\n",
      "[ Epoch 226, Train ] | Loss:2.94709 Time:0.070880\n",
      "[ Epoch 226, Val ] | Loss:3.22755 Time:0.005323\n",
      "[ Epoch 227, Train ] | Loss:2.94722 Time:0.070943\n",
      "[ Epoch 227, Val ] | Loss:3.25861 Time:0.005224\n",
      "[ Epoch 228, Train ] | Loss:2.94736 Time:0.070994\n",
      "[ Epoch 228, Val ] | Loss:3.23761 Time:0.005211\n",
      "[ Epoch 229, Train ] | Loss:2.94777 Time:0.070925\n",
      "[ Epoch 229, Val ] | Loss:3.26507 Time:0.005163\n",
      "[ Epoch 230, Train ] | Loss:2.94814 Time:0.073542\n",
      "[ Epoch 230, Val ] | Loss:3.26263 Time:0.005203\n",
      "[ Epoch 231, Train ] | Loss:2.94903 Time:0.071227\n",
      "[ Epoch 231, Val ] | Loss:3.26611 Time:0.005221\n",
      "[ Epoch 232, Train ] | Loss:2.95217 Time:0.071139\n",
      "[ Epoch 232, Val ] | Loss:3.43286 Time:0.005174\n",
      "[ Epoch 233, Train ] | Loss:3.06230 Time:0.071017\n",
      "[ Epoch 233, Val ] | Loss:3.77008 Time:0.005239\n",
      "[ Epoch 234, Train ] | Loss:3.09981 Time:0.073573\n",
      "[ Epoch 234, Val ] | Loss:3.66361 Time:0.005259\n",
      "[ Epoch 235, Train ] | Loss:3.00553 Time:0.070700\n",
      "[ Epoch 235, Val ] | Loss:3.44151 Time:0.005176\n",
      "[ Epoch 236, Train ] | Loss:2.96665 Time:0.073048\n",
      "[ Epoch 236, Val ] | Loss:3.27111 Time:0.005231\n",
      "[ Epoch 237, Train ] | Loss:2.95652 Time:0.071495\n",
      "[ Epoch 237, Val ] | Loss:3.19128 Time:0.005230\n",
      "[ Epoch 238, Train ] | Loss:2.95078 Time:0.071144\n",
      "[ Epoch 238, Val ] | Loss:3.11103 Time:0.005171\n",
      "save model with val loss 3.111\n",
      "[ Epoch 239, Train ] | Loss:2.94817 Time:0.071242\n",
      "[ Epoch 239, Val ] | Loss:3.06855 Time:0.005148\n",
      "save model with val loss 3.069\n",
      "[ Epoch 240, Train ] | Loss:2.94743 Time:0.071623\n",
      "[ Epoch 240, Val ] | Loss:3.06297 Time:0.005077\n",
      "save model with val loss 3.063\n",
      "[ Epoch 241, Train ] | Loss:2.94707 Time:0.075661\n",
      "[ Epoch 241, Val ] | Loss:3.06387 Time:0.004808\n",
      "[ Epoch 242, Train ] | Loss:2.94686 Time:0.070873\n",
      "[ Epoch 242, Val ] | Loss:3.07018 Time:0.005193\n",
      "[ Epoch 243, Train ] | Loss:2.94660 Time:0.071190\n",
      "[ Epoch 243, Val ] | Loss:3.07796 Time:0.005117\n",
      "[ Epoch 244, Train ] | Loss:2.94646 Time:0.070769\n",
      "[ Epoch 244, Val ] | Loss:3.08247 Time:0.005229\n",
      "[ Epoch 245, Train ] | Loss:2.94642 Time:0.070527\n",
      "[ Epoch 245, Val ] | Loss:3.08429 Time:0.005214\n",
      "[ Epoch 246, Train ] | Loss:2.94642 Time:0.071198\n",
      "[ Epoch 246, Val ] | Loss:3.08463 Time:0.005156\n",
      "[ Epoch 247, Train ] | Loss:2.94638 Time:0.070701\n",
      "[ Epoch 247, Val ] | Loss:3.08345 Time:0.005215\n",
      "[ Epoch 248, Train ] | Loss:2.94633 Time:0.070770\n",
      "[ Epoch 248, Val ] | Loss:3.08178 Time:0.005227\n",
      "[ Epoch 249, Train ] | Loss:2.94636 Time:0.070581\n",
      "[ Epoch 249, Val ] | Loss:3.08074 Time:0.005158\n",
      "[ Epoch 250, Train ] | Loss:2.94633 Time:0.071222\n",
      "[ Epoch 250, Val ] | Loss:3.08015 Time:0.005202\n",
      "[ Epoch 251, Train ] | Loss:2.94636 Time:0.070275\n",
      "[ Epoch 251, Val ] | Loss:3.08098 Time:0.005221\n",
      "[ Epoch 252, Train ] | Loss:2.94630 Time:0.070531\n",
      "[ Epoch 252, Val ] | Loss:3.08191 Time:0.005167\n",
      "[ Epoch 253, Train ] | Loss:2.94629 Time:0.070530\n",
      "[ Epoch 253, Val ] | Loss:3.08217 Time:0.005333\n",
      "[ Epoch 254, Train ] | Loss:2.94634 Time:0.070771\n",
      "[ Epoch 254, Val ] | Loss:3.08265 Time:0.005224\n",
      "[ Epoch 255, Train ] | Loss:2.94635 Time:0.070249\n",
      "[ Epoch 255, Val ] | Loss:3.08307 Time:0.005162\n",
      "[ Epoch 256, Train ] | Loss:2.94631 Time:0.070662\n",
      "[ Epoch 256, Val ] | Loss:3.08428 Time:0.005226\n",
      "[ Epoch 257, Train ] | Loss:2.94634 Time:0.070600\n",
      "[ Epoch 257, Val ] | Loss:3.08502 Time:0.005230\n",
      "[ Epoch 258, Train ] | Loss:2.94633 Time:0.070778\n",
      "[ Epoch 258, Val ] | Loss:3.08587 Time:0.005142\n",
      "[ Epoch 259, Train ] | Loss:2.94636 Time:0.070647\n",
      "[ Epoch 259, Val ] | Loss:3.08630 Time:0.005224\n",
      "[ Epoch 260, Train ] | Loss:2.94640 Time:0.070967\n",
      "[ Epoch 260, Val ] | Loss:3.08599 Time:0.005224\n",
      "[ Epoch 261, Train ] | Loss:2.94633 Time:0.070560\n",
      "[ Epoch 261, Val ] | Loss:3.08565 Time:0.005166\n",
      "[ Epoch 262, Train ] | Loss:2.94642 Time:0.070106\n",
      "[ Epoch 262, Val ] | Loss:3.08622 Time:0.005224\n",
      "[ Epoch 263, Train ] | Loss:2.94636 Time:0.070620\n",
      "[ Epoch 263, Val ] | Loss:3.08677 Time:0.005226\n",
      "[ Epoch 264, Train ] | Loss:2.94637 Time:0.070262\n",
      "[ Epoch 264, Val ] | Loss:3.08808 Time:0.005164\n",
      "[ Epoch 265, Train ] | Loss:2.94636 Time:0.070630\n",
      "[ Epoch 265, Val ] | Loss:3.08908 Time:0.005232\n",
      "[ Epoch 266, Train ] | Loss:2.94637 Time:0.070366\n",
      "[ Epoch 266, Val ] | Loss:3.08947 Time:0.005244\n",
      "[ Epoch 267, Train ] | Loss:2.94645 Time:0.070660\n",
      "[ Epoch 267, Val ] | Loss:3.09018 Time:0.005168\n",
      "[ Epoch 268, Train ] | Loss:2.94644 Time:0.070384\n",
      "[ Epoch 268, Val ] | Loss:3.09133 Time:0.005229\n",
      "[ Epoch 269, Train ] | Loss:2.94643 Time:0.070701\n",
      "[ Epoch 269, Val ] | Loss:3.09061 Time:0.005230\n",
      "[ Epoch 270, Train ] | Loss:2.94646 Time:0.070902\n",
      "[ Epoch 270, Val ] | Loss:3.09109 Time:0.005163\n",
      "[ Epoch 271, Train ] | Loss:2.94641 Time:0.070472\n",
      "[ Epoch 271, Val ] | Loss:3.09323 Time:0.005233\n",
      "[ Epoch 272, Train ] | Loss:2.94645 Time:0.070236\n",
      "[ Epoch 272, Val ] | Loss:3.09339 Time:0.005184\n",
      "[ Epoch 273, Train ] | Loss:2.94646 Time:0.071050\n",
      "[ Epoch 273, Val ] | Loss:3.09408 Time:0.005114\n",
      "[ Epoch 274, Train ] | Loss:2.94644 Time:0.069616\n",
      "[ Epoch 274, Val ] | Loss:3.09560 Time:0.005180\n",
      "[ Epoch 275, Train ] | Loss:2.94641 Time:0.070097\n",
      "[ Epoch 275, Val ] | Loss:3.09642 Time:0.005219\n",
      "[ Epoch 276, Train ] | Loss:2.94645 Time:0.073042\n",
      "[ Epoch 276, Val ] | Loss:3.09773 Time:0.005149\n",
      "[ Epoch 277, Train ] | Loss:2.94642 Time:0.070229\n",
      "[ Epoch 277, Val ] | Loss:3.09731 Time:0.005219\n",
      "[ Epoch 278, Train ] | Loss:2.94650 Time:0.070107\n",
      "[ Epoch 278, Val ] | Loss:3.09793 Time:0.005220\n",
      "[ Epoch 279, Train ] | Loss:2.94641 Time:0.070178\n",
      "[ Epoch 279, Val ] | Loss:3.09926 Time:0.005156\n",
      "[ Epoch 280, Train ] | Loss:2.94648 Time:0.069899\n",
      "[ Epoch 280, Val ] | Loss:3.10025 Time:0.005229\n",
      "[ Epoch 281, Train ] | Loss:2.94646 Time:0.070591\n",
      "[ Epoch 281, Val ] | Loss:3.10059 Time:0.005236\n",
      "[ Epoch 282, Train ] | Loss:2.94645 Time:0.070280\n",
      "[ Epoch 282, Val ] | Loss:3.10155 Time:0.005157\n",
      "[ Epoch 283, Train ] | Loss:2.94647 Time:0.070024\n",
      "[ Epoch 283, Val ] | Loss:3.10270 Time:0.005221\n",
      "[ Epoch 284, Train ] | Loss:2.94651 Time:0.070233\n",
      "[ Epoch 284, Val ] | Loss:3.10288 Time:0.005220\n",
      "[ Epoch 285, Train ] | Loss:2.94648 Time:0.070722\n",
      "[ Epoch 285, Val ] | Loss:3.10277 Time:0.005151\n",
      "[ Epoch 286, Train ] | Loss:2.94653 Time:0.070019\n",
      "[ Epoch 286, Val ] | Loss:3.10382 Time:0.005225\n",
      "[ Epoch 287, Train ] | Loss:2.94650 Time:0.070276\n",
      "[ Epoch 287, Val ] | Loss:3.10692 Time:0.005229\n",
      "[ Epoch 288, Train ] | Loss:2.94647 Time:0.070636\n",
      "[ Epoch 288, Val ] | Loss:3.10729 Time:0.005148\n",
      "[ Epoch 289, Train ] | Loss:2.94647 Time:0.070366\n",
      "[ Epoch 289, Val ] | Loss:3.10748 Time:0.005209\n",
      "[ Epoch 290, Train ] | Loss:2.94653 Time:0.070429\n",
      "[ Epoch 290, Val ] | Loss:3.10776 Time:0.005221\n",
      "[ Epoch 291, Train ] | Loss:2.94654 Time:0.070248\n",
      "[ Epoch 291, Val ] | Loss:3.10796 Time:0.005166\n",
      "[ Epoch 292, Train ] | Loss:2.94652 Time:0.069609\n",
      "[ Epoch 292, Val ] | Loss:3.10837 Time:0.005216\n",
      "[ Epoch 293, Train ] | Loss:2.94658 Time:0.070041\n",
      "[ Epoch 293, Val ] | Loss:3.10841 Time:0.005226\n",
      "[ Epoch 294, Train ] | Loss:2.94653 Time:0.070421\n",
      "[ Epoch 294, Val ] | Loss:3.11014 Time:0.005141\n",
      "[ Epoch 295, Train ] | Loss:2.94655 Time:0.070509\n",
      "[ Epoch 295, Val ] | Loss:3.11021 Time:0.005235\n",
      "[ Epoch 296, Train ] | Loss:2.94647 Time:0.069902\n",
      "[ Epoch 296, Val ] | Loss:3.11221 Time:0.005231\n",
      "[ Epoch 297, Train ] | Loss:2.94654 Time:0.069924\n",
      "[ Epoch 297, Val ] | Loss:3.11403 Time:0.005163\n",
      "[ Epoch 298, Train ] | Loss:2.94648 Time:0.073089\n",
      "[ Epoch 298, Val ] | Loss:3.11288 Time:0.005224\n",
      "[ Epoch 299, Train ] | Loss:2.94651 Time:0.069876\n",
      "[ Epoch 299, Val ] | Loss:3.11357 Time:0.005202\n",
      "[ Epoch 300, Train ] | Loss:2.94651 Time:0.069693\n",
      "[ Epoch 300, Val ] | Loss:3.11468 Time:0.005171\n",
      "Finish training!\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d3456b05-feac-44fc-85ce-320f8c841db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin testing\n",
      "Test loss: 3.058418869972229\n",
      "——————Test——————\n",
      "           class precision  recall f1-score support\n",
      "0              0    0.7500  1.0000   0.8571       3\n",
      "1              1    0.7500  1.0000   0.8571       3\n",
      "2              2    1.0000  1.0000   1.0000       3\n",
      "3              3    1.0000  1.0000   1.0000       3\n",
      "4              4    1.0000  0.6667   0.8000       3\n",
      "5              5    0.7500  1.0000   0.8571       3\n",
      "6              6    0.6667  0.6667   0.6667       3\n",
      "7              7    1.0000  1.0000   1.0000       3\n",
      "8              8    1.0000  1.0000   1.0000       3\n",
      "9              9    0.6667  0.6667   0.6667       3\n",
      "10            10    1.0000  1.0000   1.0000       3\n",
      "11            11    0.7500  1.0000   0.8571       3\n",
      "12            12    0.7500  1.0000   0.8571       3\n",
      "13            13    1.0000  1.0000   1.0000       3\n",
      "14            14    1.0000  1.0000   1.0000       3\n",
      "15            15    1.0000  1.0000   1.0000       3\n",
      "16            16    1.0000  1.0000   1.0000       3\n",
      "17            17    1.0000  1.0000   1.0000       3\n",
      "18            18    1.0000  0.0000   0.0000       3\n",
      "19            19    0.7500  1.0000   0.8571       3\n",
      "20            20    1.0000  0.6667   0.8000       3\n",
      "21            21    0.5000  0.3333   0.4000       3\n",
      "22            22    1.0000  0.6667   0.8000       3\n",
      "23            23    1.0000  0.6667   0.8000       3\n",
      "24            24    0.7500  1.0000   0.8571       3\n",
      "25            25    1.0000  1.0000   1.0000       3\n",
      "26            26    1.0000  1.0000   1.0000       3\n",
      "27            27    1.0000  1.0000   1.0000       3\n",
      "28            28    1.0000  1.0000   1.0000       3\n",
      "29            29    1.0000  1.0000   1.0000       3\n",
      "30            30    1.0000  1.0000   1.0000       3\n",
      "31            31    1.0000  1.0000   1.0000       3\n",
      "32            32    1.0000  1.0000   1.0000       3\n",
      "33            33    1.0000  1.0000   1.0000       3\n",
      "34            34    1.0000  1.0000   1.0000       3\n",
      "35            35    1.0000  1.0000   1.0000       3\n",
      "36            36    1.0000  1.0000   1.0000       3\n",
      "37            37    1.0000  1.0000   1.0000       3\n",
      "38            38    1.0000  1.0000   1.0000       3\n",
      "39            39    0.7500  1.0000   0.8571       3\n",
      "40            40    0.7500  1.0000   0.8571       3\n",
      "41            41    1.0000  1.0000   1.0000       3\n",
      "42            42    1.0000  1.0000   1.0000       3\n",
      "43            43    1.0000  1.0000   1.0000       3\n",
      "44            44    1.0000  1.0000   1.0000       3\n",
      "45            45    1.0000  1.0000   1.0000       3\n",
      "46            46    1.0000  0.6667   0.8000       3\n",
      "47            47    1.0000  1.0000   1.0000       3\n",
      "48            48    1.0000  1.0000   1.0000       3\n",
      "49            49    1.0000  1.0000   1.0000       3\n",
      "50      accuracy                     0.9200     150\n",
      "51     macro avg    0.9317  0.9200   0.9090     150\n",
      "52  weighted avg    0.9317  0.9200   0.9090     150\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852696d9-970c-4a98-8495-c4aea079f20e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
